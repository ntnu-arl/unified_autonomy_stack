{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Unified Autonomy Stack","text":"<p>Welcome to the documentation for the Unified Autonomy Stack.  This stack presents an autonomy architecture integrating perception, planning, and navigation algorithms developed and field tested at the  Autonomous Robots Lab across robot configurations. The stack consists of the software for the core algorithms along with drivers, utilities, and tools for simulation and testing. We currently support rotary-wing (e.g., multirotor) and certain ground systems (e.g., legged robots) with extension to other configurations, such as underwater robots, coming soon. The software distributed as a part of this stack has been thoroughly tested in real-world scenarios, demonstrating robust autonomous operation in challenging GPS-denied environments.</p> <p></p>"},{"location":"#overview","title":"Overview","text":"<p>The <code>Unified Autonomy Stack</code> is designed to provide a robust and flexible foundation for autonomous operations in various environments. It features:</p> <ul> <li>Multi-modal Perception: fusing LiDAR, radar, vision, and IMU data for robust Simultaneous Localization and Mapping (SLAM) alongside integration of Vision-Language Models (VLMs) for high-level interaction.</li> <li>Planning: Graph-based efficient path planning algorithms tailored for volumetric exploration, visual inspection, and waypoint navigation in complex environments. The planning framework extends to aerial, ground, and underwater robots.</li> <li>Multi-layered Safe Navigation: Combining map-based path planning with learning-based reactive navigation and safety layers.<ul> <li>SDF-NMPC and RL: Neural MPC and Reinforcement Learning based map-free approaches for safe navigation.</li> <li>Last-resort Safety: Control Barrier Functions for filtering unsafe commands.</li> </ul> </li> <li>Multi-platform Support: Designed for both aerial robots and ground robots with planned extension to underwater.</li> <li>Containerized Deployment: Docker-based deployment for easy setup and reproducibility across different platforms</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Please navigate through the tabs to explore setup:</p> <ul> <li>Installation: Instructions for installation and setup.</li> <li>Deployment: Docker-based deployment instructions.</li> <li>Examples: Examples for testing the stack in simulation and on datasets.</li> </ul> <p>descriptions of the subsystems:</p> <ul> <li>Architecture: High-level overview of the system components and data flow.</li> <li>Multi-modal SLAM: Details on the estimation stack.</li> <li>VLM: Details on the VLM stack.</li> <li>Planning: Explanation of the planning stack.</li> <li>Navigation: Information on Neural MPC, RL and Composite CBF Safety Filter for safe navigation.</li> <li>Simulation: Simulation environments and setup.</li> <li>Multi-platform Support: Verified on diverse aerial and ground robots with the ambition to eventually cover most morphologies across air, land and sea.</li> </ul> <p>and indicative results and datasets:</p> <ul> <li>Prior Results: Previous experiences which create the foundation for the Unified Autonomy Stack.</li> <li>Indicative Results: Results of the Unified Autonomy Stack on real robots.</li> <li>Datasets: Relevant datasets for offline evaluation.</li> </ul>"},{"location":"#technical-report","title":"Technical Report","text":"<p>For a comprehensive report of the <code>Unified Autonomy Stack</code> please refer to the Technical Report.</p>"},{"location":"#contact","title":"Contact","text":"<ul> <li>Mihir Dharmadhikari: mihir.dharmadhikari@ntnu.no</li> <li>Nikhil Khedekar: nikhil.v.khedekar@ntnu.no</li> <li>Mihir Kulkarni: mihir.kulkarni@ntnu.no</li> <li>Morten Nissov: morten.nissov@ntnu.no</li> <li>Angelos Zacharia: angelos.zacharia@ntnu.no</li> <li>Martin Jacquet: martin.jacquet@ntnu.no</li> <li>Albert Gassol Puigjaner : albert.g.puigjaner@ntnu.no</li> <li>Philipp Weiss: philipp.weiss@ntnu.no</li> <li>Kostas Alexis: konstantinos.alexis@ntnu.no</li> </ul>"},{"location":"architecture/","title":"General Architecture","text":"<p>The Unified Autonomy Stack implements a modular, containerized autonomy pipeline designed for robust exploration and navigation in subterranean and industrial environments. This document details the specific software components, data flows, and interfaces implemented in this repository.</p> <p></p>"},{"location":"architecture/#system-overview","title":"System Overview","text":"<p>The stack is composed of four primary subsystems, distributed across ROS (Noetic) and ROS 2 (Humble) environments. Communication between these distinct ecosystems is handled by a dedicated <code>ros1_bridge</code>.</p> <pre><code>graph TD\n    Sensors[\"Sensors&lt;br/&gt;(IMU, LiDAR, Radar, Camera)\"]\n\n    subgraph Perception[\"Perception Module\"]\n        SLAM[\"SLAM\"]\n        VLM[\"VLM\"]\n    end\n\n    SceneUnderstanding[\"Scene Understanding\"]\n    VLM --&gt;|Semantic Context| SceneUnderstanding\n    Planning[\"Planning\"]\n\n    subgraph Aerial[\"Aerial\"]\n        subgraph Navigation[\"Navigation Module\"]\n            Selector{{\"Switch\"}}\n            MPC[\"Neural MPC\"]\n            RL[\"Deep RL\"]\n            CBF[\"CBF Safety&lt;br/&gt;Filter\"]\n        end\n        AerialControllers[\"Low Level Controllers\"]\n    end\n\n    subgraph Ground[\"Ground\"]\n        GroundControllers[\"Low Level Controllers\"]\n    end\n\n    Sensors --&gt;|Sensor Data| SLAM\n    Sensors --&gt;|Image| VLM\n    SLAM --&gt;|Odometry &amp; Point Cloud| Planning\n    SLAM --&gt;|State Estimate| Selector\n    Planning --&gt;|Waypoints| Selector\n    Selector --&gt; MPC\n    Selector --&gt; RL\n    MPC --&gt;|Commands| CBF\n    RL -.-&gt;|\"Commands (WIP)\"| CBF\n    CBF --&gt;|Safe Commands| AerialControllers\n    Planning --&gt;|Waypoints| GroundControllers\n    RL --&gt;|Commands| AerialControllers\n\n    classDef perception fill:#d1fae530,stroke:#10b981,stroke-width:2px,color:#000\n    classDef planning fill:#dbeafe30,stroke:#3b82f6,stroke-width:2px,color:#000\n    classDef navigation fill:#ffedd530,stroke:#f97316,stroke-width:2px,color:#000\n    classDef controllers fill:#fce7f330,stroke:#ec4899,stroke-width:2px,color:#000\n    classDef sensors fill:#f3e8ff30,stroke:#a855f7,stroke-width:2px,color:#000\n    classDef switch fill:#fef3c730,stroke:#eab308,stroke-width:2px,color:#000\n    classDef scene fill:#f1f5f930,stroke:#64748b,stroke-width:2px,color:#000\n\n    class SLAM,VLM perception\n    class Planning planning\n    class Selector,MPC,RL,CBF navigation\n    class AerialControllers,GroundControllers controllers\n    class Sensors sensors\n    class SceneUnderstanding scene</code></pre> <p>The Unified Autonomy Stack employs a modular architecture with layered safety guarantees, where each component consumes the outputs of upstream modules while maintaining graceful degradation under partial failures. At the foundation, the Perception Module includes our solution for multi-modal SLAM, alongside integration with a VLM-based reasoning step. The multi-modal SLAM (dubbed MIMOSA) uses a factor graph estimator to fuse IMU preintegration with LiDAR, radar, and visual odometry factors in a windowed smoother. By design, the system continues to provide state estimates even when individual sensor modalities fail\u2014radar sustains operation in dust and smoke where LiDAR degrades, while IMU integration bridges temporary exteroceptive dropouts.</p> <p>State estimates and motion-compensated point clouds flow downstream to the Planning Module, which constructs a TSDF volumetric map, and an elevation map for ground robots, to compute trajectories for exploration, inspection, or waypoint navigation. The graph-base path planner GBPlanner3 is used at the core of the Planning Module, that has been tested and deployed across robot configurations in different environments.</p> <p>The Navigation Module transforms geometric waypoints given by the Planning Module into dynamically feasible commands through a Neural SDF-NMPC formulation that solves a receding-horizon optimal control problem with collision constraints derived from a VAE-based learned distance field. An alternative Deep Reinforcement Learning-based policy provides velocity commands directly from LiDAR observations, enabling continued operation if the MPC solver fails or latency becomes excessive. Both approaches embed obstacle avoidance directly, providing a first layer of collision protection independent of downstream filters. The Composite CBF Safety Filter enforces the final safety guarantees prior to actuation. For aerial platforms, a Control Barrier Function filter minimally modifies commanded accelerations via quadratic programming to maintain forward invariance of a safe set, rejecting any command\u2014regardless of its source\u2014that would violate proximity constraints.</p> <p>Commands are then dispatched to the PX4 autopilot through MAVROS, which provides its own onboard failsafes. Ground robots bypass the Navigation Module and interface the commanded path directly with the robot controllers. Inter-process communication between ROS and ROS 2 components is handled by a dedicated bridge node.</p>"},{"location":"architecture/#deployment-architecture","title":"Deployment Architecture","text":"<p>The system is deployed as a set of Docker containers, following the instructions listed in Deployment.</p>"},{"location":"cbf/","title":"Composite Control Barrier Functions for Last Resort Safety Filtering","text":"<p>Beyond map-based planning and reactive navigation approaches (Neural SDF-NMPC, Exteroceptive-DRL), the Unified Autonomy Stack provides a last-resort safety filter based on Composite Control Barrier Functions (C-CBFs). This filter minimally modifies reference accelerations when an unexpected impending collision is detected, using a sliding window of volumetric map data. Individual obstacle constraints are combined into a composite function that creates a smooth under-approximation of the safe set, with its gradient acting as a \"virtual obstacle\" to steer the robot away from danger. While designed for rare engagement (as upstream avoidance methods are expected to succeed), this module provides mathematically formal safety guarantees\u2014critical for autonomous robots in demanding environments where neural network-based methods cannot offer strict performance assurances.</p> <p>Source Code</p> <ul> <li>Workspace: <code>workspaces/ws_control/src</code></li> <li>Package: <code>composite_cbf</code></li> <li>GitHub: ntnu-arl/composite_cbf</li> </ul> <p>Related Publications:</p> <ul> <li>M. Harms, M. Jacquet and K. Alexis, \"Safe Quadrotor Navigation Using Composite Control Barrier Functions,\" 2025 IEEE International Conference on Robotics and Automation (ICRA), Atlanta, GA, USA, 2025, pp. 6343-6349, doi: 10.1109/icra55743.2025.11127368.</li> <li>N. Misyats, M. Harms, M. Nissov, M. Jacquet and K. Alexis, \"Embedded Safe Reactive Navigation for Multirotors Systems using Control Barrier Functions,\" 2025 International Conference on Unmanned Aircraft Systems (ICUAS), Charlotte, NC, USA, 2025, pp. 697-704, doi: 10.1109/icuas65942.2025.11007827.</li> </ul>"},{"location":"cbf/#cbf-formulation","title":"CBF Formulation","text":"<p>The safety filter acts on the acceleration setpoint within a standard cascaded position-attitude controller. Given a nominal acceleration \\(\\mathbf{a}_{sp}\\), the filter solves an optimization to produce a safe acceleration \\(\\mathbf{a}^*\\).</p>"},{"location":"cbf/#obstacle-avoidance-constraint","title":"Obstacle Avoidance Constraint","text":"<p>From \\(n\\) range observations treated as spherical obstacle centroids \\(\\mathbf{p}_1, \\ldots, \\mathbf{p}_n\\), we define position constraints:</p> \\[\\nu_{i,0}(\\mathbf{x}) \\triangleq \\|\\mathbf{p}_i - \\mathbf{p}\\|^2 - \\epsilon^2\\] <p>Each \\(\\nu_{i,0}\\) has relative degree 2, so we introduce ECBFs:</p> \\[\\nu_{i,1}(\\mathbf{x}) \\triangleq \\dot{\\nu}_{i,0}(\\mathbf{x}) - p_0 \\nu_{i,0}(\\mathbf{x}) = -2\\mathbf{v}^\\top(\\mathbf{p}_i - \\mathbf{p}) - p_0 \\nu_{i,0}(\\mathbf{x})\\] <p>A composite CBF aggregates all \\(n\\) obstacles via log-sum-exp:</p> \\[h(\\mathbf{x}) \\triangleq -\\frac{\\gamma}{\\kappa} \\ln \\sum_{i=1}^{n} \\exp\\left[-\\kappa s(\\nu_{i,1}(\\mathbf{x})/\\gamma)\\right]\\] <p>where \\(s = \\tanh\\) is a saturation function, \\(\\gamma \\geq 1\\) controls sensitivity, and \\(\\kappa\\) controls smoothness.</p>"},{"location":"cbf/#safety-filter-qp","title":"Safety Filter QP","text":"<p>The filter solves a soft-constrained QP to minimally modify the nominal command:</p> \\[\\mathbf{a}^* = \\arg\\min_{\\mathbf{a} \\in \\mathbb{R}^3} (\\mathbf{a} - \\mathbf{a}_{sp})^\\top H (\\mathbf{a} - \\mathbf{a}_{sp}) + \\sum_{i=1}^{2} \\rho \\delta_{fi}\\] <p>subject to:</p> \\[L_g h(\\mathbf{x}) \\mathbf{a} \\geq -L_f h(\\mathbf{x}) - \\alpha(h(\\mathbf{x}))\\] \\[L_g h_{fi}(\\mathbf{x}) \\mathbf{a} \\geq -L_f h_{fi}(\\mathbf{x}) - \\alpha_f h_{fi}(\\mathbf{x}) - \\delta_{fi} \\quad \\forall i \\in \\{1, 2\\}\\] \\[\\delta_{fi} \\geq 0 \\quad \\forall i \\in \\{1, 2\\}\\] <p>where \\(H\\) is a positive-definite weight matrix and \\(\\rho &gt; 0\\) is the slack multiplier. The FoV constraints are implemented as soft constraints to avoid feasibility issues from imperfect acceleration tracking and noisy observations.</p>"},{"location":"cbf/#implementation-details","title":"Implementation Details","text":"<ul> <li>Inputs/Outputs: nominal body acceleration \u2192 filtered acceleration</li> <li>Tuning: <code>epsilon</code> (margin), <code>pole_0</code> (pole location), <code>alpha</code> (decay), <code>gamma</code> (sensitivity), <code>kappa</code> (smoothness), <code>clamp_xy/z</code> (limits)</li> <li>Interface: runs in ROS; filtered acceleration commanded to PX4 via <code>mavros</code></li> </ul>"},{"location":"cbf/#cbf-topics-interfaces","title":"CBF Topics &amp; Interfaces","text":"<p>Namespace: <code>/composite_cbf/</code>.</p>"},{"location":"cbf/#input","title":"Input","text":"Topic Type Description <code>/composite_cbf/nominal_cmd</code> <code>geometry_msgs/Twist</code> Nominal acceleration from NMPC or trajectory tracker <code>/composite_cbf/odometry</code> <code>nav_msgs/Odometry</code> State estimate <code>/composite_cbf/obstacles</code> <code>sensor_msgs/PointCloud2</code> Obstacle points from LiDAR processor"},{"location":"cbf/#output","title":"Output","text":"Topic Type Description <code>/composite_cbf/safe_cmd_twist</code> <code>geometry_msgs/Twist</code> Filtered acceleration setpoint <code>/composite_cbf/safe_cmd_postarget</code> <code>mavros_msgs/PositionTarget</code> Direct command to PX4 via MAVROS <code>/composite_cbf/output_viz</code> <code>geometry_msgs/TwistStamped</code> Output visualization <code>/composite_cbf/input_viz</code> <code>geometry_msgs/TwistStamped</code> Input visualization"},{"location":"cbf/#cbf-configuration","title":"CBF Configuration","text":"<p>All parameters in <code>composite_cbf/config/&lt;preset&gt;.yaml</code> (e.g., <code>magpie.yaml</code>).</p>"},{"location":"cbf/#safety-margins","title":"Safety Margins","text":"Parameter Description <code>epsilon</code> Barrier function margin (m, typically 0.5) <code>pole_0</code> Barrier pole location (typically -2.5) <code>kappa</code> Barrier smoothness parameter (typically 80) <code>gamma</code> Barrier sensitivity parameter (typically 40) <code>alpha</code> Barrier decay rate (typically 1.5)"},{"location":"cbf/#filtering","title":"Filtering","text":"Parameter Description <code>lp_gain_in</code> Low-pass filter gain for input (typically 0.4) <code>lp_gain_out</code> Low-pass filter gain for output (typically 0.2)"},{"location":"cbf/#limits","title":"Limits","text":"Parameter Description <code>clamp_xy</code> Maximum horizontal acceleration correction (m/s\u00b2, typically 3) <code>clamp_z</code> Maximum vertical acceleration correction (m/s\u00b2, typically 2)"},{"location":"cbf/#timing","title":"Timing","text":"Parameter Description <code>ctrl_freq</code> Control loop frequency (Hz, typically 50) <code>obs_to</code> Obstacle timeout (s, typically 1) <code>cmd_to</code> Command timeout (s, typically 1)"},{"location":"cbf/#frames","title":"Frames","text":"Parameter Description <code>output_frame_viz</code> Frame ID for visualization messages"},{"location":"cbf/#citation","title":"Citation","text":"<p>If you use this method in your work, please cite the following publication:</p> <pre><code>@inproceedings{navigation_ccbf,\n    title = {Safe {Quadrotor} {Navigation} {Using} {Composite} {Control} {Barrier} {Functions}},\n    url = {https://ieeexplore.ieee.org/document/11127368},\n    doi = {10.1109/ICRA55743.2025.11127368},\n    booktitle = {2025 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},\n    author = {Harms, Marvin and Jacquet, Martin and Alexis, Kostas},\n    year = {2025},\n}\n\n@inproceedings{navigation_embedded_ccbf,\n  author={Misyats, Nazar and Harms, Marvin and Nissov, Morten and Jacquet, Martin and Alexis, Kostas},\n  booktitle={2025 International Conference on Unmanned Aircraft Systems (ICUAS)}, \n  title={Embedded Safe Reactive Navigation for Multirotors Systems using Control Barrier Functions}, \n  year={2025},\n  pages={697-704},\n  doi={10.1109/ICUAS65942.2025.11007827},\n}\n</code></pre>"},{"location":"datasets/","title":"Datasets","text":""},{"location":"datasets/#datasets","title":"Datasets","text":"<p>Datasets from various missions conducted using the Unified Autonomy Stack can be found here</p>"},{"location":"deployment/","title":"Deployment","text":"<p>The Unified Autonomy Stack is designed to be deployed using Docker and Docker Compose. This ensures a consistent environment across development, simulation, and hardware deployment, as well as keeps the dependencies bundled together without intefering with the rest of the packages. This further adds modularity to the stack as any componenet can be replaced or modified without affecting the rest of the stack.</p>"},{"location":"deployment/#organization-of-the-packages","title":"Organization of the packages","text":""},{"location":"deployment/#workspace-structure","title":"Workspace Structure","text":"<p>The Unified Autonomy Stack is divided across a set of ROS and ROS 2 workspaces that are located in the <code>unified_autonomy_stack/workspaces</code> folder. Each workspace represents a separate component such as a sensor driver, SLAM software, etc. containing the core packages and dependencies. </p> <p>The packages are tracked using <code>git</code> through <code>vcstool</code>. Separate <code>.repos</code> and <code>exact.repos</code> files (hereafter referred to as 'repos file') are created for each workspace and are located in the <code>unified_autonomy_stack/repos</code> folder. These files track the location, remote url, and branch/git commit hash for each package in that workspace. The <code>.repos</code> file tracks the branch, whereas, the <code>exact.repos</code> file tracks the exact commit hash for the package. All workspace and corresponding repos file names start with the prefix <code>ws_</code>. E.g., the workspace for the simulator is <code>unified_autonomy_stack/workspaces/ws_sim</code> and the packages in it are listed in <code>unified_autonomy_stack/repos/ws_sim.repos</code> and <code>unified_autonomy_stack/repos/ws_sim_exact.repos</code>.</p>"},{"location":"deployment/#robot-bringup-package","title":"Robot Bringup Package","text":"<p>Often times, it is important to know the state of the entire stack during a mission which includes the parameter files, launch files, scripts, etc. Hence, a bringup package called <code>robot_bringup</code> is created and located in <code>unified_autonomy_stack/workspaces/robot_bringup</code>. When launching the stack, this package is symlinked inside the <code>src</code> directory of each repository. The package has the config file, launch files, and scripts needed for launching the stack. <code>robot_bringup</code> is structured such that it will be compiled as a ROS or ROS 2 package based on the detected ROS version inside the docker container for each workspace.</p>"},{"location":"deployment/#tracking-packages-through-vcstool","title":"Tracking packages through vcstool","text":"<p>There are two scripts provided in the <code>unified_autonomy_stack/scripts</code> folder to clone the packages from the repos files or to update the repos files when any local changes are made.</p> <ol> <li>Cloning all packages at the state in the repos files: <code>./scripts/import_all_repos.sh</code></li> <li>Updating the repos files to track the current state of the packages: <code>./scripts/export_all_repos.sh</code>.</li> </ol> <p>To check the status of all packages in the stack:</p> <pre><code>cd unified_autonomy_stack/workspaces\nvcs status\n</code></pre>"},{"location":"deployment/#docker-based-deployment-structuring-the-dockerfiles","title":"Docker based deployment - Structuring the dockerfiles","text":"<p>The system is launched through docker compose services located in the docker-compose.\\&lt;purpose&gt;.yml files. We have crated a <code>Makefile</code> to provide easy to use commands to use the docker compose functionalities. The available commands are:</p> <pre><code>help                                Show the help message\nimages                              Build all Docker images\nbuild                               Build all services in parallel (messy output)\nbuild-sequential                    Build all services sequentially (one-by-one and slower, but cleaner output)\nlaunch DOCKER_COMPOSE_FILE=file     Launch all services in the docker compose file named 'file'\nstop                                Stop all launched services\nrestart                             art all launched services\nstatus-all                          All Services Status (all profiles)\nlogs                                show logs from all services\n</code></pre> <p>As the entire stack runs through a set of docker containers, separate dockerfiles are created for containers having specific dependencies. There are three main dockerfiles from which other dockerfiles are created:</p> <ol> <li><code>Dockerfile.ros1_base</code>:<ul> <li>Base image: ros:noetic-perception</li> <li>Description: Ubuntu 20.04 docker with standard ROS packages and supporting linux packages</li> </ul> </li> <li><code>Dockerfile.ros2_base</code>:<ul> <li>Base image: ros:humble-perception</li> <li>Description: Ubuntu 22.04 docker with standard ROS 2 packages and supporting linux packages</li> </ul> </li> <li><code>Dockerfile.ros2_cuda</code>:<ul> <li>Base image:<ul> <li>Default: A custom image (created from <code>Dockerfile.cuda_pytorch</code>) containing cuda 11.8 with pytorch 2.1</li> <li>A cuda and pytorch bundled image for the system specific architecture can be used (e.g. for jetson boards)</li> </ul> </li> <li>Description: Ubuntu 22.04 image with cuda 11.8, pytorch 2.1, and ros2 humble</li> </ul> </li> </ol>"},{"location":"deployment/#building-the-docker-images","title":"Building the docker images","text":"<p>To build the docker images from the dockerfiles, the command <code>make images</code> is used. All dockerfiles are located in the root folder of the <code>unified_autonomy_stack</code> repository. The <code>docker-bake.hcl</code> file lists the target images. The structure of a target is as follows: All docker images will be created under the repository <code>unified_autonomy</code> defined by the snippet:</p> <pre><code>variable \"REGISTRY\" {\n  default = \"unified_autonomy\"\n}\n</code></pre> <p>The <code>\"default\"</code> group specifies the list of target images to be generated:</p> <pre><code>group \"default\" {\n  targets = [\"ros1_base\", \"ros2_base\", \"ros1_gbplanner\", \"ros2_sim\", \"cuda_pytorch\", \"ros2_cuda\", \"ros2_nmpc\", \"ros2_cbf\", \"ros1-bridge-builder\", \"ros2_ros1_bridge\", \"ros2_rl\", \"ros2_vlm\"]\n}\n</code></pre> <p>Each target definition has the following structure:</p> <pre><code>target \"target_name\" {      # Target name (to be added to the group \"default\" for the image to be built)\n  context    = \".\"\n  dockerfile = \"Dockerfile.target_name\"     # Dockerfile to be used for this image\n  tags       = [\"${REGISTRY}:target_name\"]  # Tag of the generated image. We recommend the target name, dockerfile name, and the image take to have the same name\n  contexts   = {\n    \"unified_autonomy:ros2_base\" = \"target:ros2_base\"   # Base image used in the dockerfile (unified_autonomy:ros1_base, unified_autonomy:ros2_base, unified_autonomy:ros2_cuda, or any other if custom base image is created)\n  }\n  network = \"host\"\n}\n</code></pre>"},{"location":"deployment/#docker-based-deployment-running-the-stack","title":"Docker based deployment - Running the stack","text":"<p>All operations (compilation and launching) are done using the docker compose tool. There are two types of docker compose files used in the stack.</p> <p>The file <code>.env</code> stores the environment variables that can be used in the docker compose files.</p>"},{"location":"deployment/#building-the-code","title":"Building the code","text":"<p>The <code>docker-compose.build.yml</code> file contains all the services to build each of the workspaces.</p> A set of templates for ease of use <pre><code>x-system-template: &amp;system-template\n# runtime: nvidia\nworking_dir: /workspace\nnetwork_mode: host\nipc: host\nrestart: \"no\"\nstdin_open: true\ntty: true\nenvironment:\n- DISPLAY=${DISPLAY:-:0}\n- NVIDIA_VISIBLE_DEVICES=all\n- NVIDIA_DRIVER_CAPABILITIES=all\n- QT_X11_NO_MITSHM=1\n- DISABLE_ROS1_EOL_WARNINGS=1\nvolumes:\n    - /tmp/.X11-unix:/tmp/.X11-unix:rw\n\nx-x11-volume: &amp;x11-volume\n/tmp/.X11-unix:/tmp/.X11-unix:rw\n\nx-robot-bringup-volume: &amp;robot-bringup-volume\n./workspaces/robot_bringup:/workspace/src/robot_bringup:rw\n\nx-recorded-data-volume: &amp;recorded-data-volume\n./data:/data:rw\n\nx-catkin-build-template: &amp;catkin-build-template\n&lt;&lt;: *system-template\ncommand: catkin build -DCMAKE_BUILD_TYPE=Release\nprofiles: [\"build\"]\n\nx-colcon-build-template: &amp;colcon-build-template\n&lt;&lt;: *system-template\ncommand: colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release\nprofiles: [\"build\"]\n\nx-ros1-wait-for-roscore: &amp;ros1-wait-for-roscore\ndepends_on:\n    ros1_launch_roscore:\n    condition: service_healthy\n\nx-ros1-launch-template: &amp;ros1-launch-template\n&lt;&lt;: [*system-template, *ros1-wait-for-roscore]\nprofiles: [\"launch\"]\n\nx-ros2-launch-template: &amp;ros2-launch-template\n&lt;&lt;: [*system-template]\nprofiles: [\"launch\"]\n</code></pre> <p>The structure of a build service is as follows: The build service can be a ROS (catkin) or ROS 2 (colcon) build service. Two templates are created for each:</p> <pre><code>x-catkin-build-template: &amp;catkin-build-template\n  &lt;&lt;: *system-template\n  command: catkin build -DCMAKE_BUILD_TYPE=Release\n  profiles: [\"build\"]\n\nx-colcon-build-template: &amp;colcon-build-template\n  &lt;&lt;: *system-template\n  command: colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release \n  profiles: [\"build\"]\n</code></pre> <p>A build service can use one of these (depending on the ros version). A typical build service looks as follows:</p> <pre><code>build_&lt;target&gt;:\n    &lt;&lt; [*catkin-build-template] # Use *colcon-build-template for ROS 2\n    image: &lt;docker image&gt;\n    command: &lt;command&gt; # The templates provide the default commands to build a catkin or colcon package. If a custom command is needed, then use this field otherwise don't add it.\n    volumes:\n        - *robot-bringup-volume # This mounts the robot_bringup package inside the /workspace/src folder\n        - ./workspaces/ws_&lt;name&gt;:/workspace # Mount the workspace inside which this build command will run\n        - ... # Any other volumes\n    ... # Other environment variables, or tags (e.g., runtime: nvidia for containers that need gpu access)\n</code></pre> <p>Note: The docker containers are NOT persistent. Hence, all the code and persistent files are in the workspaces which are then mounted to the directory <code>/workspace</code> inside the docker when the container is created. Hence, when the code compiles, the installed files (build, install, devel directories) remain outside the docker so the compiled binaries remain even when the container is stopped.</p> <p>All the build services are located inside the <code>docker-compose.build.yml</code> file. When the <code>make build</code> or <code>make build-sequential</code> command is used, it runs all the services of the profile <code>\"build\"</code> in the <code>docker-compose.build.yml</code> file.</p>"},{"location":"deployment/#launching-the-stack","title":"Launching the stack","text":"<p>We provide a set of examples to test the autonomy stack. We describe the structure of the docker compose files for launching the stack below.</p> <p>Since each mission might not require running all the packages, a separate docker compose file should be created with the specific services for a particular mission. We provide several docker compose files for the demo scenarios which can be used for reference.</p> <p>To launch all the services of the profile <code>\"launch\"</code> in a docker compose file, from the <code>unified_autonomy_stack</code> directory run:</p> <pre><code>make launch DOCKER_COMPOSE_FILE=filename\n</code></pre> <p>The structure of a docker compose file is as follows. It contains the same templates as in the <code>docker-compose.build.yml</code></p> <p>Among the services, a common service across all files having ROS related packages is the roscore:</p> <pre><code>services:\n  ros1_launch_roscore:\n    &lt;&lt;: [*system-template]\n    image: unified_autonomy:ros1_base\n    profiles: [\"launch\"]\n    command: roscore\n    healthcheck:\n      test: [\"CMD-SHELL\", \"bash\", \"-c\", \"source /opt/ros/noetic/setup.bash &amp;&amp; rostopic list || exit 1\"]\n      interval: 2s\n      timeout: 2s\n      retries: 5\n      start_period: 3s  # Give it time to start before checking\n</code></pre> <p>This launches a roscore. The ros1-launch-template waits for the roscore service to start before launching any other ros1 service</p> Example ROS service <pre><code>ros1_launch_&lt;target&gt;:\n    &lt;&lt;: [*ros1-launch-template]\n    image: &lt;docker image&gt;\n    command: &lt;command&gt; # It is prefered to add the launch files to the robot_bringup package and lauch them here so that the state of the entire stack can be tracked through that one package.\n    volumes:\n    - *x11-volume # Optional\n    - *robot-bringup-volume # This mounts the robot_bringup package inside the /workspace/src folder\n            - ./workspaces/ws_&lt;name&gt;:/workspace # Mount the workspace inside which this build command will run\n            - ... # Any other volumes\n        ... # Other environment variables, or tags (e.g., runtime: nvidia for containers that need gpu access)\n</code></pre> Example ROS 2 service <pre><code>ros2_launch_&lt;target&gt;:\n    &lt;&lt;: [*ros2-launch-template]\n    image: &lt;docker image&gt;\n    command: &lt;command&gt; # It is prefered to add the launch files to the robot_bringup package and lauch them here so that the state of the entire stack can be tracked through that one package.\n    volumes:\n    - *x11-volume # Optional\n    - *robot-bringup-volume # This mounts the robot_bringup package inside the /workspace/src folder\n            - ./workspaces/ws_&lt;name&gt;:/workspace # Mount the workspace inside which this build command will run\n            - ... # Any other volumes\n        environment:\n    - ROS_DOMAIN_ID=${DOMAIN_ID} # The parameter DOMAIN_ID is set in the .env file\n            - ... # OTher environment variables\n        ... # Other environment variables, or tags (e.g., runtime: nvidia for containers that need gpu access)\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>We have created a set of examples that the user can run to test the stack and get familiarized with it.</p> <p>Each example has it's own docker compose file that contains all the services necessary to run the example.</p>"},{"location":"examples/#simulation-uav-exploration","title":"Simulation - UAV Exploration","text":"<p>Upon launching the simulation, follow the steps below to start the mission</p> <ul> <li>Click the <code>Initialize</code> button in the UI. Wait until you see the message <code>initMotion() - Done</code> in the terminal.</li> <li>Click the <code>Start Planner</code> button to start the mission.</li> </ul> <p>More details about the UI can be found here.</p> <p></p>"},{"location":"examples/#using-nmpc","title":"Using NMPC","text":""},{"location":"examples/#cylindrical-lidar","title":"Cylindrical lidar","text":"<pre><code>cd unified_autonomy_stack\nmake launch DOCKER_COMPOSE_FILE=docker-compose.uav_nmpc_sim.yml\n</code></pre>"},{"location":"examples/#dome-lidar","title":"Dome lidar","text":"<pre><code>cd unified_autonomy_stack\nmake launch DOCKER_COMPOSE_FILE=docker-compose.uav_nmpc_unipilot_sim.yml\n</code></pre>"},{"location":"examples/#using-rl","title":"Using RL","text":""},{"location":"examples/#dome-lidar_1","title":"Dome lidar","text":"<pre><code>cd unified_autonomy_stack\nmake launch DOCKER_COMPOSE_FILE=docker-compose.uav_rl_sim.yml\n</code></pre>"},{"location":"examples/#simulation-ugv-exploration","title":"Simulation - UGV Exploration","text":"<pre><code>cd unified_autonomy_stack\nmake launch DOCKER_COMPOSE_FILE=docker-compose.ugv_sim.yml \n</code></pre>"},{"location":"examples/#slam-example","title":"SLAM Example","text":"<p>Download the dataset to test the SLAM from here</p> <p>Terminal 1 (All necessary ROS packages):</p> <pre><code>cd unified_autonomy_stack\nmake launch DOCKER_COMPOSE_FILE=docker-compose.slam_demo.yml\n</code></pre> <p>Terminal 2 (Playing the rosbag):</p> <p>If you have ROS installed locally on your system, you can play the rosbag directly on your machine:</p> <pre><code>rosbag play --clock &lt;bag_name&gt;.bag\n</code></pre> <p>If you do not have ROS installed locally, we provide a docker service to run the rosbags:</p> <pre><code>cd unified_autonomy_stack\nBAG_FOLDER=&lt;path to the directory where the bag is stored&gt; BAG_NAME=&lt;bag_name&gt;.bag  docker compose -f docker-compose.rosbag_play.yml --profile launch up\n</code></pre>"},{"location":"hardware/","title":"Hardware and Drivers","text":""},{"location":"hardware/#included-hardware-and-drivers","title":"Included Hardware and Drivers","text":"<p>This section provides an overview of the hardware components and drivers that are compatible with the Unified Autonomy Stack. It includes details on supported sensors, actuators, and communication interfaces, as well as instructions for integrating new hardware into the framework.</p>"},{"location":"hardware/#supported-hardware","title":"Supported Hardware","text":"<p>We currently support the hardware components mounted on the UniPilot autonomy payload, which has been specifically designed for seamless integration with the Unified Autonomy Stack.</p> <p></p> <p>This hardware is listed in this section is tested and verified to work with an NVIDIA Jetson Orin NX 16GB compute module with a ConnectTech Boson 22 (MIPI) Carrier Board.</p> <p>The list of supported hardware includes:</p> <ul> <li>LiDAR Sensors: Robosense Airy LiDAR sensor</li> </ul> <p>LiDAR Driver Source Code</p> <ul> <li>Workspace: <code>workspaces/ws_rslidar_sdk/src</code></li> <li>Package: <code>rslidar_sdk</code></li> <li>GitHub: ntnu-arl/rslidar_sdk</li> </ul> <ul> <li>Radar Sensors: D3 Embedded RS-6843AOPU</li> </ul> <p>Radar Driver Source Code</p> <ul> <li>Workspace: <code>workspaces/ws_mmwave_ti_ros/src</code></li> <li>Package: <code>ti_mmwave_rospkg</code></li> <li>GitHub: ti/mmwave_ti_ros (with custom patch)</li> </ul> <ul> <li>Cameras: Vision Components IMX296 MIPI Cameras</li> </ul> <p>Camera Driver Source Code</p> <ul> <li>Workspace: <code>workspaces/ws_ros_gst_bridge/src</code></li> <li>Package: <code>ros-gst-bridge</code></li> <li>GitHub: ntnu-arl/ros-gst-bridge</li> </ul> <ul> <li>Camera Synchronization: Custom electronics for hardware triggering and synchronization of multiple cameras</li> </ul> <p>Camera Sync Source Code</p> <ul> <li>Workspace: <code>workspaces/ws_cam_sync/src</code></li> <li>Package: <code>cam_sync</code></li> <li>GitHub: ntnu-arl/cam_sync</li> </ul> <ul> <li>Time of Flight (ToF) Sensor: pmd PicoFlexx 2 ToF Camera</li> </ul> <p>ToF Driver Source Code</p> <ul> <li>Workspace: <code>workspaces/ws_royale_in_ros/src</code></li> <li>Package: <code>royale_in_ros</code></li> <li>GitHub: ntnu-arl/royale_in_ros</li> </ul> <ul> <li>Inertial Measurement Unit (IMU): VectorNav VN-100 IMU</li> </ul> <p>IMU Driver Source Code</p> <ul> <li>Workspace: <code>workspaces/ws_vectornav/src</code></li> <li>Package: <code>vectornav_driver</code></li> <li>GitHub: ntnu-arl/vectornav</li> </ul> <ul> <li>Flight Controller: PX4 Autopilot</li> </ul> <p>Flight Controller Interface Source Code</p> <ul> <li>Workspace: <code>workspaces/ws_mavros/src</code></li> <li>Package: <code>mavros</code></li> <li>GitHub: ntnu-arl/mavros</li> </ul>"},{"location":"indicative_results/","title":"Indicative Results","text":""},{"location":"indicative_results/#indicative-results","title":"Indicative Results","text":""},{"location":"indicative_results/#manual-flight-runehamar-tunnel","title":"Manual Flight (Runehamar Tunnel)","text":"<p>Here an aerial platform with VectorNav VN-100 IMU, Ouster OS0-128 LiDAR, and Texas Instruments IWR6843AOP Radar flying manually through the Runehamar tunnel in Norway. The performance of the LiDAR-radar-inertial fusion can be see in the aggregated point cloud map. Note, the estimate experiences some vertical drift due to difficulties with respect to bias observability, exaggerated by the large distance traveled by the mission.</p> <p></p> <p>Metadata:</p> <ul> <li>Distance: 1444 m</li> <li>Maximum Velocity: 11 m/s</li> </ul>"},{"location":"indicative_results/#exploration","title":"Exploration","text":""},{"location":"indicative_results/#runehamar-tunnel","title":"Runehamar Tunnel","text":"<p>In the same environment, we conduct exploration missions with the autonomy stack, executed with the UniPilot module mounted on a quad rotor frame. Here you can see the path it executes, starting at the entrace and continuing onwards into the tunnel, before turning around and returning to home.</p> <p></p> <p>Metadata:</p> <ul> <li>Distance: 270 m</li> <li>Maximum Velocity: 1.9 m/s</li> </ul>"},{"location":"indicative_results/#lkken-mine","title":"L\u00f8kken Mine","text":"<p>Here we show an exporation mission conducted with the UniPilot module in the L\u00f8kken mine. Note, the estimation and planning performance as it transitions from a small, confined corridor out to a large, open space and back again for homing. In the second figure, the aggregated point cloud of the radar can be seen instead. Clearly, the points are significantly noisier than the corresponding map for the LiDAR.</p> <p></p> <p></p> <p>Metadata:</p> <ul> <li>Distance: 197 m</li> <li>Maximum Velocity: 1.1 m/s</li> </ul>"},{"location":"indicative_results/#changing-the-environment","title":"Changing the Environment","text":"<p>Although the planner ensures collision free paths, if the environment were to change a given path may no longer be collision free. In this series of experiments, we conduct exploration missions in a cluttered basement corridor, and intentionally sabotage the planner at two distinct points. Both the NMPC- and RL-based safety methods ensure the aerial platform continues safely, when the planners path is compromised.</p>"},{"location":"indicative_results/#nmpc","title":"NMPC","text":"<p>Here you can see the NMPC, ensuring safety, by pushing the aerial platform away from the planned path to avoid an object put to cause collision with the path from the planner.</p> <p></p> <p>Metadata:</p> <ul> <li>Distance: 125 m</li> <li>Maximum Velocity: 1.8 m/s</li> </ul>"},{"location":"indicative_results/#rl","title":"RL","text":"<p>Here you can see the RL policy, ensuring safety, by pushing the aerial platform away from the planned path to avoid an object put to cause collision with the path from the planner.</p> <p></p> <p>Metadata:</p> <ul> <li>Distance: 125 m</li> <li>Maximum Velocity: 2.1 m/s</li> </ul>"},{"location":"indicative_results/#legged-robot","title":"Legged Robot","text":"<p>The UniPilot with the Unified Autonomy Stack is deployed on the ANYmal D legged robot tasked to explore a part of a university building. The mission involved navigation through both wide open and narrow corridors. The long narrow geometry of the narrow corridors can render a LiDAR only or LiDAR-inertial method to become degenerate. However, through the fusion of radar, the robot is able to resiliently localize and complete the mission.</p> <p></p> <p>Metadata:</p> <ul> <li>Distance: 873 m</li> <li>Maximum Velocity: 1.1 m/s</li> </ul> <p>Stay tuned for more evaluations! </p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#this-page-details-the-instructions-to-install-the-unified-autonomy-stack","title":"This page details the instructions to install the Unified Autonomy Stack.","text":"<p>The instructions explain how to setup the entire stack to use in your work or run the provided demos. Further details about the structuring of the stack can be found on the Deployment page.</p>"},{"location":"installation/#requirements","title":"Requirements","text":""},{"location":"installation/#computer-requirements","title":"Computer requirements","text":"<p>The stack has been tested on Ubuntu 20.04 and 22.04. As the stack includes learning-based navigation modules, an Nvidia GPU is required to run those.</p>"},{"location":"installation/#docker","title":"Docker","text":"<p>The stack is organized as a collection of docker containers, hence, first install docker using the instructions on the official webpage.</p>"},{"location":"installation/#docker-configuration","title":"Docker configuration","text":"<p>Configure the <code>/etc/docker/daemon.json</code> as follows:</p> <pre><code>{\n    \"runtimes\": {\n        \"nvidia\": {\n            \"args\": [],\n            \"path\": \"nvidia-container-runtime\"\n        }\n    }\n}\n</code></pre> <p>Note: Do not use \"nvidia\" as the default run time as this may cause issues with some of the packages.</p> <p>Add the following line to <code>~/.bashrc</code> or <code>~/.zshrc</code>:</p> <pre><code>xhost +local:docker\n</code></pre>"},{"location":"installation/#install-other-dependencies","title":"Install other dependencies","text":"<pre><code>sudo apt install python3-vcstool\n</code></pre>"},{"location":"installation/#unified-autonomy-stack-installation","title":"Unified Autonomy Stack Installation","text":""},{"location":"installation/#clone-the-required-packages","title":"Clone the required packages","text":"<p>Clone the base repository:</p> <pre><code>git clone git@github.com:ntnu-arl/unified_autonomy_stack.git\n</code></pre> <p>Clone individual packages:</p> <pre><code>cd unified_autonomy_stack\nmkdir workspaces\n./scripts/import_all_repos.sh  # recursively clones all the repositories\n</code></pre> <p>Note: Cloning all repositories will take some time (especially ws_sim.repos), please be patient</p>"},{"location":"installation/#generate-docker-images","title":"Generate docker images","text":"<pre><code># cd unified_autonomy_stack\nmake images\n</code></pre> <p>Note: Building the images for the firs time will take some time, please be patient</p>"},{"location":"installation/#build-the-code","title":"Build the code","text":""},{"location":"installation/#there-are-two-ways-to-build-the-code","title":"There are two ways to build the code:","text":"<p>Option1: Build all workspaces in parallel. This method is faster but it can be tedious to see the output of individual workspaces.</p> <pre><code># cd unified_autonomy_stack\nmake build\n</code></pre> <p>Option2: Build one workspace at a time. This method is slower but easier to track the output of each workspace.</p> <pre><code># cd unified_autonomy_stack\nmake build-sequential\n</code></pre> <p>Note: If you are building the stack on a low ram computer, it is advised to use Option2 as it will not make the ram fill up causing the build to fail.</p>"},{"location":"installation/#configure-ros_domain_id","title":"Configure ROS_DOMAIN_ID","text":"<p>Set the variable <code>DOMAIN_ID</code> in the <code>unified_autonomy_stack/.env</code> file</p>"},{"location":"navigation/","title":"Navigation &amp; Control","text":"<ul> <li>See Navigation below.<ul> <li>Neural MPC (NMPC)</li> <li>Reinforcement Learning (RL)</li> </ul> </li> <li>See Control (CBF safety layer) below.</li> </ul>"},{"location":"navigation/#navigation","title":"Navigation","text":"<p>The Unified Autonomy Stack takes a multi-layered approach to safety. Conventionally, safe navigation relied solely on map-based path planning\u2014a single point of failure that could lead to collisions due to odometry drift or incomplete mapping (e.g., thin obstacles). While the stack maintains map-based avoidance as its core approach, it adds reactive safety layers through two complementary methods:</p> <ol> <li>Neural SDF-NMPC \u2014 exteroceptive nonlinear MPC with learned collision constraints, paired with a CBF-based safety filter.</li> <li>Exteroceptive DRL \u2014 reinforcement learning policies trained for smooth collision avoidance directly from depth observations.</li> <li>Composite CBF Safety Filter \u2014 a Control Barrier Function filter that modifies commands to ensure safety.</li> </ol> <p>Both methods consume online sensor data and can locally deviate from planned paths when necessary, replacing conventional position controllers that blindly follow map-based trajectories.</p>"},{"location":"navigation/#navigation-module-architecture","title":"Navigation Module Architecture","text":""},{"location":"navigation/#system-integration-control-flow","title":"System Integration &amp; Control Flow","text":"<pre><code>---\ntitle: Aerial Robots\n---\ngraph TB\n    Planner[\"Planning&lt;br/&gt;(GBPlanner3)\"]\n    Selector{{\"Switch&lt;br/&gt;NMPC/RL/Override\"}}\n\n    Planner --&gt;|Reference&lt;br/&gt;Trajectory| Selector\n\n    Selector --&gt;|NMPC Mode| NMPC[\"NMPC&lt;br/&gt;(ROS 2)\"]\n    Selector --&gt;|RL Mode| RL[\"RL Policy&lt;br/&gt;(ROS 2)\"]\n\n    NMPC --&gt;|Acceleration&lt;br/&gt;Setpoints| CBF[\"CBF Safety&lt;br/&gt;Filter&lt;br/&gt;(ROS 2)\"]\n    RL -.-&gt;|\"Acceleration&lt;br/&gt;Setpoints&lt;br/&gt;(WIP)\"| CBF\n\n    CBF --&gt;|\"Filtered&lt;br/&gt;Acceleration&lt;br/&gt;(MAVROS)\"| Autopilot[\"Onboard&lt;br/&gt;Autopilot\"]\n    RL --&gt;|\"Velocity&lt;br/&gt;Setpoints&lt;br/&gt;(MAVROS)\"| Autopilot\n\n    Autopilot --&gt;|Motor&lt;br/&gt;Commands| Robot[\"Multirotor\"]\n    Selector --&gt;|\"Override&lt;br/&gt;Reference&lt;br/&gt;Trajectory&lt;br/&gt;(MAVROS)\"| Autopilot\n\n    classDef nmpc fill:#e0e7ff30,stroke:#6366f1,stroke-width:2px,color:#000\n    classDef rl fill:#ffedd530,stroke:#f97316,stroke-width:2px,color:#000\n    classDef cbf fill:#ede9fe30,stroke:#a855f7,stroke-width:2px,color:#000\n    classDef autopilot fill:#cffafe30,stroke:#06b6d4,stroke-width:2px,color:#000\n    classDef switch fill:#fef3c730,stroke:#eab308,stroke-width:2px,color:#000\n    classDef other fill:#f1f5f930,stroke:#64748b,stroke-width:2px,color:#000\n\n    class NMPC nmpc\n    class RL rl\n    class CBF cbf\n    class Autopilot autopilot\n    class Selector switch\n    class Planner,Robot other</code></pre> <pre><code>---\ntitle: Ground Robots\n---\ngraph TB\n    Planner2[\"Planning&lt;br/&gt;(GBPlanner3)\"]\n    TrajectoryTracker[\"Robot&lt;br/&gt;Controller\"]\n    Robot2[\"Ground Robot\"]\n\n    Planner2 --&gt;|Reference&lt;br/&gt;Trajectory| TrajectoryTracker\n    TrajectoryTracker --&gt;|Joint Commands| Robot2\n\n    classDef planner fill:#f1f5f930,stroke:#64748b,stroke-width:2px,color:#000\n    classDef loco fill:#d1fae530,stroke:#10b981,stroke-width:2px,color:#000\n    classDef robot fill:#f1f5f930,stroke:#64748b,stroke-width:2px,color:#000\n\n    class Planner2 planner\n    class TrajectoryTracker loco\n    class Robot2 robot</code></pre>"},{"location":"navigation/#low-level-controller","title":"Low-level Controller","text":""},{"location":"navigation/#real-world-platforms","title":"Real-world Platforms","text":"<p>The onboard controller from the PX4 autopilot is used to track the filtered acceleration or velocity setpoint commands. MAVROS is used to communicate these commands to the PX4 firmware.</p>"},{"location":"navigation/#simulation","title":"Simulation","text":"<p>In simulation, geometric controllers based on T. Lee et. al., \"Control of Complex Maneuvers for a Quadrotor UAV using Geometric Methods on SE(3)\" are are used in Gazebo.</p>"},{"location":"nmpc/","title":"Neural SDF-MPC (NMPC)","text":"<p>Neural SDF-MPC is a nonlinear model predictive control framework for mapless, collision-free navigation in unknown environments using onboard range sensing. The method leverages deep neural networks to encode a single range image\u2014capturing all available information about the environment\u2014into a Signed Distance Function (SDF). This representation is used to formulate collision avoidance constraints within a receding-horizon optimal control problem, solved using an efficient SQP-RTI scheme.</p> <p>Source Code</p> <ul> <li>Workspace: <code>workspaces/ws_nmpc/src</code></li> <li>Package: <code>sdf_nmpc_ros</code></li> <li>GitHub: ntnu-arl/sdf_nmpc_ros</li> </ul> <p>Related Publication:</p> <ul> <li>Jacquet, Martin, Marvin Harms, and Kostas Alexis. \"Neural NMPC through Signed Distance Field Encoding for Collision Avoidance.\" arXiv preprint arXiv:2511.21312 (2025).</li> </ul> <p>The neural architecture consists of two cascaded networks:</p> <ol> <li>Convolutional Encoder: Compresses the input depth/range image into a low-dimensional latent vector.</li> <li>Coordinate-based MLP: Approximates the corresponding spatial SDF, parametrizing an explicit position constraint for collision avoidance.</li> </ol> <p>This SDF network is embedded in a velocity-tracking NMPC that outputs acceleration and yaw rate commands to the robot. The controller enables reactive collision avoidance directly from sensor observations, without requiring a pre-built map\u2014providing an additional safety layer when odometry drifts or obstacles are incompletely mapped.</p> <p>Key Properties:</p> <ul> <li>Mapless navigation: Operates directly on range images without global map construction.</li> <li>Theoretical guarantees: Recursive feasibility and stability under fixed observations.</li> <li>Real-time performance: ~10 ms solve time via acados SQP-RTI solver.</li> <li>Resilience: Robust to drifting odometry and sensor noise.</li> </ul>"},{"location":"nmpc/#vae-latent-for-sdf-representation","title":"VAE Latent for SDF Representation","text":"<p>We use a two-step process to handle observations:</p> <ol> <li>Encoder (CNN): Encodes the observation into a compact latent representation, exploiting spatial correlation via convolutions.</li> <li>Decoder (MLP): Processes the latent vector concatenated with a 3D query point to predict collision scores.</li> </ol> <p>This architecture provides several advantages:</p> <ul> <li>Computational efficiency: Avoids redundant computation when evaluating the SDF at multiple points given the same observation. Leverages spatial patterns in the input via the CNN, while keeping the MLP lightweight.</li> <li>Robustness: Improves resilience to noise and systematic errors (e.g., stereo shadows, invalid LiDAR rays). A \\(\\beta\\)-VAE architecture with a ResNet-10 network as the encoder. It incorporates ReLU activations, batch normalization, and dropout regularization for improved generalizability.</li> </ul>"},{"location":"nmpc/#sdf-mlp-architecture","title":"SDF MLP Architecture","text":"<p>The latent encoding \\(\\mathbf{z}\\) is processed using a coordinate-based MLP that approximates the distance transform for the corresponding observation.</p> <p>Positional Embedding: Similar to NeRF-style architectures, positional embedding is first applied to the 3D query point \\(\\mathbf{p}\\), mapping it to a high-dimensional space using periodic activation functions. This sinusoidal mapping allows MLPs to better represent high-frequency content.</p> <p>Sensor Specific</p> <p>The SDF MLP operates on spatial data and implicitly encodes the intrinsic projection matrix. It is therefore trained for a specific sensor configuration.</p>"},{"location":"nmpc/#problem-formulation","title":"Problem Formulation","text":"<p>The NMPC solves a velocity-tracking optimal control problem over a receding horizon \\(T\\) with \\(N\\) shooting nodes, subject to learned collision avoidance constraints.</p>"},{"location":"nmpc/#collision-avoidance-constraint","title":"Collision Avoidance Constraint","text":"<p>Given an observation \\(o\\) encoded into latent \\(\\mathbf{z}\\) via the VAE encoder, the SDF network defines \\(\\text{SDF}_{\\theta,\\mathbf{z}}: \\mathbb{R}^3 \\to \\mathbb{R}\\). The collision-free constraint is:</p> \\[\\text{SDF}_{\\theta,\\mathbf{z}}({}^{S_0}\\mathbf{p}_B) \\geq r + \\epsilon\\] <p>where \\(r\\) is the robot-enclosing radius and \\(\\epsilon &gt; 0\\) is a user-defined safety margin.</p>"},{"location":"nmpc/#field-of-view-constraints","title":"Field of View Constraints","text":"<p>Since the collision-free set is defined within the sensor frustum \\(\\mathcal{F}(t_0)\\), the predicted trajectory must remain within this volume:</p> \\[-\\alpha_H \\leq \\mathcal{S}_{\\text{azimuth}}({}^{S_0}\\mathbf{p}_S) \\leq \\alpha_H, \\quad -\\alpha_V \\leq \\mathcal{S}_{\\text{elevation}}({}^{S_0}\\mathbf{p}_S) \\leq \\alpha_V\\]"},{"location":"nmpc/#optimal-control-problem","title":"Optimal Control Problem","text":"<p>The stage cost \\(\\ell(\\mathbf{x}, \\mathbf{u})\\) penalizes velocity tracking error, heading error, and control effort:</p> \\[\\ell(\\mathbf{x}, \\mathbf{u}) = \\left\\| \\begin{bmatrix} q_{e,z} \\\\ {}^{V_0}\\mathbf{v} - {}^{V_0}\\mathbf{v}_{\\text{ref}} \\end{bmatrix} \\right\\|^2_Q + \\left\\| \\begin{bmatrix} T_z - mg \\\\ {}^V\\phi \\\\ {}^V\\theta \\\\ {}^B\\omega_z \\end{bmatrix} \\right\\|^2_R\\] <p>where \\(q_{e,z}\\) is the yaw error in quaternion form, \\(T_z\\) is the vertical thrust component, and \\(Q\\), \\(R\\) are tunable positive semidefinite and positive definite weight matrices, respectively.</p>"},{"location":"nmpc/#recursive-feasibility","title":"Recursive Feasibility","text":"<p>A terminal constraint ensures recursive feasibility by requiring that a \"maximum braking to standstill\" policy remains feasible from the terminal state. Under fixed observations, this guarantees recursive feasibility and local stability.</p>"},{"location":"nmpc/#nmpc-topics-interfaces","title":"NMPC Topics &amp; Interfaces","text":"<p>Namespace: <code>/sdf_nmpc/</code>. Topics are remapped in the launch file.</p>"},{"location":"nmpc/#input","title":"Input","text":"Topic Type Description <code>odometry</code> <code>nav_msgs/Odometry</code> State estimate (remapped to <code>/msf_core/odometry</code>) <code>horizon_ref</code> <code>trajectory_msgs/MultiDOFJointTrajectory</code> Reference trajectory from <code>ref_gen_node.py</code> <code>wps</code> <code>nav_msgs/Path</code> Waypoints from planner (remapped to <code>/gbplanner_path</code>) <code>latent</code> <code>sdf_nmpc_ros/Latent</code> VAE latent vector (custom: header + <code>Float32MultiArray</code>) <code>observation</code> <code>sensor_msgs/Image</code> LiDAR range image (remapped to <code>/img_node/range_image</code>)"},{"location":"nmpc/#output","title":"Output","text":"Topic Type Description <code>cmd/acc</code> <code>mavros_msgs/PositionTarget</code> Acceleration command (remapped to <code>/mavros/setpoint_raw/local</code>) <code>output/cpt</code> <code>std_msgs/Float32</code> Solver compute time <code>output/speed</code> <code>std_msgs/Float32</code> Current speed <code>viz/horizon_traj</code> <code>nav_msgs/Path</code> Predicted horizon trajectory (visualization)"},{"location":"nmpc/#services","title":"Services","text":"Service Type Description <code>get_flag</code> <code>std_srvs/Trigger</code> Get SDF enable flag <code>set_flag</code> <code>std_srvs/SetBool</code> Set SDF enable flag"},{"location":"nmpc/#nmpc-configuration","title":"NMPC Configuration","text":"<p>All parameters in <code>sdf_nmpc_ros/config/&lt;preset&gt;.yaml</code> (e.g., <code>sim_lidar.yaml</code>, <code>magpie.yaml</code>).</p>"},{"location":"nmpc/#reference","title":"Reference","text":"Parameter Description <code>ref/yaw_mode</code> Yaw control mode (<code>align</code>, <code>ref</code>, <code>current</code>, <code>zero</code>) <code>ref/vref</code> Reference velocity norm (m/s) <code>ref/wzref</code> Reference yaw rate norm (rad/s) <code>ref/zref</code> Desired hovering altitude (m)"},{"location":"nmpc/#flags","title":"Flags","text":"Parameter Description <code>flags/simulation</code> Simulation or hardware mode (bool) <code>flags/enable_sdf</code> Enable collision prediction (bool) <code>flags/sdf_cost</code> Use SDF in cost function (bool) <code>flags/sdf_constraint</code> Use SDF in constraints (bool) <code>flags/vfov_constraint</code> Enable vertical FoV constraint (bool)"},{"location":"nmpc/#neural-network","title":"Neural Network","text":"Parameter Description <code>nn/size_latent</code> Latent vector dimension (typically 128) <code>nn/vae_device</code> Device for VAE inference (<code>cuda</code>) <code>nn/sdf_device</code> Device for SDF network (<code>cuda</code>) <code>nn/vae_weights</code> Path to VAE model weights <code>nn/sdf_weights</code> Path to SDF model weights"},{"location":"nmpc/#mpc","title":"MPC","text":"Parameter Description <code>mpc/N</code> Number of shooting nodes (typically 20) <code>mpc/T</code> Horizon length (s, typically 1.5) <code>mpc/bound_margin</code> Safety margin for collision constraint (m, typically 0.15) <code>mpc/control_loop_time</code> Minimum control period (ms, typically 10) <code>mpc/max_solver_fail</code> Maximum successive solver failures before reset (-1 to disable)"},{"location":"nmpc/#mpc-weights","title":"MPC Weights","text":"Parameter Description <code>mpc/weights/pos</code> Position tracking weights <code>[px, py, pz]</code> <code>mpc/weights/vel</code> Velocity tracking weights <code>[vx, vy, vz]</code> <code>mpc/weights/att</code> Attitude tracking weights <code>[roll, pitch, yaw]</code> <code>mpc/weights/rates</code> Angular rate weights <code>[wx, wy, wz]</code> <code>mpc/weights/acc</code> Control effort weight <code>mpc/weights/slack_df</code> Slack weights for distance constraint <code>[L1, L2]</code> <code>mpc/weights/slack_fov</code> Slack weights for FoV constraint <code>[L1, L2]</code>"},{"location":"nmpc/#robot-limits","title":"Robot Limits","text":"Parameter Description <code>robot/limits/vx</code>, <code>vy</code>, <code>vz</code> Maximum velocities (m/s) <code>robot/limits/ax</code>, <code>ay</code>, <code>az</code> Maximum accelerations (m/s\u00b2) <code>robot/limits/wz</code> Maximum yaw rate (rad/s) <code>robot/limits/roll</code>, <code>pitch</code> Maximum attitude angles (rad)"},{"location":"nmpc/#ros","title":"ROS","text":"Parameter Description <code>ros/control_interface</code> Control interface type (<code>acc</code>) <code>ros/timeout_ref</code> Reference timeout (s) <code>ros/timeout_img</code> Image/observation timeout (s) <code>ros/frames/world</code> World frame ID <code>ros/frames/body</code> Body frame ID <code>ros/frames/sensor</code> Sensor frame ID"},{"location":"nmpc/#citation","title":"Citation","text":"<p>If you use this code or the associated methods in your research, please cite the following publication:</p> <pre><code>@misc{navigation_neuralmpc,\n    title = {Neural {NMPC} through {Signed} {Distance} {Field} {Encoding} for {Collision} {Avoidance}},\n    url = {http://arxiv.org/abs/2511.21312},\n    doi = {10.1177/02783649251401223},\n    urldate = {2025-11-30},\n    author = {Jacquet, Martin and Harms, Marvin and Alexis, Kostas},\n    month = nov,\n    year = {2025},\n}\n</code></pre>"},{"location":"planned_upgrades/","title":"Planned Upgrades","text":"<ul> <li> Migrate to ROS 2 Humble</li> <li> Switch from <code>vcstool</code> to <code>git</code> submodule</li> <li> Vision integration into MIMOSA</li> <li> Additional experiments</li> <li> Expansion to fixed-wing</li> <li> Expansion to underwater</li> <li> Reinforcement-learning and CBF integration</li> <li> Release of sensor triggering hardware and software</li> </ul>"},{"location":"planning/","title":"Planning","text":"<p>GBPlanner 3.0 is the extension of our previous work on Graph-Based Exploration Planner. This release presents a set of improvements and new features:</p> <ul> <li>Improved path planning through batch sampling</li> <li>Brand new Inspection mode to inspect the mapped surfaces with a FoV constrained camera sensor</li> <li>Extended the state space of exploration and inspection to include sensor pitch</li> </ul> <p>The method is fully open-source and integrated into the Unified Autonomy Stack. It has been validated in both simulation and real-world experiments on aerial and ground robots.</p> <p>Source Code</p> <ul> <li>Workspace: <code>workspaces/ws_gbplanner/src</code></li> <li>Package: <code>gbplanner_ros</code></li> <li>GitHub: ntnu-arl/gbplanner_ros</li> </ul> <p>Related Publications:</p> <ul> <li>Dharmadhikari, M., et al. \"Autonomous Exploration and General Visual Inspection of Ship Ballast Water Tanks Using Aerial Robots.\" 2023 21st International Conference on Advanced Robotics (ICAR), pp. 409-416, 2023.</li> <li>Kulkarni, M., et al. \"Autonomous Teamed Exploration of Subterranean Environments using Legged and Aerial Robots.\" 2022 International Conference on Robotics and Automation (ICRA), pp. 3306-3313, 2022.</li> <li>Dang, T., et al. \"Graph-based subterranean exploration path planning using aerial and legged robots.\" Journal of Field Robotics, vol. 37, no. 8, pp. 1363-1388, 2020.</li> </ul> <p></p>"},{"location":"planning/#local-planner","title":"Local Planner","text":"<p>The local planner operates in a subspace around the current robot location to maximize newly mapped volume. It builds a 3D graph by sampling vertices in free space and connecting them with admissible edges. Shortest paths are computed using Dijkstra's algorithm from the current robot location, and an exploration gain\u2014reflecting the expected volume to be mapped along each path\u2014is calculated for candidate trajectories. GBPlanner3 offers clustering of vertices for gain calculation and an option to compute gain only at leaf vertices of the shortest paths to accelerate computation. Additional ray casting optimizations further reduce computation time. The path with the highest exploration gain is selected, refined for safety, and commanded to the robot.</p> <p>Edge Admissibility:</p> Robot Type Admissibility Criteria Aerial Edge lies entirely in free space Ground Sampled points and interpolated edges are projected downward onto the map to verify ground presence; each segment must be collision-free and within inclination limits set by locomotion constraints"},{"location":"planning/#global-planner","title":"Global Planner","text":"<p>The global planner handles two tasks:</p> <ol> <li>Repositioning: When local exploration is exhausted, the robot is guided to a previously identified high-gain location on the map.</li> <li>Return-to-home: The robot is brought back to the home location when the mission time limit is reached or the entire bounded volume has been explored.</li> </ol>"},{"location":"planning/#inspection-mode","title":"Inspection Mode","text":"<p>In inspection mode, the planner builds a grid of viewpoints along mapped surfaces using the underlying volumetric map. These viewpoints are connected by collision-free edges to form a 3D graph; additional points are sampled if any viewpoints remain disconnected. The minimal set of viewpoints covering the entire visible mapped surface is selected, and their visitation order is determined by solving the Traveling Salesman Problem.</p>"},{"location":"planning/#planning-architecture","title":"Planning Architecture","text":"<p>The following diagram describes the entities in the Planning Module along with the data exchange within and outside the module.</p> <pre><code>graph TB\n    SLAM[\"Multi-modal SLAM\"]\n\n    subgraph Mapping\n        VolMap[\"Volumetric Mapping\"]\n        EleMap[\"Elevation Mapping\"]\n    end\n\n    subgraph GBPlanner\n        BT[\"Behavior Tree\"]\n        LP[\"Local Planner\"]\n        GP[\"Global Planner\"]\n    end\n\n    VolMap --&gt;|\"TSDF Map\"| GBPlanner\n    EleMap --&gt;|\"Elevation Map\"| GBPlanner\n\n    PCI[\"Planner Control Interface\"]\n\n    SLAM --&gt;|\"Downsampled LiDAR Cloud\"| VolMap\n    SLAM --&gt;|\"Downsampled LiDAR Cloud\"| EleMap\n    SLAM --&gt;|\"Odometry\"| VolMap\n    SLAM --&gt;|\"Odometry\"| EleMap\n    SLAM --&gt;|\"Odometry\"| PCI\n    SLAM --&gt;|\"Odometry\"| GBPlanner\n\n    PCI --&gt;|\"Forward UI request\"| BT\n    GBPlanner --&gt;|\"Requested path\"| PCI\n\n    Path[\"Path to&lt;br/&gt;Navigation Module\"]\n    PCI --&gt;|\"Path for robot\"| Path\n\n    UI[\"User Interface\"]\n    UI --&gt;|\"Mission request&lt;br/&gt;(start, stop)\"| PCI\n\n    classDef input fill:#e0e7ff30,stroke:#6366f1,stroke-width:2px,color:#000\n    classDef frontend fill:#fef3c730,stroke:#eab308,stroke-width:2px,color:#000\n    classDef backend fill:#d1fae530,stroke:#10b981,stroke-width:2px,color:#000\n    classDef output fill:#cffafe30,stroke:#06b6d4,stroke-width:2px,color:#000\n\n    class SLAM,UI input\n    class PCI frontend\n    class BT,LP,GP,VolMap,EleMap backend\n    class Path output</code></pre> <p>The planner uses a bifurcated local and global planning architecture.</p>"},{"location":"planning/#topics-interfaces","title":"Topics &amp; Interfaces","text":"<p>The planner stack consists of two ROS nodes: GBPlanner and Planner Control Interface (PCI).</p>"},{"location":"planning/#services-input","title":"Services (Input)","text":"Service Type Description <code>pci/std_srvs/automatic_planning</code> <code>std_srvs/Trigger</code> Trigger planner; PCI keeps triggering at end of each iteration <code>pci/std_srvs/single_planning</code> <code>std_srvs/Trigger</code> Trigger single planning iteration <code>pci/stop_request</code> <code>std_srvs/Trigger</code> Stop planner and halt robot <code>pci/std_srvs/homing_trigger</code> <code>std_srvs/Trigger</code> Return to home after current iteration <code>pci/std_srvs/go_to_waypoint</code> <code>std_srvs/Trigger</code> Plan path to waypoint along global graph <code>pci_initialization_trigger</code> <code>planner_msgs/pci_initialization</code> Trigger initialization motion <code>pci_global</code> <code>planner_msgs/pci_global</code> Send robot to specific global graph vertex <code>pci/std_srvs/inspection_srv_trigger</code> <code>std_srvs/Trigger</code> Trigger inspection mode"},{"location":"planning/#subscribed-topics","title":"Subscribed Topics","text":""},{"location":"planning/#gbplanner","title":"GBPlanner","text":"Topic Type Description <code>pose</code> <code>geometry_msgs/PoseWithCovarianceStamped</code> Robot pose <code>pose_stamped</code> <code>geometry_msgs/PoseStamped</code> Robot pose (alternative) <code>odometry</code> <code>nav_msgs/Odometry</code> Robot odometry <code>cam_pitch</code> <code>sensor_msgs/JointState</code> Actuated camera pitch angle <code>/robot_status</code> <code>planner_msgs/RobotStatus</code> Battery time remaining <code>/gbplanner_node/tsdf_map_in</code> <code>voxblox_msgs/Layer</code> External TSDF map input (optional) <code>/gbplanner_node/esdf_map_in</code> <code>voxblox_msgs/Layer</code> External ESDF map input (optional)"},{"location":"planning/#planner-control-interface","title":"Planner Control Interface","text":"Topic Type Description <code>pose</code> <code>geometry_msgs/PoseWithCovarianceStamped</code> Robot pose (either pose or odom) <code>odometry</code> <code>nav_msgs/Odometry</code> Robot odometry (either pose or odom) <code>cam_pitch</code> <code>sensor_msgs/JointState</code> Actuated camera pitch angle"},{"location":"planning/#published-topics","title":"Published Topics","text":""},{"location":"planning/#trajectory-output-pci","title":"Trajectory Output (PCI)","text":"Topic Type Description <code>command/trajectory</code> <code>trajectory_msgs/MultiDOFJointTrajectory</code> Output path as a multi-dof trajectory <code>/gbplanner_path</code> <code>nav_msgs/Path</code> Output path as a path (Used in the unified autonomy stack) <code>act_cam_cmd</code> <code>std_msgs/Float64</code> Actuated camera pitch command"},{"location":"planning/#map-visualization-gbplanner","title":"Map &amp; Visualization (GBPlanner)","text":"Topic Type Description <code>/gbplanner_node/mesh</code> <code>voxblox_msgs/Mesh</code> Voxblox mesh <code>/gbplanner_node/tsdf_map_out</code> <code>voxblox_msgs/Layer</code> TSDF map output <code>/gbplanner_node/esdf_map_out</code> <code>voxblox_msgs/Layer</code> ESDF map output <code>/gbplanner_node/surface_pointcloud</code> <code>sensor_msgs/PointCloud2</code> Surface point cloud <code>vis/*</code> <code>visualization_msgs/MarkerArray</code> Planner visualization markers"},{"location":"planning/#configuration","title":"Configuration","text":"<p>Parameters are set in <code>robot_bringup/config/gbplanner/&lt;mission_name&gt;/gbplanner_config.yaml</code>.</p>"},{"location":"planning/#behavior-tree","title":"Behavior Tree","text":"<ul> <li><code>behavior_tree_path</code>: Path to mission behavior tree XML file</li> <li><code>lge_homing.xml</code>: Exploration with homing when complete or time limit reached</li> <li><code>lge_insp.xml</code>: Exploration followed by surface inspection</li> <li><code>lnav</code>: Navigate to a waypoint in unknown space</li> </ul>"},{"location":"planning/#graph-building","title":"Graph Building","text":"Parameter Description <code>PlanningParams/graph_building_mode</code> <code>kBasic</code> (default) or <code>kBatch</code> sampling <code>PlanningParams/num_vertices_max</code> Maximum graph vertices <code>PlanningParams/num_edges_max</code> Maximum graph edges <code>PlanningParams/edge_length_min/max</code> Edge length bounds (m) <code>PlanningParams/nearest_range</code> Vertex connection range (m) <code>PlanningParams/box_check_method</code> Collision check: <code>0</code>=voxel grid, <code>1</code>=sphere approx, <code>2</code>=SDF grid <code>PlanningParams/line_check_method</code> Edge check: <code>0</code>=interpolated, <code>1</code>=cuboid around edge (default), <code>2</code>/<code>3</code>=SDF variants"},{"location":"planning/#robot-geometry","title":"Robot Geometry","text":"Parameter Description <code>RobotParams/type</code> <code>kAerialRobot</code> or <code>kGroundRobot</code> <code>RobotParams/size</code> Robot dimensions <code>[x, y, z]</code> (m) <code>RobotParams/size_extension</code> Default collision margin <code>[x, y, z]</code> (m) <code>RobotParams/size_extension_min</code> Minimum collision margin <code>[x, y, z]</code> (m) <code>RobotParams/center_offset</code> Offset from cuboid center to odometry frame <code>[x, y, z]</code> <code>RobotParams/bound_mode</code> <code>0</code>=extended, <code>1</code>=relaxed, <code>2</code>=min, <code>3</code>=exact, <code>4</code>=point <code>RobotParams/safety_extension</code> Freespace corridor dimensions for path safety"},{"location":"planning/#ground-robot-specific","title":"Ground Robot Specific","text":"Parameter Description <code>PlanningParams/max_ground_height</code> Target height above ground (m) <code>PlanningParams/robot_height</code> Odometry frame height from ground (m) <code>PlanningParams/max_inclination</code> Maximum traversable slope (rad)"},{"location":"planning/#exploration-bounds","title":"Exploration Bounds","text":"Parameter Description <code>BoundedSpaceParams/Global/type</code> <code>kCuboid</code> or <code>kSphere</code> <code>BoundedSpaceParams/Global/min_val</code> Lower bound <code>[x, y, z]</code> (cuboid) <code>BoundedSpaceParams/Global/max_val</code> Upper bound <code>[x, y, z]</code> (cuboid) <code>BoundedSpaceParams/Local/*</code> Local sampling space (for <code>kBasicExploration</code>) <code>BoundedSpaceParams/LocalAdaptiveExp/*</code> Adaptive sampling space (for <code>kAdaptiveExploration</code>)"},{"location":"planning/#sensor-model","title":"Sensor Model","text":"Parameter Description <code>SensorParams/sensor_list</code> List of sensor names <code>SensorParams/&lt;name&gt;/type</code> <code>kLidar</code> or <code>kCamera</code> <code>SensorParams/&lt;name&gt;/max_range</code> Sensor range (m) <code>SensorParams/&lt;name&gt;/fov</code> Field of view <code>[h, v]</code> (rad) <code>SensorParams/&lt;name&gt;/resolution</code> Ray-cast resolution <code>[h, v]</code> (rad) <code>SensorParams/&lt;name&gt;/center_offset</code> Sensor offset from odometry <code>[x, y, z]</code> <code>SensorParams/&lt;name&gt;/rotations</code> Sensor orientation <code>[roll, pitch, yaw]</code> <code>SensorParams/&lt;name&gt;/rot_lims</code> Actuated camera limits <code>[min, max]</code> (rad)"},{"location":"planning/#exploration-gain","title":"Exploration Gain","text":"Parameter Description <code>PlanningParams/exp_sensor_list</code> Sensors for gain calculation <code>PlanningParams/free_voxel_gain</code> Weight for free voxels <code>PlanningParams/occupied_voxel_gain</code> Weight for occupied voxels <code>PlanningParams/unknown_voxel_gain</code> Weight for unknown voxels <code>PlanningParams/path_length_penalty</code> Path length cost weight <code>PlanningParams/leafs_only_for_volumetric_gain</code> Evaluate gain at leaf vertices only <code>PlanningParams/cluster_vertices_for_gain</code> Cluster vertices for gain calculation <code>PlanningParams/nonuniform_ray_cast</code> Increase ray-cast step size with distance"},{"location":"planning/#path-extraction","title":"Path Extraction","text":"Parameter Description <code>PlanningParams/traverse_length_max</code> Maximum path length (m) <code>PlanningParams/traverse_time_max</code> Maximum path execution time (s) <code>PlanningParams/path_safety_enhance_enable</code> Improve path safety <code>PlanningParams/path_interpolation_distance</code> Path interpolation step (m)"},{"location":"planning/#global-planner_1","title":"Global Planner","text":"Parameter Description <code>PlanningParams/auto_global_planner_enable</code> Trigger global planner when local exhausted <code>PlanningParams/go_home_if_fully_explored</code> Return home if no frontiers remain <code>PlanningParams/time_budget_limit</code> Mission time limit (s)"},{"location":"planning/#inspection","title":"Inspection","text":"Parameter Description <code>PlanningParams/min_coverage_percentage</code> Target surface coverage fraction <code>PlanningParams/inspection_xy_spacing</code> Viewpoint grid resolution XY (m) <code>PlanningParams/inspection_z_spacing</code> Viewpoint grid resolution Z (m) <code>PlanningParams/inspection_target_viewing_range</code> Viewpoint projection distance (m) <code>PlanningParams/max_inspection_vertices</code> Maximum viewpoints in path <code>PlanningParams/num_lkh_iterations</code> TSP solver iterations"},{"location":"planning/#mapping-voxblox","title":"Mapping (Voxblox)","text":"<p>Set in <code>config/&lt;robot_name&gt;/voxblox_config.yaml</code>:</p> Parameter Description <code>world_frame</code> Fixed frame for planning <code>tsdf_voxel_size</code> Voxel size (m) <code>truncation_distance</code> TSDF truncation (2\u20135\u00d7 voxel size) <code>max_ray_length_m</code> Maximum ray-cast distance (m)"},{"location":"planning/#planner-control-interface_1","title":"Planner Control Interface","text":"<p>Set in <code>config/&lt;robot_name&gt;/planner_control_interface_sim_config.yaml</code>:</p> Parameter Description <code>robot_type</code> <code>kAerial</code> or <code>kGround</code> <code>init_motion_enable</code> Perform initialization motion on start <code>smooth_heading_enable</code> Limit heading rate to achievable values <code>interpolate_traj</code> Interpolate trajectory in time <code>RobotDynamics/v_max</code> Maximum linear velocity (m/s) <code>RobotDynamics/v_homing_max</code> Maximum homing velocity (m/s) <code>RobotDynamics/yaw_rate_max</code> Maximum yaw rate (rad/s) <code>RobotDynamics/a_max</code> Acceleration for velocity profile (m/s\u00b2) <code>RobotDynamics/dt</code> Trajectory interpolation step (s)"},{"location":"planning/#citation","title":"Citation","text":"<p>If you use this method in your work, please cite the relevant publications:</p> <pre><code>@inproceedings{planning_gvi,\n    title = {Autonomous {Exploration} and {General} {Visual} {Inspection} of {Ship} {Ballast} {Water} {Tanks} {Using} {Aerial} {Robots}},\n    url = {https://ieeexplore.ieee.org/document/10406928},\n    doi = {10.1109/ICAR58858.2023.10406928},\n    booktitle = {2023 21st {International} {Conference} on {Advanced} {Robotics} ({ICAR})},\n    author = {Dharmadhikari, Mihir and De Petris, Paolo and Kulkarni, Mihir and Khedekar, Nikhil and Nguyen, Huan and Stene, Arnt Erik and Sj\u00f8vold, Eivind and Solheim, Kristian and Gussiaas, Bente and Alexis, Kostas},\n    month = dec,\n    year = {2023},\n    note = {ISSN: 2572-6919},\n    keywords = {Electronic ballasts, Geometry, Inspection, Robot vision systems, Storage management, Visualization, Water},\n    pages = {409--416},\n}\n\n@inproceedings{planning_gbplanner2,\n    title = {Autonomous {Teamed} {Exploration} of {Subterranean} {Environments} using {Legged} and {Aerial} {Robots}},\n    url = {https://ieeexplore.ieee.org/document/9812401},\n    doi = {10.1109/ICRA46639.2022.9812401},\n    booktitle = {2022 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},\n    author = {Kulkarni, Mihir and Dharmadhikari, Mihir and Tranzatto, Marco and Zimmermann, Samuel and Reijgwart, Victor and De Petris, Paolo and Nguyen, Huan and Khedekar, Nikhil and Papachristos, Christos and Ott, Lionel and Siegwart, Roland and Hutter, Marco and Alexis, Kostas},\n    month = may,\n    year = {2022},\n    keywords = {Legged locomotion, Navigation, Network topology, Robot kinematics, Space missions, Three-dimensional displays, Wireless communication},\n    pages = {3306--3313},\n}\n\n@article{planning_gbplanner1,\n    title = {Graph-based subterranean exploration path planning using aerial and legged robots},\n    volume = {37},\n    copyright = {\u00a9 2020 Wiley Periodicals LLC},\n    issn = {1556-4967},\n    url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21993},\n    doi = {10.1002/rob.21993},\n    number = {8},\n    urldate = {2025-11-30},\n    journal = {Journal of Field Robotics},\n    author = {Dang, Tung and Tranzatto, Marco and Khattak, Shehryar and Mascarich, Frank and Alexis, Kostas and Hutter, Marco},\n    year = {2020},\n    note = {\\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.21993},\n    keywords = {aerial robots, legged robots, path planning, subterranean robotics},\n    pages = {1363--1388},\n}\n</code></pre>"},{"location":"platforms/","title":"Platforms","text":""},{"location":"platforms/#fully-integrated-platforms","title":"Fully integrated platforms","text":""},{"location":"platforms/#unipilot","title":"UniPilot","text":"<p>UniPilot is a compact, plug-and-play hardware-software autonomy payload designed for integration across diverse robot embodiments including aerial and ground platforms. It features a multi-modal sensing suite (LiDAR, radar, vision, ToF, IMU) and an NVIDIA Jetson Orin NX compute board running complete autonomy software for SLAM, path planning, and learning-based navigation. The lightweight design enables robust autonomous operation in GPS-denied environments across multirotors, legged robots, and hybrid VTOL platforms. </p> <p></p> <p>Sensor FoV Visualization:</p> <p> </p>"},{"location":"platforms/#multirotors","title":"Multirotors","text":"<p>The complete Unified Autonomy Stack has been integrated and tested on a collision-tolerant multirotor platform equipped with the UniPilot payload.</p>"},{"location":"platforms/#legged-robots","title":"Legged Robots","text":"<p>The Unified Autonomy Stack has been integrated with the ANYmal D quadruped robot via the UniPilot payload, enabling robust autonomous perception, planning and navigation in GPS-denied environments using the full Unified Autonomy Stack.</p>"},{"location":"platforms/#handheld-unit","title":"Handheld Unit","text":"<p>The UniPilot payload running the Unified Autonomy Stack has been integrated as a handheld unit for rapid data collection across environments.</p>"},{"location":"platforms/#partial-integration","title":"Partial Integration","text":""},{"location":"platforms/#hybrid-vtol-x-wing","title":"Hybrid VTOL - x-Wing","text":"<p>The Unified Autonomy Stack has now been integrated with the x-Wing hybrid VTOL platform. The UniPilot payload provides the necessary sensing and compute capabilities to enable autonomous navigation in GNSS-denied environments.</p>"},{"location":"platforms/#underwater-platform-blue-rov","title":"Underwater Platform - Blue ROV","text":"<p>The Unified Autonomy Stack has been partially integrated with the Blue ROV underwater platform, planning and inspection capabilities underwater.</p>"},{"location":"platforms/#citation","title":"Citation","text":"<p>If you use the Unified Autonomy Stack or the UniPilot reference hardware platform in your research, please cite the following publications:</p> <pre><code>@article{unipilot,\n  title={UniPilot: Enabling GPS-Denied Autonomy Across Embodiments},\n  author={Kulkarni, Mihir and Dharmadhikari, Mihir and Khedekar, Nikhil and Nissov, Morten and Singh, Mohit and Weiss, Philipp and Alexis, Kostas},\n  journal={arXiv preprint arXiv:2509.11793},\n  year={2025}\n}\n</code></pre>"},{"location":"prior_results/","title":"Prior Results","text":"<p>Use the following color-coded tags for quick reference:</p> <ul> <li>M \u2013 Perception</li> <li>P \u2013 Planning</li> <li>S \u2013 Safety</li> <li>E \u2013 Embodied</li> </ul>"},{"location":"prior_results/#autonomous-teamed-exploration-of-subterranean-environments-using-legged-and-aerial-robots","title":"Autonomous Teamed Exploration of Subterranean Environments using Legged and Aerial Robots","text":"<p>Tags: M, P, E</p>"},{"location":"prior_results/#autonomous-underwater-exploration-inspection-in-visually-degraded-environments","title":"Autonomous Underwater Exploration &amp; Inspection in Visually-degraded Environments","text":"<p>Tags: M, P</p>"},{"location":"prior_results/#semantics-aware-predictive-inspection-path-planning-field-deployment-1","title":"Semantics-aware Predictive Inspection Path Planning: Field Deployment 1","text":"<p>Tags: P</p>"},{"location":"prior_results/#lidar-inertial-odometry-and-mapping-at-high-speeds","title":"LiDAR-Inertial Odometry and Mapping at High Speeds","text":"<p>Tags: M</p>"},{"location":"prior_results/#degradation-resilient-lidar-radar-inertial-odometry","title":"Degradation Resilient LiDAR-Radar-Inertial Odometry","text":"<p>Tags: M</p>"},{"location":"prior_results/#keyframe-based-thermal-inertial-odometry","title":"Keyframe-based Thermal-Inertial Odometry","text":"<p>Tags: M</p>"},{"location":"prior_results/#aerial-gym-simulator-a-framework-for-highly-parallelized-simulation-of-aerial-robots","title":"Aerial Gym Simulator: A Framework for Highly Parallelized Simulation of Aerial Robots","text":"<p>Tags: S</p>"},{"location":"prior_results/#reinforcement-learning-for-collision-free-flight-exploiting-deep-collision-encoding","title":"Reinforcement Learning for Collision-free Flight Exploiting Deep Collision Encoding","text":"<p>Tags: S</p>"},{"location":"prior_results/#team-cerberus-wins-the-darpa-subterranean-challenge","title":"Team CERBERUS Wins the DARPA Subterranean Challenge","text":"<p>Tags: M, P,  E</p>"},{"location":"prior_results/#neural-nmpc-through-signed-distance-field-encoding-for-collision-avoidance","title":"Neural NMPC through Signed Distance Field Encoding for Collision Avoidance","text":"<p>Tags: S, M</p>"},{"location":"prior_results/#graph-based-exploration-path-planning-aerial-robot-inside-an-underground-mine","title":"Graph-based Exploration Path Planning - Aerial Robot inside an Underground Mine","text":"<p>Tags: M, P</p>"},{"location":"rl/","title":"Deep Reinforcement Learning (DRL)","text":"<p>The Deep Reinforcement Learning (DRL) navigation module employs policies that utilize both the robot odometry and instantaneous raw exteroceptive depth information from depth cameras or LiDAR sensors. The method exploits local ego-centric representations of the goal location and commands reference body velocity or acceleration (WIP) setpoints. The sensor can be compressed into a compact latent representation using a neural encoder. Alternatively, raw images can be downsampled and processed with convolutional layers. Policies are trained in simulation using the Aerial Gym Simulator. </p> <p>This method enables safe collision-free navigation in complex environments without a map, and thus provides an add-on safety layer to traditional map-based planning approaches, offering resilience when the onboard SLAM experiences drift or when certain obstacles are not mapped sufficiently.</p> <p>Source Code</p> <ul> <li>Workspace: <code>workspaces/ws_rl/src</code></li> <li>Package: <code>rl_nav</code></li> <li>GitHub: ntnu-arl/rl_nav</li> </ul> <p>Related Publications:</p> <ul> <li>M. Kulkarni, W. Rehberg and K. Alexis, \"Aerial Gym Simulator: A Framework for Highly Parallelized Simulation of Aerial Robots,\" in IEEE Robotics and Automation Letters, vol. 10, no. 4, pp. 4093-4100, April 2025.</li> <li>Kulkarni, Mihir, and Kostas Alexis. \"Reinforcement learning for collision-free flight exploiting deep collision encoding.\" 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024.</li> </ul> Aerial Gym Simulator <p>The Aerial Gym Simulator is used to train and evaluate the RL navigation policy. The training environments and policies used for this task are open-sourced.</p>"},{"location":"rl/#observations","title":"Observations","text":"<ul> <li>\\([\\mathbf{e}_p, \\phi, \\theta, e_{\\psi}, \\mathbf{v}, \\boldsymbol{\\omega}, \\mathbf{a}_{t-1}, \\mathbf{C}]\\)<ul> <li>\\(\\mathbf{e}_p\\) is the position error to the goal in the body frame.</li> <li>\\(\\phi, \\theta\\) are roll and pitch angles.</li> <li>\\(e_{\\psi}\\) is the yaw error to the goal direction.</li> <li>\\(\\mathbf{v}, \\boldsymbol{\\omega}\\) are linear and angular velocities in the body frame.</li> <li>\\(\\mathbf{a}_{t-1}\\) is the previous action taken by the agent.</li> </ul> </li> <li>\\(\\mathbf{C}\\) is a representation of the sensor data<ul> <li>\\(\\mathbf{C}\\) is the downsampled LiDAR range image (16 \u00d7 20 grid) in the case for LiDAR-based navigation.</li> <li>\\(\\mathbf{C}\\) is the compressed latent representation encoding collision information when using a VAE-based encoder and a depth camera.</li> </ul> </li> </ul> <p>where \\(\\mathbf{e}_p\\) is the position error to the goal in the body frame, \\(\\phi, \\theta\\) are roll and pitch angles, \\(\\mathbf{e}_{\\psi}\\) is the yaw error to the goal direction, \\(\\mathbf{v}, \\boldsymbol{\\omega}\\) are linear and angular velocities in the body frame, and \\(\\mathbf{a}_{t-1}\\) is the previous action taken by the agent.</p>"},{"location":"rl/#actions-and-bounds","title":"Actions and Bounds","text":"<p>The policy outputs a normalized action vector \\(\\mathbf{a}_t \\in [-1,1]^4\\), which is mapped to body-frame velocity commands \\([v_x, v_y, v_z, \\dot{\\psi}] \\in \\mathbb{R}^4\\):</p> \\[v_x = -(a_0 + 1), \\quad v_y = a_1, \\quad v_z = a_2, \\quad \\dot{\\psi} = a_3 \\cdot \\tfrac{\\pi}{3}\\] <p>This mapping produces velocities \\(v_x \\in [-2, 0]\\) m/s (The LiDAR is mounted backwards), lateral/vertical velocities \\(v_y, v_z \\in [-1, 1]\\) m/s, and yaw rate \\(\\dot{\\psi} \\in [-\\frac{\\pi}{3}, \\frac{\\pi}{3}]\\) rad/s.</p>"},{"location":"rl/#reward-function","title":"Reward Function","text":"<p>The reward function encourages goal progress, directional alignment, stability, and safety through shaped rewards and penalties. All rewards are combined and scaled by curriculum progress.</p> <p>Helper Functions:</p> <ul> <li>Exponential reward: \\(\\mathcal{R}(m, e, x) = m \\cdot \\exp(-e \\cdot x^2)\\)</li> <li>Exponential penalty: \\(\\mathcal{P}(m, e, x) = m \\cdot (\\exp(-e \\cdot x^2) - 1)\\)</li> </ul> Detailed Reward Components Category Term Formula Description Attraction to Goal \\(R_{\\text{pos}}\\) \\(\\mathcal{R}(3, 1, d)\\) Exponential reward based on distance to goal \\(R_{\\text{close}}\\) \\(\\mathcal{R}(5, 8, d) \\cdot \\alpha_{\\psi}\\) Bonus when very close, gated by yaw alignment \\(R_{\\text{dist}}\\) \\(\\frac{20-d}{20}\\) Linear reward for reducing distance Direction Alignment \\(R_{\\text{dir}}\\) \\(\\hat{\\mathbf{v}} \\cdot \\hat{\\mathbf{p}}_{\\text{goal}} \\cdot R_{\\text{vel}} \\cdot \\min(\\frac{d}{3}, 1)\\) Velocity direction toward goal, scaled by distance \\(R_{\\text{vel}}\\) \\(\\mathcal{R}(2, 2, \\|\\mathbf{v}\\|-2)\\) Reward for desired velocity (~2 m/s) \\(\\alpha_{\\psi}\\) \\(\\mathcal{R}(1, 2, e_{\\psi})\\) Yaw alignment factor Stability at Goal \\(R_{\\text{stable}}\\) \\(R_{\\text{low_vel}} + R_{\\psi} + R_{\\text{low}_\\omega}\\) Active only when \\(d &lt; 1\\) m \\(R_{\\text{low_vel}}\\) \\(\\mathcal{R}(1.5, 10, \\|\\mathbf{v}\\|) + \\mathcal{R}(1.5, 0.5, \\|\\mathbf{v}\\|)\\) Low velocity reward \\(R_{\\psi}\\) \\(\\mathcal{R}(2, 0.2, e_{\\psi}) + \\mathcal{R}(4, 15, e_{\\psi})\\) Correct yaw orientation \\(R_{\\text{low}_\\omega}\\) \\(\\mathcal{R}(1.5, 5, \\omega_z) \\cdot \\alpha_{\\psi}\\) Low yaw rate, gated by alignment Velocity Penalties \\(P_{\\text{vel}}\\) \\(\\mathcal{P}(2, 2, \\|\\mathbf{v}\\|-3)\\) Penalize speeds \\(&gt; 3\\) m/s \\(P_{\\text{bwd}}\\) \\(\\mathcal{P}(2, 8, v_x)\\) Penalize positive \\(v_x\\) (backward motion) Action Smoothness \\(P_{\\text{diff},i}\\) \\(\\mathcal{P}(0.1, 5, a_{i,t} - a_{i,t-1})\\) Smooth action transitions Absolute Action \\(P_{\\text{abs},x/y}\\) \\(C \\cdot \\mathcal{P}(0.1, 0.3, a_i)\\) Penalize large \\(x\\)/\\(y\\) actions \\(P_{\\text{abs},z}\\) \\(C \\cdot \\mathcal{P}(0.15, 1.0, a_z)\\) Penalize large \\(z\\) action \\(P_{\\text{abs},\\dot\\psi}\\) \\(C \\cdot \\mathcal{P}(0.15, 2.0, a_{\\dot\\psi})\\) Penalize large yaw rate action Safety \\(P_{\\text{TTC}}\\) \\(-\\mathcal{R}(3, 1, \\text{TTC})\\) Time-to-collision penalty \\(P_{\\text{crash}}\\) \\(-10\\) Collision penalty <p>where \\(d\\) is distance to goal, \\(e_{\\psi}\\) is yaw error.</p> <p>Time-to-Collision (TTC):</p> <p>For each LiDAR ray with unit direction \\(\\hat{\\mathbf{u}}_{\\text{ray}}\\), range \\(r\\), and robot velocity \\(\\mathbf{v}\\):</p> \\[\\text{TTC}_{\\text{ray}} = \\begin{cases} \\frac{r}{\\mathbf{v} \\cdot \\hat{\\mathbf{u}}_{\\text{ray}}} &amp; \\text{if approaching} \\\\ +\\infty &amp; \\text{otherwise} \\end{cases}\\] <p>The TTC metric is the minimum across all rays, encouraging the agent to maintain safe distance.</p> <p>Total Reward:</p> \\[R_{\\text{total}} = \\gamma \\left( R_{\\text{pos}} + R_{\\text{close}} + R_{\\text{dir}} + R_{\\text{dist}} + R_{\\text{stable}} + P_{\\text{vel}} + P_{\\text{action}} + P_{\\text{TTC}} \\right)\\] <p>where \\(\\gamma = 1 + 2 C_{\\text{prog}}\\) scales rewards based on curriculum progress \\(C_{\\text{prog}} \\in [0,1]\\).</p>"},{"location":"rl/#rl-topics-interfaces","title":"RL Topics &amp; Interfaces","text":""},{"location":"rl/#input","title":"Input","text":"Topic Type Description <code>/msf_core/odometry</code> <code>nav_msgs/Odometry</code> State estimate from MSF <code>/target</code> <code>geometry_msgs/PoseStamped</code> Target position from planner or user <code>/gbplanner_path</code> <code>nav_msgs/Path</code> Path from planner; last pose extracted as target <code>/rslidar_points</code> <code>sensor_msgs/PointCloud2</code> LiDAR point cloud <code>/reset</code> <code>std_msgs/Empty</code> Reset policy hidden state and action filter <code>/rmf/mavros/state</code> <code>mavros_msgs/State</code> MAVROS state (optional, for OFFBOARD check)"},{"location":"rl/#output","title":"Output","text":"Topic Type Description <code>/cmd_vel</code> <code>geometry_msgs/Twist</code> Velocity setpoint for trajectory tracker <code>/cmd_vel_filtered</code> <code>geometry_msgs/Twist</code> EMA-filtered velocity command <code>/mavros/setpoint_raw/local</code> <code>mavros_msgs/PositionTarget</code> Direct velocity command to PX4 <code>/cmd_vel_viz</code> <code>geometry_msgs/TwistStamped</code> Velocity command (visualization)"},{"location":"rl/#rl-configuration","title":"RL Configuration","text":"<p>Configuration defined in the <code>Config</code> class within <code>lidar_nav_vel_ROS 2_node.py</code>.</p>"},{"location":"rl/#observation-space","title":"Observation Space","text":"Parameter Description <code>STATE_DIM</code> State observation dimension (17): \\([e_p, \\phi, \\theta, e_{\\psi}, \\mathbf{v}, \\boldsymbol{\\omega}, \\mathbf{a}_{t-1}]\\) <code>LIDAR_DIM</code> Downsampled LiDAR grid size (16 \u00d7 20 = 320) <code>TOTAL_OBS_DIM</code> Total observation dimension (337)"},{"location":"rl/#lidar-processing","title":"LiDAR Processing","text":"Parameter Description <code>IMAGE_HEIGHT</code> Binned LiDAR height (48) <code>IMAGE_WIDTH</code> Binned LiDAR width (480) <code>LIDAR_MAX_RANGE</code> Maximum range clipping (m, typically 10.0) <code>LIDAR_MIN_RANGE</code> Minimum range clipping (m, typically 0.4) <code>MEDIAN_FILTER</code> Enable median filtering for noise removal (bool) <code>MEDIAN_FILTER_KERNEL_SIZE</code> Median filter kernel size (typically 7)"},{"location":"rl/#action-space","title":"Action Space","text":"Parameter Description <code>ACTION_DIM</code> Action dimension (4: vx, vy, vz, yaw_rate) <code>ACTION_SCALE</code> Scaling factors for actions <code>[1.0, 1.0, 0.75, 1.0]</code>"},{"location":"rl/#control","title":"Control","text":"Parameter Description <code>ACTION_FILTER_ALPHA</code> EMA filter coefficient (0.3) <code>USE_MAVROS_STATE</code> Enable MAVROS state checking (bool) <code>DEVICE</code> Inference device (<code>cuda:0</code> or <code>cpu</code>)"},{"location":"rl/#frame-ids","title":"Frame IDs","text":"Parameter Description <code>BODY_FRAME_ID</code> Body frame for visualization (<code>mimosa_body</code>)"},{"location":"rl/#citation","title":"Citation","text":"<p>If you use the RL navigation module in your work, please cite the relevant publication:</p> <pre><code>@inproceedings{navigation_drl,\n    title = {Reinforcement {Learning} for {Collision}-free {Flight} {Exploiting} {Deep} {Collision} {Encoding}},\n    url = {https://ieeexplore.ieee.org/document/10610287},\n    doi = {10.1109/ICRA57147.2024.10610287},\n    booktitle = {2024 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},\n    author = {Kulkarni, Mihir and Alexis, Kostas},\n    month = may,\n    year = {2024},\n}\n\n@article{navigation_aerialgym,\n    title = {Aerial {Gym} {Simulator}: {A} {Framework} for {Highly} {Parallelized} {Simulation} of {Aerial} {Robots}},\n    volume = {10},\n    issn = {2377-3766},\n    shorttitle = {Aerial {Gym} {Simulator}},\n    url = {https://ieeexplore.ieee.org/document/10910148/},\n    doi = {10.1109/LRA.2025.3548507},\n    number = {4},\n    journal = {IEEE Robotics and Automation Letters},\n    author = {Kulkarni, Mihir and Rehberg, Welf and Alexis, Kostas},\n    month = apr,\n    year = {2025},\n}\n\n@inproceedings{navigation_dce,\n    address = {Cham},\n    title = {Task-{Driven} {Compression} for\u00a0{Collision} {Encoding} {Based} on\u00a0{Depth} {Images}},\n    isbn = {978-3-031-47966-3},\n    doi = {10.1007/978-3-031-47966-3_20},\n    booktitle = {Advances in {Visual} {Computing}},\n    publisher = {Springer Nature Switzerland},\n    author = {Kulkarni, Mihir and Alexis, Kostas},\n    editor = {Bebis, George and Ghiasi, Golnaz and Fang, Yi and Sharf, Andrei and Dong, Yue and Weaver, Chris and Leo, Zhicheng and LaViola Jr., Joseph J. and Kohli, Luv},\n    year = {2023},\n    pages = {259--273},\n}\n</code></pre>"},{"location":"simulation/","title":"Simulation","text":"<p>The Simulation environment utilizes Gazebo and Aerial Gym Simulator. Gazebo is used to test and validate the complete autonomy stack in realistic 3D environments, while Aerial Gym Simulator provides a massively parallelized environment for training data-driven and learning-based methods.</p>"},{"location":"simulation/#gazebo","title":"Gazebo","text":"<p>We provide ROS 2 simulators for multi-rotor and differntial drive wheeled robot integrated with the Unified Autonomy Stack. Both simulators are located in the <code>unified_autonomy_stack/workspaces/ws_sim</code> workspace.</p>"},{"location":"simulation/#multirotor-simulator","title":"Multirotor Simulator","text":"<p>The multirotor simulator consists of the ROS 2 package <code>unified_autonomy_stack/workspaces/ws_sim/src/rmf_gz</code>. The simulator provides two robots each carrying a color camera, a depth camera, and a LiDAR sensor. The difference is in the type of lidar onboard the robot. For both robots, the following sensors and control options are setup. <code>&lt;ns&gt;</code> referes to the namespace for that robot.</p> <p>Sensors:</p> Sensor Topic Data type Rate (Hz) Color camera <code>/&lt;ns&gt;/cam/rgb</code> <code>sensor_msgs/msg/Image</code> 10 Depth camera (Image) <code>/&lt;ns&gt;/cam/depth</code> <code>sensor_msgs/msg/Image</code> 30 Depth camera (Point Cloud) <code>/&lt;ns&gt;/cam/pc</code> <code>sensor_msgs/msg/PointCloud2</code> 30 LiDAR <code>/&lt;ns&gt;/lidar/points</code> <code>sensor_msgs/msg/PointCloud2</code> 10 <p>Control Options</p> Command Type Topic Data type Velocity <code>/&lt;ns&gt;/cmd/vel</code> <code>geometry_msgs/msg/Twist</code> Acceleration <code>/&lt;ns&gt;/cmd/vel</code> <code>geometry_msgs/msg/Twist</code>"},{"location":"simulation/#sensor-configuration-1-traditional-lidar","title":"Sensor configuration 1: Traditional LiDAR","text":"<p>Robot namespace (<code>&lt;ns&gt;</code>): <code>rmf</code></p> <p></p> <p>LiDAR configuration:</p> <ul> <li>Field of View: <code>360 x 90 deg</code></li> <li>Orientation:  </li> </ul>"},{"location":"simulation/#sensor-configuration-2-dome-lidar","title":"Sensor configuration 2: Dome LiDAR","text":"<p>Robot namespace (<code>&lt;ns&gt;</code>): <code>rmf_unipilot</code></p> <p></p> <p>LiDAR configuration:</p> <ul> <li>Field of View: Dome <code>360 x 90 deg</code></li> <li>Orientation:  </li> </ul>"},{"location":"simulation/#wheeled-robot-simulator","title":"Wheeled Robot Simulator","text":"<p>The wheeled robot simulator consists of the ROS 2 package <code>unified_autonomy_stack/workspaces/ws_sim/src/ugv_gz</code>. The simulator provides a differential drive wheeled robot carrying a color camera, a depth camera, and a LiDAR sensor. </p> <p></p> <p>The following sensors and control options are setup.</p> <p>Sensors:</p> Sensor Topic Data type Rate (Hz) Color camera <code>smb_arl/cam/rgb</code> <code>sensor_msgs/msg/Image</code> 10 Depth camera (Image) <code>smb_arl/cam/depth</code> <code>sensor_msgs/msg/Image</code> 30 Depth camera (Point Cloud) <code>smb_arl/cam/pc</code> <code>sensor_msgs/msg/PointCloud2</code> 30 LiDAR <code>smb_arl/lidar/points</code> <code>sensor_msgs/msg/PointCloud2</code> 10 <p>Control Options</p> Command Type Topic Data type Velocity <code>smb_arl/cmd/vel</code> <code>geometry_msgs/msg/Twist</code>"},{"location":"simulation/#aerial-gym-simulator","title":"Aerial Gym Simulator","text":"<p>The Aerial Gym Simulator is an open-source simulator for massively parallelized simulation of multirotor platforms.</p> <p>To train policies for the autonomy stack, a dedicated training task description is set up within the simulator and can be found [here]. This includes the environment setup, reward formulation, obstacle management and episode management. The training task uses a simulated quadrotor platform with a dome LiDAR sensor placed, facing backwards.</p>"},{"location":"slam/","title":"Multi-Modal SLAM","text":"<p>The perception layer provides robust state estimation and mapping through MIMOSA (Multi-modal Inertial Odometry and SLAM), a factor graph-based fusion framework designed for robust performance across challenging environments.</p> <p>Source Code</p> <ul> <li>Workspace: <code>workspaces/ws_mimosa/src</code></li> <li>Package: <code>mimosa</code></li> <li>GitHub: ntnu-arl/mimosa</li> </ul> <p>Related Publications:</p> <ul> <li>Khedekar, N. and Alexis, K. \"PG-LIO: Photometric-Geometric fusion for Robust LiDAR-Inertial Odometry.\" arXiv preprint arXiv:2506.18583, 2025.</li> <li>Nissov, M., Edlund, J.A., Spieler, P., Padgett, C., Alexis, K., and Khattak, S. \"Robust High-Speed State Estimation for Off-Road Navigation Using Radar Velocity Factors.\" IEEE Robotics and Automation Letters, vol. 9, no. 12, pp. 11146-11153, 2024.</li> <li>Nissov, M., Khattak, S., Edlund, J.A., Padgett, C., Alexis, K., and Spieler, P. \"ROAMER: Robust Offroad Autonomy using Multimodal State Estimation with Radar Velocity Integration.\" 2024 IEEE Aerospace Conference, pp. 1-10, 2024.</li> <li>Nissov, M., Khedekar, N., and Alexis, K. \"Degradation Resilient LiDAR-Radar-Inertial Odometry.\" 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 8587-8594, 2024.</li> </ul>"},{"location":"slam/#system-overview","title":"System Overview","text":"<pre><code>graph TB\n    subgraph Sensors\n        Lidar[\"LiDAR\"]\n        IMU[\"IMU\"]\n        Radar[\"FMCW Radar\"]\n        Cameras[\"Cameras\"]\n    end\n\n    Lidar --&gt;|\"Point Cloud&lt;br/&gt;(10-20 Hz)\"| Deskew[\"Motion Compensation&lt;br/&gt;and Filtering\"]\n\n    fanout(( )):::invis\n    IMU ---|\"Acceleration/Rate&lt;br/&gt;(200 Hz)\"| fanout\n    fanout --&gt; Deskew\n    fanout --&gt; VIO\n    fanout --&gt; RadarFront\n\n    Deskew --&gt;|\"Filtered &amp; Deskewed Cloud\"| ScanMatch[\"Point-to-plane Associations\"]\n\n    Radar --&gt;|\"Point Cloud&lt;br/&gt;(10-30 Hz)\"| RadarFront[\"Filter and Assemble&lt;br/&gt;Doppler Factor\"]\n\n    Cameras --&gt;|\"Images&lt;br/&gt;(10-30 Hz)\"| VIO[\"Visual-Inertial&lt;br/&gt;Odometry\"]\n\n    ScanMatch --&gt;|\"LiDAR Factors\"| Backend[\"Factor Graph\"]\n    fanout --&gt; Backend\n    RadarFront --&gt;|\"Radar Factors\"| Backend\n    VIO --&gt;|\"Odometry\"| OdomBuffer[\"Odometry Buffer\"]\n    OdomBuffer --&gt;|\"Odometry Factor&lt;br/&gt;(WIP)\"| Backend\n\n    Backend --&gt;|\"State Estimate\"| EstOutput[\"Odometry&lt;br/&gt;Biases\"]\n    Backend --&gt;|\"LiDAR/Radar Point Clouds\"| CloudOutput[\"Full-resolution LiDAR Cloud&lt;br/&gt;Downsampled LiDAR Cloud&lt;br/&gt;Filtered Radar Cloud\"]\n\n    classDef sensor fill:#e0e7ff30,stroke:#6366f1,stroke-width:2px,color:#000\n    classDef frontend fill:#fef3c730,stroke:#eab308,stroke-width:2px,color:#000\n    classDef backend fill:#d1fae530,stroke:#10b981,stroke-width:2px,color:#000\n    classDef output fill:#cffafe30,stroke:#06b6d4,stroke-width:2px,color:#000\n\n    class Lidar,Radar,IMU,Cameras sensor\n    class Deskew,ScanMatch,RadarFront,VIO,OdomBuffer frontend\n    class Backend backend\n    class EstOutput,CloudOutput output</code></pre>"},{"location":"slam/#sensors","title":"Sensors","text":"<p>The stack fuses multiple sensor modalities for robust perception in diverse environments:</p>"},{"location":"slam/#lidar-ouster-os0-128-or-robosense-rs-airy","title":"LiDAR (Ouster OS0-128) or (RoboSense RS-AIRY)","text":"<ul> <li>Frequency: 10-20 Hz (configurable)</li> <li>Output: Point cloud with per-point:<ul> <li>3D position</li> <li>Timestamp</li> </ul> </li> </ul>"},{"location":"slam/#radar-ti-mmwave","title":"Radar (TI mmWave)","text":"<ul> <li>Type: Millimeter-wave radar</li> <li>Frequency: Variable (typically 10-30 Hz)</li> <li>Output: Point cloud with per-point:<ul> <li>3D Position</li> <li>Radial speed</li> </ul> </li> </ul>"},{"location":"slam/#inertial-measurement-unit-imu","title":"Inertial Measurement Unit (IMU)","text":"<ul> <li>Model: VectorNav VN-100 (or equivalent)</li> <li>Frequency: 200 Hz</li> <li>Outputs:<ul> <li>Specific force</li> <li>Angular velocity</li> </ul> </li> </ul>"},{"location":"slam/#cameras-experimental","title":"Cameras (Experimental)","text":"<ul> <li>Frequency: Variable</li> <li>Output: RBG or grayscale</li> </ul>"},{"location":"slam/#factor-graph-optimization","title":"Factor Graph Optimization","text":"<p>The backend maintains a sliding-window factor graph for calculating the state estimate, using the fixed lag smoother from the GTSAM library.</p> <p>State Space:</p> \\[ \\mathbf{x}_k = \\begin{bmatrix} \\boldsymbol{p}_k, &amp;\\boldsymbol{q}_k, &amp;\\boldsymbol{v}_k, &amp;\\boldsymbol{b}_k^a, &amp;\\boldsymbol{b}_k^\\omega,   &amp;\\boldsymbol{g} \\end{bmatrix}^{\\top} \\] <p>where \\(\\boldsymbol{p}_k\\) is the position, \\(\\boldsymbol{q}_k\\) is the attitude quaternion, \\(\\boldsymbol{v}_k\\) is the linear velocity, \\(\\boldsymbol{b}_k^a\\), \\(\\boldsymbol{b}_k^\\omega\\) are accelerometer and gyroscope biases, and \\(\\boldsymbol{g}\\) is the gravity direction.</p> <p>Factor Types:</p> <ol> <li> <p>Prior Factors:</p> <ul> <li>Initializing entire state space</li> <li>Relevant parameters: <code>graph/smoother/initial_*</code></li> </ul> </li> <li> <p>IMU Preintegration Factors (between exteroceptive measurements):</p> <ul> <li>Links consecutive graph nodes using integrated IMU measurements</li> <li>Relevant parameters: <code>imu/*</code></li> </ul> </li> <li> <p>LiDAR Odometry Factors (point-to-plane residuals):</p> <ul> <li>Unary Hessian factor containing all the scan-map point-to-plane residuals</li> <li>Relevant parameters: <code>lidar/*</code></li> </ul> </li> <li> <p>Radar Factors (radial speed residuals)</p> <ul> <li>Unary Hessian factor containing all the radial speed residuals</li> <li>Relevant parameters: <code>radar/*</code></li> </ul> </li> <li> <p>External Odometry (experimental):</p> <ul> <li>Binary factor encoding relative transform between two pose estimates from an external odometry sources (e.g., from visual-inertial odometry)</li> <li>Relevant parameters: <code>odometry/*</code></li> </ul> </li> </ol>"},{"location":"slam/#topics","title":"Topics","text":"<p>Topics are remapped in the launch file. Note, all topics are in the <code>/mimosa_node</code> namespace.</p>"},{"location":"slam/#input","title":"Input","text":"Topic Type Source <code>/imu/manager/imu_in</code> <code>sensor_msgs/Imu</code> IMU driver (e.g., <code>vectornav_driver</code>) <code>/lidar/manager/lidar_in</code> <code>sensor_msgs/PointCloud2</code> LiDAR driver (e.g., <code>ouster_ros</code>, <code>rslidar_sdk</code>) <code>/radar/manager/radar_in</code> <code>sensor_msgs/PointCloud2</code> Radar driver (e.g., <code>ti_mmwave_rospkg</code>); fields: [x, y, z, radial_speed] <code>/odometry/manager/odometry_in</code> <code>nav_msgs/Odometry</code> External odometry (e.g., VIO from rovio)"},{"location":"slam/#output","title":"Output","text":"Topic Type Description <code>/graph/odometry</code> <code>nav_msgs/Odometry</code> Graph-rate odometry; provided to MSF to create IMU-rate estimates <code>/graph/path</code> <code>nav_msgs/Path</code> Graph-rate path; visualization <code>/graph/optimized_path</code> <code>nav_msgs/Path</code> Smoother window path; visualization <code>/lidar/manager/points_full_res</code> <code>sensor_msgs/PointCloud2</code> Deskewed full-resolution LiDAR; visualization <code>/lidar/geometric/sm_cloud_ds</code> <code>sensor_msgs/PointCloud2</code> Downsampled deskewed LiDAR; provided to planner <code>/radar/manager/filtered_points</code> <code>sensor_msgs/PointCloud2</code> Filtered radar points; visualization"},{"location":"slam/#configuration-tuning","title":"Configuration &amp; Tuning","text":"<p>Parameters most likely to need changes for a new system. Noise parameters are assumed to be calibrated (e.g., Allan Variance for IMU) or manually tuned.</p>"},{"location":"slam/#graph-optimization","title":"Graph Optimization","text":"<p>All parameters use the <code>graph/smoother</code> prefix.</p>"},{"location":"slam/#initial-covariance","title":"Initial Covariance","text":"Parameter Description <code>initial_position_sigma</code> Position standard deviation (m) <code>initial_rotation_yaw_sigma_deg</code> Yaw standard deviation (deg) <code>initial_rotation_pitch_roll_sigma_deg</code> Roll/pitch standard deviation (deg) <code>initial_velocity_sigma</code> Linear velocity standard deviation (m/s) <code>initial_bias_acc_sigma</code> Accelerometer bias standard deviation (m/s\u00b2) <code>initial_bias_gyro_sigma</code> Gyroscope bias standard deviation (rad/s) <code>initial_gravity_sigma</code> Gravity direction standard deviation"},{"location":"slam/#optimization","title":"Optimization","text":"Parameter Description <code>lag</code> Fixed lag smoother temporal duration (s) <code>relinearize_threshold_translation</code> Position relinearization threshold (m) <code>relinearize_threshold_rotation</code> Attitude relinearization threshold (rad) <code>relinearize_threshold_velocity</code> Velocity relinearization threshold (m/s) <code>relinearize_threshold_bias_acc</code> Accelerometer bias relinearization threshold (m/s\u00b2) <code>relinearize_threshold_bias_gyro</code> Gyroscope bias relinearization threshold (rad/s) <code>relinearize_threshold_gravity</code> Gravity relinearization threshold <code>additional_update_iterations</code> Additional Gauss-Newton iterations (int)"},{"location":"slam/#extrinsic-calibration","title":"Extrinsic Calibration","text":"<p>For each source in [<code>lidar</code>, <code>radar</code>, <code>odometry</code>]:</p> Parameter Description <code>&lt;source&gt;/T_B_S</code> Transform from source to body frame: <code>[x, y, z, qx, qy, qz, qw]</code>"},{"location":"slam/#imu","title":"IMU","text":"<p>All parameters use the <code>imu/preintegration</code> prefix.</p> Parameter Description <code>acc_noise_density</code> Accelerometer noise density (m/(s\u00b2\u221aHz)) <code>acc_bias_random_walk</code> Accelerometer bias random walk (m/(s\u00b3\u221aHz)) <code>gyro_noise_density</code> Gyroscope noise density (rad/(s\u221aHz)) <code>gyro_bias_random_walk</code> Gyroscope bias random walk (rad/(s\u00b2\u221aHz))"},{"location":"slam/#lidar","title":"LiDAR","text":"<p>Parameters with <code>lidar/manager</code> prefix:</p> Parameter Description <code>range_min</code> Minimum range (m) <code>range_max</code> Maximum range (m) <code>create_full_res_pointcloud</code> Publish full-resolution point cloud (bool) <code>full_res_pointcloud_publish_rate_divisor</code> Full-res publish rate divisor (uint) <p>Parameters with <code>lidar/geometric</code> prefix:</p> Parameter Description <code>map_keyframe_trans_thresh</code> Keyframe translation threshold (m) <code>map_keyframe_rot_thresh_deg</code> Keyframe rotation threshold (deg) <code>scan_to_map/lidar_point_noise_std_dev</code> Point-to-plane residual std dev (m)"},{"location":"slam/#radar","title":"Radar","text":"<p>All parameters use the <code>radar/manager</code> prefix.</p> Parameter Description <code>range_min</code> Minimum range (m) <code>range_max</code> Maximum range (m) <code>threshold_azimuth_deg</code> Maximum absolute azimuth (deg) <code>threshold_elevation_deg</code> Maximum absolute elevation (deg) <code>frame_ms</code> Chirp duration (ms) <code>noise_sigma</code> Doppler residual noise std dev (m/s)"},{"location":"slam/#external-odometry","title":"External Odometry","text":"<p>All parameters use the <code>odometry/manager</code> prefix.</p> Parameter Description <code>sigma_trans_m</code> Translation noise std dev (m) <code>sigma_rot_deg</code> Rotation noise std dev (deg)"},{"location":"slam/#citation","title":"Citation","text":"<p>If you use this method in your work, please cite the relevant publications:</p> <pre><code>@misc{perception_pglio,\n    title = {{PG}-{LIO}: {Photometric}-{Geometric} fusion for {Robust} {LiDAR}-{Inertial} {Odometry}},\n    shorttitle = {{PG}-{LIO}},\n    url = {http://arxiv.org/abs/2506.18583},\n    doi = {10.48550/arXiv.2506.18583},\n    publisher = {arXiv},\n    author = {Khedekar, Nikhil and Alexis, Kostas},\n    month = jun,\n    year = {2025},\n    note = {arXiv:2506.18583 [cs]},\n    keywords = {Computer Science - Robotics},\n}\n\n@ARTICLE{perception_jplRadar,\n    author={Nissov, Morten and Edlund, Jeffrey A. and Spieler, Patrick and Padgett, Curtis and Alexis, Kostas and Khattak, Shehryar},\n    journal={IEEE Robotics and Automation Letters}, \n    title={Robust High-Speed State Estimation for Off-Road Navigation Using Radar Velocity Factors}, \n    year={2024},\n    volume={9},\n    number={12},\n    pages={11146-11153},\n    keywords={Radar;Sensors;Velocity measurement;Radar measurements;Laser radar;Odometry;State estimation;Robustness;Robot sensing systems;Field robots;localization;sensor fusion},\n    doi={10.1109/lra.2024.3486189}\n}\n\n@inproceedings{perception_roamer,\n    title = {{ROAMER}: {Robust} {Offroad} {Autonomy} using {Multimodal} {State} {Estimation} with {Radar} {Velocity} {Integration}},\n    shorttitle = {{ROAMER}},\n    url = {https://ieeexplore.ieee.org/document/10521170},\n    doi = {10.1109/AERO58975.2024.10521170},\n    booktitle = {2024 {IEEE} {Aerospace} {Conference}},\n    author = {Nissov, Morten and Khattak, Shehryar and Edlund, Jeffrey A. and Padgett, Curtis and Alexis, Kostas and Spieler, Patrick},\n    month = mar,\n    year = {2024},\n    note = {ISSN: 1095-323X},\n    keywords = {Hardware, Laser radar, Radar measurements, Robustness, Sensor systems, Sensors, Vehicle driving},\n    pages = {1--10},\n}\n\n@inproceedings{perception_dlrio,\n    title = {Degradation {Resilient} {LiDAR}-{Radar}-{Inertial} {Odometry}},\n    url = {https://ieeexplore.ieee.org/document/10611444},\n    doi = {10.1109/ICRA57147.2024.10611444},\n    booktitle = {2024 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},\n    author = {Nissov, Morten and Khedekar, Nikhil and Alexis, Kostas},\n    month = may,\n    year = {2024},\n    keywords = {Degradation, Estimation, Laser radar, Odometry, Prevention and mitigation, Robot sensing systems, Sensors},\n    pages = {8587--8594},\n}\n</code></pre>"},{"location":"vlm/","title":"Visual Language Model (VLM) Reasoning","text":"<p>The system currently integrates two complementary vision-language model (VLM) modalities:</p> <ul> <li>Open-vocabulary object detection with 3D spatial grounding.</li> <li>Binary visual question-answering (Yes/No) with reasoning. </li> </ul> <p>In their combination, these capabilities collectively enable semantic scene understanding and reasoning from visual data.</p> <p>Source Code</p> <ul> <li>Workspace: <code>workspaces/ws_vlm/src</code></li> <li>Package: <code>detection_vlm</code></li> <li>GitHub: ntnu-arl/detection_vlm</li> </ul>"},{"location":"vlm/#system-overview","title":"System Overview","text":"<p>The VLM stack is designed to run alongside the core perception and planning modules, providing semantic annotations and reasoning.</p> <p>The following diagram describes the entities in the VLM system along with the data exhange within and outside the system.</p> <pre><code>graph TB\n\n    FC[\"Front Camera\"]\n    SLAM[\"Multi-modal SLAM\"]\n\n    Q&amp;AVLM[\"Q&amp;A VLM\"]\n\n    subgraph Detection VLM\n        VoxelGrid[\"Voxel Grid Map\"]\n        VLM[\"VLM\"]\n        3D[\"3D projection &amp; clustering\"]\n    end\n\n    UI[\"User Interface\"]\n\n    2DDets[\"2D Detections\"]\n    3DDets[\"3D Detections\"]\n    ReasonOutput[\"Reasoning Output\"]\n\n    SLAM --&gt;|\"LiDAR Cloud\"| VoxelGrid\n    SLAM --&gt;|\"Odometry\"| VoxelGrid\n    SLAM --&gt;|\"Odometry\"| 3D\n    FC --&gt;|\"Images\"| VLM\n    FC --&gt;|\"Images\"| Q&amp;AVLM\n    VoxelGrid --&gt;|\"Map\"| 3D\n    VLM --&gt;|\"2D detections\"| 3D\n    UI --&gt;|\"Question (prompt)\"| Q&amp;AVLM\n    VLM --&gt; 2DDets\n    3D --&gt; 3DDets\n    Q&amp;AVLM --&gt;|\"Yes/No + reasoning\"| ReasonOutput\n\n\n    classDef input fill:#e0e7ff30,stroke:#6366f1,stroke-width:2px,color:#000\n    classDef frontend fill:#fef3c730,stroke:#eab308,stroke-width:2px,color:#000\n    classDef backend fill:#d1fae530,stroke:#10b981,stroke-width:2px,color:#000\n    classDef output fill:#cffafe30,stroke:#06b6d4,stroke-width:2px,color:#000\n\n    class SLAM,FC,UI input\n    class VLM,VoxelGrid,Q&amp;AVLM,3D backend\n    class 2DDets,3DDets,ReasonOutput output</code></pre>"},{"location":"vlm/#detection-vlm","title":"Detection VLM","text":"<p>Object detection is performed using either an open-vocabulary object detector (YOLOe) or a VLM-based detector (GPT-4V via API call) initialized with a set of labels or a description of the objects to detect. These models operate on the front camera image and produce 2D bounding boxes. In parallel, a downsampled voxel grid derived from the LiDAR point cloud and our SLAM odometry estimate is maintained. Accordingly, LiDAR points are projected into the camera frame using the current pose estimate and the camera projection matrix. Valid points are clustered to identify those that fall within each 2D detection/mask. This produces aligned 2D detections and corresponding 3D bounding volumes.</p>"},{"location":"vlm/#qa-vlm","title":"Q&amp;A VLM","text":"<p>For high-level semantic assessment, a VLM (GPT-4V via API call) processes the front-camera image together with a binary \u201cYes/No\u201d question. For example, queries related to assessing safety or navigation-related properties of the scene (e.g., is the exit of this environment blocked). The model returns the binary answer, alongside a color-coded confidence overlay on the input image, and a brief explanation of its reasoning.</p>"},{"location":"vlm/#topics-interfaces","title":"Topics &amp; Interfaces","text":"<p>Topics are remapped in the launch file. Note, all topics are in the <code>/detection_vlm</code> or <code>/reasoning_vlm</code> namespaces.</p>"},{"location":"vlm/#input-topics","title":"Input Topics","text":""},{"location":"vlm/#detection-vlm_1","title":"Detection VLM","text":"Topic Type Description <code>input_image</code> <code>sensor_msgs/Image</code> or <code>sensor_msgs/CompressedImage</code> Input image <code>input_pointcloud</code> <code>sensor_msgs/PointCloud2</code> Input Pointcloud"},{"location":"vlm/#qa-vlm_1","title":"Q&amp;A VLM","text":"Topic Type Description <code>input_image</code> <code>sensor_msgs/Image</code> or <code>sensor_msgs/CompressedImage</code> Input image"},{"location":"vlm/#input-frames-for-tf-querying","title":"Input Frames (for TF querying)","text":""},{"location":"vlm/#detection-vlm_2","title":"Detection VLM","text":"Frame name Description <code>target_frame</code> Target world frame <code>camera_frame</code> Camera frame <code>body_frame</code> Body/IMU frame"},{"location":"vlm/#output-topics","title":"Output Topics","text":""},{"location":"vlm/#detection-vlm_3","title":"Detection VLM","text":"Topic Type Description <code>detections_image</code> <code>sensor_msgs/Image</code> 2D detections overlayed on the input image <code>accumulated_pointcloud</code> <code>sensor_msgs/PointCloud2</code> Accumulated voxel grid map <code>detected_bboxes_3d</code> <code>vision_msgs/BoundingBox3DArray</code> 3D detections <code>visualization_3d_bboxes</code> <code>visualization_msgs/MarkerArray</code> 3D detections for visualization"},{"location":"vlm/#qa-vlm_2","title":"Q&amp;A VLM","text":"Topic Type Description <code>output_image</code> <code>sensor_msgs/Image</code> Color-coded confidence overlay on the input image and brief explanation of the VLM reasoning"},{"location":"vlm/#services-input","title":"Services (Input)","text":""},{"location":"vlm/#qa-vlm_3","title":"Q&amp;A VLM","text":"Service Type Description <code>set_prompt</code> <code>detection_vlm_msgs/SetPrompt</code> Prompt used by the VLM, it should be a Yes/No question"},{"location":"vlm/#configuration","title":"Configuration","text":""},{"location":"vlm/#detection-vlm_4","title":"Detection VLM","text":"<p>Parameters are set in:</p> <ul> <li>YOLOe: <code>detection_vlm/detection_vlm_ros/config/detection_yoloe.yaml</code></li> <li>GPT-4V: <code>detection_vlm/detection_vlm_ros/config/detection_vlm.yaml</code></li> </ul> <p>Topics and frame remappings are set in:</p> <ul> <li><code>detection_vlm/detection_vlm_ros/launch/detection_vlm.launch.yaml</code></li> </ul>"},{"location":"vlm/#vlm","title":"VLM","text":"Parameter Description <code>vlm/type</code> Which vlm to use: <code>yoloe</code> or <code>openai</code> <code>vlm/model</code> (YOLOe) Model to use (see here) <code>vlm/confidence_threshold</code> (YOLOe) Detection confidence threshold <code>vlm/verbose</code> (YOLOe) If <code>True</code>, prints inference statistics <code>vlm/confidence_threshold</code> (YOLOe) Detection confidence threshold <code>vlm/cuda</code> (YOLOe) Whether to use cuda <code>vlm/client_config/model</code> (OpenAI) Model to use <code>prompt</code> (OpenAI) Detection prompt. Specify/describe objects to detect"},{"location":"vlm/#image-worker","title":"Image worker","text":"Parameter Description <code>worker/min_separation_s</code> Processing process: take an image every <code>min_separation_s</code> seconds <code>compressed_image</code> Wether the input image topic is compressed"},{"location":"vlm/#voxel-map-and-clustering","title":"Voxel map and clustering","text":"Parameter Description <code>voxel_size</code> Map voxel size (m) <code>min_points_per_cluster</code> Minimum number of points a cluster must have <code>eps_dbscan</code> DBSCAN epsilon parameter (m) <code>min_point_r</code> Filter out points closer than this value to the body frame (m) <code>max_point_r</code> Filter out points further than this value to the body frame (m)"},{"location":"vlm/#camera-intrinsics","title":"Camera intrinsics","text":"Parameter Description <code>camera_intrinsics/fx</code> x focal length <code>camera_intrinsics/fy</code> y focal length <code>camera_intrinsics/cx</code> x principal point <code>camera_intrinsics/cy</code> y principal point"},{"location":"vlm/#others","title":"Others","text":"Parameter Description <code>use_tf_current_time</code> <code>False</code>: use message stamp to query TF. <code>True</code>: use current time to query TF. <code>use_masks_for_projection</code> Whether to use masks when projecting to 3D. Otherwise use bounding boxes. <code>verbose</code> Whether to print timings and info."},{"location":"vlm/#qa-vlm_4","title":"Q&amp;A VLM","text":"<p>Parameters are set in <code>detection_vlm/detection_vlm_ros/config/reasoning_vlm.yaml</code>.</p> <p>Topics and frame remmapings are set in <code>detection_vlm/detection_vlm_ros/launch/reasoning_vlm.launch.yaml</code>.</p>"},{"location":"vlm/#vlm_1","title":"VLM","text":"Parameter Description <code>vlm/type</code> Which vlm to use: only <code>openai</code> is supported for now <code>vlm/type</code> Which vlm to use: only <code>openai</code> is supported for now <code>vlm/client_config/model</code> (OpenAI) Model to use <code>prompt</code> (OpenAI) Detection prompt. Specify/describe objects to detect"},{"location":"vlm/#image-worker_1","title":"Image worker","text":"Parameter Description <code>worker/min_separation_s</code> Processing process: take an image every <code>min_separation_s</code> seconds <code>compressed_image</code> Wether the input image topic is compressed"},{"location":"vlm/#visualization","title":"Visualization","text":"Parameter Description <code>overlay_alpha</code> Alpha used for the color-coded overlay. <code>footer_height</code> Foother height for the reasoning on the output image."},{"location":"vlm/#others_1","title":"Others","text":"Parameter Description <code>footer_height</code> Footer height for the reasoning on the output image. <code>verbose</code> Whether to print timings and info."},{"location":"report/UnifiedAutonomyStack-Outline/","title":"The Unified Autonomy Stack: Toward a Blueprint for Generalizable Robot Autonomy","text":"<p>Autonomous Robots Lab, Norwegian University of Science and Technology, Norway</p>"},{"location":"report/UnifiedAutonomyStack-Outline/#abstract","title":"Abstract","text":"<p>We introduce and open-source the <code>Unified Autonomy Stack</code>, a system-level solution that enables robust autonomy across a wide range of aerial and ground robot morphologies. The architecture centers on three broadly applicable modules -multi-modal perception, multi-stage planning, and multi-layered safety mechanisms- that together deliver end-to-end mission autonomy. The resulting behaviors include safe navigation into unknown regions, exploration of complex environments, efficient inspection planning, and object-level scene reasoning. The stack has been validated on multiple multirotor platforms and legged robots operating in GPS-denied and perceptually degraded environments (e.g., self-similar, textureless, smoke/dust-filled settings), demonstrating resilient performance in demanding conditions. It currently supports two major embodiment families, namely rotorcraft and certain ground systems such as legged and differential-drive platforms, while several of its modules have already been validated on additional robot types, including fixed-wing aircraft for GPS-denied flight and underwater exploration planning. To facilitate ease of adoption and extension, we additionally release a reference hardware design that integrates a full multi-modal sensing suite, time-synchronization electronics, and high-performance compute capable of running the entire stack while leaving headroom for further development. Strategically, we aim to expand the Unified Autonomy Stack to cover most robot configurations across air, land, and sea.</p>"},{"location":"report/UnifiedAutonomyStack-Outline/#1-introduction","title":"1. Introduction","text":"<p>Autonomy is conventionally designed in a manner bespoke to distinct robot classes. Despite some generalizability in certain subsystems of autonomy, existing literature and available autonomy stacks (e.g., the works in [1, 2, 3, 4, 5, 6, 7, 8]) do not emphasize a unified solution across diverse robot configurations. However, recent advances point toward the potential for a universal autonomy engine. Despite the fact that such a goal is still immature and a wide range of approaches are worth investigating - spanning from ``conventional'' strategies to fully data-driven techniques - the benefits of unification and the collective need to advance robot capabilities highlight the need for general autonomy solutions.</p> <p> Figure 1: We release the <code>Unified Autonomy Stack</code> aiming for a common blueprint across diverse robot configurations. Currently supporting widely adopted aerial and ground robot types, the stack is planned to cover different robot morphologies across air, land, and sea. Several of its submodules have been tested across the three operational domains.</p> <p>Motivated by the above, we present the <code>Unified Autonomy Stack</code>, a comprehensive open-source autonomy stack applicable across diverse air and ground robot configurations. The <code>Unified Autonomy Stack</code> represents a step towards a common autonomy blueprint across diverse robot types - from multirotors and other rotorcrafts to legged robots - with the goal to push the frontier toward universal approaches in embodied AI. Its design emphasizes resilience in that it presents robustness, resourcefulness, and redundancy, enabling it to retain high performance across environments and conditions, including GPS-denied, visually-degraded, geometrically complex, and potentially adversarial settings that typically challenge safe navigation and mission autonomy.</p> <p>At its core, the <code>Unified Autonomy Stack</code> enables navigation for diverse systems in GPS-denied conditions. Simultaneously, it currently offers mission-complete capabilities for a set of information-gathering tasks such as autonomous exploration and inspection. Importantly, the <code>Unified Autonomy Stack</code> is open for extension, both from the perspective of the robots it readily supports and the missions it enables. Our team is explicitly targeting its full-fledged expansion to a) fixed-wing aerial robots, and b) small-scale underwater robots, while facilitating enhanced object-guided behaviors. Developed over a decade of research with its modules (independently or collectively) verified across a great variety of environments and missions spanning from subterranean spaces such as caves and mines - especially in the DAPRA Subterranean Challenge (planner module and a previous version of our perception stack [9, 10, 11]) - to ship ballast tanks and offshore facilities [12], industrial facilities [13], urban facilities [14, 15], and dense forests [16], the presented stack brings high technology readiness while simultaneously facilitating open research investigations.</p>"},{"location":"report/UnifiedAutonomyStack-Outline/#2-unified-autonomy","title":"2. Unified Autonomy","text":"<p>This section presents the architecture and key modules of the unified autonomy stack.</p>"},{"location":"report/UnifiedAutonomyStack-Outline/#21-autonomy-architecture","title":"2.1 Autonomy Architecture","text":"<p> Figure 2. The architecture of the Unified Autonomy Stack.</p> <p>The <code>Unified Autonomy Stack</code> is organized around three core modules - <code>perception</code>, <code>planning</code>, and <code>navigation</code> - following the key principles of the \"sense-think-act\" loop, while targeting generalizability across aerial and ground robot configurations, and resilience in demanding environments. Its overall architecture is presented in Figure 2. As shown, the <code>Unified Autonomy Stack</code> consumes sensor data and outputs low-level commands to standard controllers available in most modern robotic systems, for example on Pixhawk/PX4-based drones [17] (or any other MAVLink-compatible autopilot [18]) and standard velocity controllers on ground platforms. Its key features are as follows:</p> <ul> <li>Generalizability: The <code>Unified Autonomy Stack</code> applies with few adjustments to particularly diverse robot configurations, offering identical user-experience for navigation and informative path planning tasks despite system differences. Currently supporting out-of-the-box multirotors and other rotorcrafts, alongside several ground systems and especially legged robots, while many of its sub-modules have been verified on a broader set of systems including fixed-wing aircraft and underwater robots, it represents a sound point of departure for research in unified embodied AI.</li> <li>Multi-modality: The <code>Unified Autonomy Stack</code> fuses complementary sensor cues currently including LiDAR, FMCW radar, vision, and IMU in order to enable resilience in perceptually-degraded GPS-denied conditions [20].</li> <li>Multi-layer Safety: The <code>Unified Autonomy Stack</code> departs from conventional architectures in which safety is ensured by solutions with a single-point-of-failure. Most commonly, modern autonomy solutions rely on a cascade of calculations in which collision-free planning takes place only on an online (or offline) reconstructed map. In practice, this means that non-trivial localization or mapping errors (often encountered in perceptually-degraded settings or with thin obstacles) can lead to collisions. The <code>Unified Autonomy Stack</code> implements multiple layers of safety by combining map-based collision-free motion planning with a set of navigation policies and safety filters that directly consume online exteroception data and if necessary adjust the robot's path to correct for safety.</li> <li>Hybrid Methods: The <code>Unified Autonomy Stack</code> integrates both \"conventional\" techniques and deep learning methods. Indicative examples include its factor graph-based multi-modal Simultaneous Localizalition And Mapping (SLAM) and its navigation policies offering options for Exteroceptive Deep Reinforcement Learning (DRL)-based and Neural Signed Distance Field (SDF)-based Nonlinear Model Predictive Control (Neural SDF-NMPC).</li> </ul> <p>Subsequently, we outline the key modules of the <code>Unified Autonomy Stack</code> and point to prior works as applicable. Furthermore, we discuss the interfaces considered and how the <code>Unified Autonomy Stack</code> can be extended to new robot configurations.</p>"},{"location":"report/UnifiedAutonomyStack-Outline/#22-perception-module","title":"2.2 Perception Module","text":"<p>The <code>perception module</code> includes our solution for multi-modal SLAM, alongside integration with a VLM-based reasoning step.</p>"},{"location":"report/UnifiedAutonomyStack-Outline/#221-multi-modal-slam","title":"2.2.1 Multi-Modal SLAM","text":"<p>The multi-modal SLAM (dubbed MIMOSA) system (depicted in Figure 3) uses a factor graph estimator to fuse LiDAR, radar, camera, and IMU measurements using a windowed smoother [19], for computational efficiency. This architecture, as shown in the referenced figures, is based on the methods proposed in [20, 21, 22, 23] for enhanced LiDAR and radar integration. The estimator considers a state space comprised of position, velocity, attitude, accelerometer/gyroscope bias, and gravity direction states. These are estimated over a temporal window by a nonlinear optimizer [24], where the optimal estimate \\(\\mathcal{X}^{*}\\) is found by minimizing the weighted (by their covariances \\(\\Sigma_{*}\\)) sum of the residuals \\(\\boldsymbol{e}_{*}\\) derived from the different sensor measurements. This minimization problem can thus be written as follows:</p> \\[ \\mathcal{X}^{*} = \\underset{\\mathcal{X}}{\\arg\\min} \\Big[          \\lVert \\boldsymbol{e}_{0} \\rVert_{\\Sigma_{0}}^{2}         + \\sum_{i\\in\\mathcal{F}_{\\mathcal{I}}} \\lVert \\boldsymbol{e}_{\\mathcal{I}_{i}} \\rVert_{\\Sigma_{\\mathcal{I}}}^{2}         + \\sum_{i\\in\\mathcal{F}_{\\mathcal{L}}} \\lVert \\boldsymbol{e}_{\\mathcal{L}_{i}} \\rVert_{\\Sigma_{\\mathcal{L}}}^{2}         + \\sum_{i\\in\\mathcal{F}_{\\mathcal{R}}} \\lVert \\boldsymbol{e}_{\\mathcal{R}_{i}} \\rVert_{\\Sigma_{\\mathcal{R}}}^{2}         + \\sum_{i\\in\\mathcal{F}_{\\mathcal{V}}} \\lVert \\boldsymbol{e}_{\\mathcal{V}_{i}} \\rVert_{\\Sigma_{\\mathcal{V}}}^{2}     \\Big], \\] <p>for the marginalization prior \\(\\boldsymbol{e}_{0}\\),\\(\\Sigma_{0}\\), and the IMU, LiDAR, radar, and vision factors \\(\\mathcal{F}_{*}\\) in the window time frame (denoted by \\(\\mathcal{I}\\), \\(\\mathcal{L}\\), \\(\\mathcal{R}\\), and \\(\\mathcal{V}\\), respectively). The details on how sensor measurements are used to construct each of these factors are provided in the following sections.</p> <p> Figure 3. Overarching factor graph of our MIMOSA Multi-modal.</p> <p>Inertial Measurement Unit Inertial Measurement Unit (IMU) measurements are stored in a buffer on arrival, for easy use upon receiving measurements from one of the aiding sensors. At that point, the corresponding exteroceptive factors are created and connected to the graph by an IMU preintegration factor, following [25]. Note that, given the multiple exteroceptive sensors and the possibility of transmission latency, a node in the graph with the same or very similar timestamp may already exist at this point. If this is the case, the new factors are attached to the existing node.</p> <p>LiDAR Upon receiving a LiDAR measurement, the point cloud is first deskewed, using the IMU measurements corresponding to the LiDAR's sweep. Afterwards, the point cloud is downsampled, for computational efficiency, first by removing three out of four points, and second by organizing the point cloud into a voxel grid and subsampling. Subsequently, the correspondences are found by relating points in the current cloud with planes so as to fit in the map; these correspondences are added to the graph in the form of point-to-plane residuals. For outlier rejection, these residuals are augmented with Huber M-estimators. Post-optimization, the pose is compared with previous key frames, and if a significant difference in position or attitude is detected, a new key frame is created, and the current point cloud is added to the monolithic map. The core method is presented in [22].</p> <p>Radar Unlike the estimator proposed in [21], here the least-squares calculation of linear velocity from the radar point cloud is omitted. Instead, the individual points from the radar point cloud are integrated into the graph. This avoids the potential limitations associated with first estimating linear velocity independently. Namely, these are the minimum number and diversity of points required for fully resolving the 3 axes of linear velocity. As the radar sensor is known to be noisy [26], the residual is augmented with a Cauchy M-estimator for outlier rejection. This has the added benefit of improved resilience against dynamic objects.</p> <p>Vision Vision factors are added in a loosely-coupled manner, taking advantage of the wealth of capable estimators that exist in the vision community. Specifically, a visual-inertial estimator based on [27] processes the camera and IMU measurements, creating odometry estimates as a result. The pose estimates from this fusion are stored in a buffer and added to the graph of our multi-modal SLAM as a between factor by calculating the relative transform between two pose estimates. This approach draws inspiration from [20].</p>"},{"location":"report/UnifiedAutonomyStack-Outline/#222-vision-language-reasoning","title":"2.2.2 Vision-Language Reasoning","text":"<p>The system currently integrates two complementary vision-language model (VLM) modalities: (a) open-vocabulary object detection with 3D spatial grounding, and (b) binary visual question-answering (Yes/No) with reasoning. In their combination, these capabilities collectively enable semantic scene understanding and contextual judgment based on online visual data.</p> <p>Open-Vocabulary Object Detection with 3D Projection Object detection is performed using either a prompt-free detector (YOLOe) or a VLM-based detector (GPT-4V) initialized with a set of labels. These models operate on the front-camera image and produce 2D bounding boxes. In parallel, a downsampled voxel grid derived from the LiDAR point cloud and our MIMOSA SLAM odometry estimate is maintained. Accordingly, LiDAR points are projected into the camera frame using the current pose estimate (and the relevant extrinsics), and clustered to identify which points fall within each 2D detection. This produces aligned 2D detections and corresponding 3D bounding volumes.</p> <p>Yes/No VLM Question Answering For high-level semantic assessment, a VLM (currently GPT-4V) processes the front-camera image together with a ``Yes/No'' question. Indicative queries relate to assessing safety- or navigation-related properties of the scene (e.g., is an object blocking a door?). The model returns the binary answer, alongside a color-coded confidence overlay, and a brief explanation of the reasoning.</p>"},{"location":"report/UnifiedAutonomyStack-Outline/#23-planning-module","title":"2.3 Planning Module","text":"<p>Path planning in the <code>Unified Autonomy Stack</code> is facilitated through an updated and extended version of the graph-based planner GBPlanner (called GBPlanner3) originally presented in [10, 11] and augmented for coverage behaviors in [12]. Currently applicable to systems for which graph-based planning is a viable choice, the system enables both target navigation as well as informative planning behaviors such as exploration and inspection in a unified approach (Figure 4).</p> <p> Figure 4. Planning module architecture.</p>"},{"location":"report/UnifiedAutonomyStack-Outline/#231-core-planning-strategy","title":"2.3.1 Core Planning Strategy","text":"<p>GBPlanner's core principles rely on random sampling-based graphs and a local/global architecture that facilitates scalable operation across environments of vast scale and complex geometries. The method samples a dense local graph around the robot's location at any given time, while it simultaneously maintains a sparse global graph that spans over all the regions the robot has visited up to any point in its mission. Graph vertices and edges are sampled only in a collision-free manner, exploiting the volumetric representation in Voxblox [28]. LiDAR or any other depth data is used to update Voxblox online. On top of this planning kernel, the system facilitates collision-free path planning to desired targets, as well as informative behaviors such as exploration of unknown environments and inspection planning. Specifically for ground robots, the method further incorporates traversability constraints, building upon the representation in [29]. A set of other modifications as compared to the prior works in [10, 11, 12] improve the overall behavior, including but not limited to batch vertex-edge sampling in the local graph, availability of multiple distributions (uniform and normal tailored to narrow spaces) for vertex sampling, and active collision re-check and path modification during global re-positioning and return-to-home plans.</p>"},{"location":"report/UnifiedAutonomyStack-Outline/#232-collision-free-planning-to-a-target","title":"2.3.2 Collision-free Planning to a Target","text":"<p>The <code>Unified Autonomy Stack</code> facilitates planning to a desired waypoint both within the already explored space, as well as in the unknown, as long as this is iteratively found to be possible. For the first goal, the method simply exploits its local and global graphs to plan a safe path to any of the explored locations. For the latter, the method plans to a waypoint close to the frontier of the explored volume and in the direction of the desired target, while it then iteratively repeats this process as the robot unveils more of the space based on its onboard depth sensing.</p>"},{"location":"report/UnifiedAutonomyStack-Outline/#233-exploration-planning","title":"2.3.3 Exploration Planning","text":"<p>When the behavior exploration of unknown volumes is set, the planner is searching within its local graph for paths that maximize the anticipated gain of new volume to be explored, considering a defined range sensor model, as detailed in [10, 11]. When the solutions within the local graph report that no such informative path exists, the method re-positions the robot to a previously explored area in order for the exploration mission to be effectively continued. If the considered robot endurance limits are about to be reached, the method timely commands a return-to-home path for the system (considering the time it takes to return to that location).</p>"},{"location":"report/UnifiedAutonomyStack-Outline/#234-inspection-planning","title":"2.3.4 Inspection Planning","text":"<p>When the behavior of optimized inspection of previously mapped structures is set, the planner is using its random graph to search for a route that observes as much as possible of the considered subset of the environment while keeping the collective path length small. The method assumes a certain sensor frustum for inspection that may be distinct from the sensor with which the robot explores the world volumetrically. Inspection resolution and angle guarantees are provided and the overall methodology is inspired by the concept of General Visual Inspection (GVI) in industrial facilities. The procedure is detailed in [12]. Critically, the exploration and inspection behaviors can be combined with the planner triggering each mode as necessary for a broader mission.</p>"},{"location":"report/UnifiedAutonomyStack-Outline/#24-navigation-module","title":"2.4 Navigation Module","text":"<p>The <code>Unified Autonomy Stack</code> takes a multi-layered approach to safety illustrated in Figure 5. Conventionally, modern safe navigation and collision avoidance are based on the planning of paths (and trajectories) which are then blindly followed by an onboard controller. However, as discussed in [16, 30], this represents a single point of failure which can lead robots to collisions due to odometry errors or erroneous/incomplete mapping. While the <code>Unified Autonomy Stack</code> maintains map-based avoidance as the core approach to safety, it adds layers of safety either through (a) the combined use of exteroceptive Neural SDF-NMPC [16] and last-resort safety filtering based on CBFs, or (b) via exteroceptive DRL-based policies trained for safe navigation and smooth collision avoidance. The two distinct approaches -presented below- are offered simultaneously for the user to select owing to their relative benefits in certain conditions and in order to enable research across areas. These methods replace conventional position controllers - or similar methods - that assume that the provided path planned on the reconstructed map is to be followed blindly, and instead enable local deviations if necessary. They provide commands to a low-level controller following standard interfaces applicable across most widely used Autopilots and Robot Operating System (ROS)-based ground vehicles or similar systems. </p> <p> Figure 5. Navigation module architecture Note that currently Neural SDF-NMPC provides acceleration references potentially adjusted by the Composite CBF safety filter, while when Exteroceptive-DRL is utilized the output is velocity references and integration with the safety filter is not currently provided.</p>"},{"location":"report/UnifiedAutonomyStack-Outline/#241-neural-sdf-nmpc","title":"2.4.1 Neural SDF-NMPC","text":"<p>Detailed in [16], the Neural SDF-NMPC enables collision-free navigation in unknown environments even without access to a map, errors in a robot's map or drifting odometry. The method employs onboard range sensing (e.g., from a LiDAR or depth from stereo/RGB-D) and a deep neural network to convert range images to a SDF representation through a cascade structure. First, a convolutional encoder compresses the input image and then a Multi-Layer Perceptron (MLP) approximates the SDF. The learned SDF is then providing explicit position constraints for obstacle avoidance, directly into a nonlinear MPC. Thus, the method outputs acceleration commands to be tracked by a low-level control and enables safer navigation to target locations by adding a further layer of safety. Critically, the Neural SDF-NMPC provides theoretical guarantees when it comes to recursive feasibility and stability (under fixed sensor observations), which are essential features for the emphasis <code>Unified Autonomy Stack</code> puts on assured safety. Note that the method specifically tracks reference velocities and thus the interface with the <code>planning module</code> employs a function to produce such commands from planned paths.</p>"},{"location":"report/UnifiedAutonomyStack-Outline/#242-deep-reinforcement-learning","title":"2.4.2 Deep Reinforcement Learning","text":"<p>Alternatively to the use of the Neural SDF-NMPC, the <code>Unified Autonomy Stack</code> further offers exteroceptive DRL-based navigation (dubbed Exteroceptive-DRL) based on the work in [30, 31]. The provided policies consider as input both the robot's odometry and the instantaneous real-time depth (or range) image from any relevant sensor (including stereo or RGB-D cameras, as well as LiDARs). We offer the option to both train the policy by (a) first employing the DCE originally presented in [32] and thus encode the depth image to a low-dimension latent space responsible to maintain collision information, or (b) directly employ end-to-end learning from depth and odometry data to robot commands. In both cases, the command vector is the robot's reference body velocities. Like in the case of the Neural SDF-NMPC the method enables safe collision-free navigation even without a map and is thus provided under the concept of multi-layered safety, offering further assurance even when the onboard SLAM experiences odometry drift or the map fails to appropriately map certain obstacles (e.g., thin wires [33]).</p>"},{"location":"report/UnifiedAutonomyStack-Outline/#243-composite-cbf-based-safety-filter","title":"2.4.3 Composite CBF-based Safety Filter","text":"<p>Beyond the abovementioned navigation approaches -which combine map-based safety of the <code>planning module</code> with reactive collision avoidance- the <code>Unified Autonomy Stack</code> further provides a last-resort safety filter. Based on Composite Control Barrier Functions (C-CBFs) on a short sliding window of volumetric map data as detailed in [34], the solution modifies the reference acceleration (or velocity) transmitted to the robot's low-level controller when an unexpected impending collision is detected. To that point, the robot motion model is taken into consideration, while the individual obstacle constraints acquired by the map representation are combined into a composite function. The latter creates a smooth under-approximation of the safe set, and its gradient creates a \"virtual obstacle\" effect, acting as a weighted average of nearby obstacles to steer the robot away from danger. Although by-design considered for extremely rare engagement by the overall autonomy stack (as avoidance through planning and the presented navigation methods is made to function successfully), it is an important module to fully -and mathematically formally- safeguard autonomous robots operating in hard-to-access and overall demanding environments. Furthermore, a key decision variable for its use is the fact that both Neural SDF-NMPC and Exteroceptive-DRL involve a deep neural network the assured and explainable performance of which is not strictly guaranteed.</p>"},{"location":"report/UnifiedAutonomyStack-Outline/#244-exteroceptive-overwrite","title":"2.4.4 Exteroceptive Overwrite","text":"<p>When map-based safety from the <code>planning module</code> is considered sufficient or when the specific exteroceptive navigation methods are not thought to be desirable for (or compatible with) a certain robot, the <code>Unified Autonomy Stack</code> allows to use a more conventional state feedback controller by the user. This can be any such solution, such as Linear MPC, the underlying MPC method in Neural SDF-NMPC disabling collision constraints, or a solution for any other particular robot or through any autopilot (e.g., PX4). A common option can be the use of PX4's position control mode or a standard waypoint controller provided by the robot's manufacturer or through existing open-source packages.</p>"},{"location":"report/UnifiedAutonomyStack-Outline/#25-assumed-low-level-interfaces","title":"2.5 Assumed Low-level Interfaces","text":"<p>The <code>Unified Autonomy Stack</code> interfaces diverse aerial and ground robots through the following interfaces: * If the multi-layered safe navigation is not necessary, we provide waypoints or 3D accelerations to existing autopilots and low-level controllers to diverse rotorcrafts and ground robots. Waypoints are straightforward for all systems that have such control. However, it does not deliver the full stack functionality of multi-layered safe navigation. * When the full stack with multi-layered safety is considered, we command reference accelerations or velocities. Those we can provide directly to compatible autopilots and low-level controllers such as any PX4/ArduPilot-based drone.</p> <p>We plan to provide support for most widely-adopted low-level control/autopilot interfaces.</p>"},{"location":"report/UnifiedAutonomyStack-Outline/#3-hardware","title":"3. Hardware","text":"<p>To support early adoption, we combine the open-source release of the <code>Unified Autonomy Stack</code> with a reference hardware design incorporating all the considered sensing modalities, time-synchronization electronics and ample onboard compute. Called \"UniPilot\" the system is detailed in [35]. UniPilot is a compact modular autonomy payload combining LiDAR, radar, vision, Time-of-Flight (ToF)-depth and IMU sensing with an NVIDIA Orin NX-based compute and is deployable across heterogeneous robots. The specific sensor types are shown in the table below. Power, interfacing and synchronization electronics enable a compact and high-performance solution. All subsystems are housed in a lightweight SLA-printed enclosure engineered for mechanical robustness and cross-platform mounting. Tested onboard multitors, legged robots, and VTOL convertible aerial robots, UniPilot represents a practical, fully integrated autonomy payload enabling mapping, planning, safety, and control across diverse platforms in challenging GPS-denied visually-degraded, communications-denied environments. It can readily run the <code>Unified Autonomy Stack</code> and thus enable its immediate adoption. The full sensing configuration (and optional choices) of UniPilot are shown in the table below.</p>"},{"location":"report/UnifiedAutonomyStack-Outline/#components-mounted-on-the-unipilot-module","title":"Components Mounted on the UniPilot Module","text":"Category Name Weight [g] Power [W] Description Compute 1x Orin NX 28 25 RAM: 16 GB, 8-core Arm Cortex CPU, 1024-core NVIDIA GPU 1x Boson-22 carrier board + heat sink 152 13.8 4x MIPI CSI-2, 2x Gigabit Ethernet, 1x USB 2.0, 1x USB 3.1, 8x GPIO, 1x CAN, 2x I2C, 3x UART, 2x SPI interfaces IMU 1x VectorNav VN-100 15 0.22 800 Hz, Accel.: \\(140 \\mu g/\\sqrt{Hz}\\), Gyro.: \\(0.0035^{\\circ}/s/\\sqrt{Hz}\\) Cameras 2x VC MIPI IMX296M 4 0.759 Grayscale, Resolution: \\(1440\\times1080\\), FoV: \\(185^{\\circ}\\) 1x VC MIPI IMX296C 4 0.759 Color, Resolution: \\(1440\\times1080\\), FoV: \\(118^{\\circ}\\times94^{\\circ}\\) 1x PMD Flexx2 ToF 13 0.68 Range: 4 m, FoV: \\(56 \\times 44^{\\circ}\\) Radar 1x D3 Embedded RS-6843AOPU 25 7.5 Range: 49 m, FoV: \\(180 \\times 180^{\\circ}\\) LiDAR 1x RoboSense Airy or OUSTER OS0 230 8 Range: 30 m, FoV: \\(360 \\times 90^{\\circ}\\) <p>The animation below presents the UniPilot core subsystems.</p> <p> Figure 6. 3D view of the UniPilot reference hardware design.</p>"},{"location":"report/UnifiedAutonomyStack-Outline/#4-validation-results","title":"4. Validation Results","text":"<p>A set of prior results (documented in the linked publications and this YouTube List) demonstrate the individual evaluation of the underlying modules, while a set of new results, simulation set-ups and open datasets are planned for release with the <code>Unified Autonomy Stack</code>. All results will be documented (continuously) in this repository.</p>"},{"location":"report/UnifiedAutonomyStack-Outline/#5-conclusion-future-work","title":"5. Conclusion &amp; Future Work","text":"<p>The <code>Unified Autonomy Stack</code> is openly released with the aim of serving as a foundation for a common autonomy blueprint across diverse robot configurations operating in the air, on land, and at sea. We seek to collaborate with the research community towards its most reliable operation and resilient performance, alongside its extension to different robot morphologies. We are confident that robot autonomy can take the explosive trajectory observed in autopilots and thus be highly democratized while maintaining exceptional performance and robustness.</p>"},{"location":"report/UnifiedAutonomyStack-Outline/#references","title":"References","text":"<ol> <li>M. Fernandez-Cortizas, M. Molina, P. Arias-Perez, R. Perez-Segui, D. Perez-Saura, and P. Campoy, \u201cAerostack2: A software framework for developing multi-robot aerial systems,\u201d arXiv preprint, arXiv:2303.18237, 2023.</li> <li>J. L. Sanchez-Lopez, R. A. S. Fern\u00e1ndez, H. Bavle, C. Sampedro, M. Molina, J. Pestana, and P. Campoy, \u201cAerostack: An architecture and open-source software framework for aerial robotics,\u201d in 2016 International Conference on Unmanned Aircraft Systems (ICUAS). IEEE, 2016, pp. 332\u2013341.</li> <li>T. Baca, M. Petrlik, M. Vrba, V. Spurny, R. Penicka, D. Hert, and M. Saska, \u201cThe mrs uav system: Pushing the frontiers of reproducible research, real-world deployment, and education with autonomous unmanned aerial vehicles,\u201d Journal of Intelligent &amp; Robotic Systems, vol. 102, no. 1, p. 26, 2021.</li> <li>K. Mohta, M. Watterson, Y. Mulgaonkar, S. Liu, C. Qu, A. Makineni, K. Saulnier, K. Sun, A. Zhu, J. Delmerico, D. Thakur, K. Karydis, N. Atanasov, G. Loianno, D. Scaramuzza, K. Daniilidis, C. J. Taylor, and V. Kumar, \u201cFast, autonomous flight in gps-denied and cluttered environments,\u201d Journal of Field Robotics, vol. 35, no. 1, pp. 101\u2013120, 2018.</li> <li>P. Foehn, E. Kaufmann, A. Romero, R. Penicka, S. Sun, L. Bauersfeld, T. Laengle, G. Cioffi, Y. Song, A. Loquercio et al., \u201cAgilicious: Opensource and open-hardware agile quadrotor for vision-based flight,\u201d Science robotics, vol. 7, no. 67, p. eabl6259, 2022.</li> <li>C. Goodin, M. N. Moore, D. W. Carruth, C. R. Hudson, L. D. Cagle, S. Wapnick, and P. Jayakumar, \u201cThe nature autonomy stack: an open-source stack for off-road navigation,\u201d in Unmanned Systems Technology XXVI, vol. 13055. SPIE, 2024, pp. 8\u201317.</li> <li>Carnegie Mellon University, AirLab, \u201cAirstack,\u201d https://github.com/castacks/AirStack, 2025, accessed: 2025-02-04.</li> <li>F. Real, A. Torres-Gonz\u00e1lez, P. R. Soria, J. Capit\u00e1n, and A. Ollero, \u201cUnmanned aerial vehicle abstraction layer: An abstraction layer to operate unmanned aerial vehicles,\u201d International Journal of Advanced Robotic Systems, vol. 17, no. 4, pp. 1\u201313, 2020. [Online]. Available: https://doi.org/10.1177/1729881420925011</li> <li>M. Tranzatto, T. Miki, M. Dharmadhikari, L. Bernreiter, M. Kulkarni, F. Mascarich, O. Andersson, S. Khattak, M. Hutter, R. Siegwart, and K. Alexis, \u201cCerberus in the darpa subterranean challenge,\u201d Science Robotics, vol. 7, no. 66, p. eabp9742, 2022.</li> <li>T. Dang, M. Tranzatto, S. Khattak, F. Mascarich, K. Alexis, and M. Hutter, \u201cGraph-based subterranean exploration path planning using aerial and legged robots,\u201d Journal of Field Robotics, vol. 37, no. 8, pp. 1363\u20131388, 2020, eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.21993. [Online]. Available: https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21993</li> <li>M. Kulkarni, M. Dharmadhikari, M. Tranzatto, S. Zimmermann, V. Reijgwart, P. De Petris, H. Nguyen, N. Khedekar, C. Papachristos, L. Ott, R. Siegwart, M. Hutter, and K. Alexis, \u201cAutonomous Teamed Exploration of Subterranean Environments using Legged and Aerial Robots,\u201d in 2022 International Conference on Robotics and Automation (ICRA), May 2022, pp. 3306\u20133313. [Online]. Available: https://ieeexplore.ieee.org/document/9812401</li> <li>M. Dharmadhikari, P. De Petris, M. Kulkarni, N. Khedekar, H. Nguyen, A. E. Stene, E. Sj\u00f8vold, K. Solheim, B. Gussiaas, and K. Alexis, \u201cAutonomous Exploration and General Visual Inspection of Ship Ballast Water Tanks Using Aerial Robots,\u201d in 2023 21st International Conference on Advanced Robotics (ICAR), Dec. 2023, pp. 409\u2013416, iSSN: 2572-6919. [Online]. Available: https://ieeexplore.ieee.org/document/10406928</li> <li>M. Dharmadhikari, N. Khedekar, P. De Petris, M. Kulkarni, M. Nissov, and K. Alexis, \u201cMaritime vessel tank inspection using aerial robots: Experience from the field and dataset release,\u201d arXiv preprint arXiv:2404.19045, 2024.</li> <li>M. Singh, M. Dharmadhikari, and K. Alexis, \u201cAriel explores: Vision-based underwater exploration and inspection via generalist drone-level autonomy,\u201d arXiv preprint arXiv:2507.10003, 2025.</li> <li>M. Tranzatto, F. Mascarich, L. Bernreiter, C. Godinho, M. Camurri, S. Khattak, T. Dang, V. Reijgwart, J. Loeje, D. Wisth et al., \u201cCerberus: Autonomous legged and aerial robotic exploration in the tunnel and urban circuits of the darpa subterranean challenge,\u201d arXiv preprint arXiv:2201.07067, vol. 3, 2022.</li> <li>M. Jacquet, M. Harms, and K. Alexis, \u201cNeural NMPC through Signed Distance Field Encoding for Collision Avoidance,\u201d Nov. 2025, arXiv:2511.21312 [cs]. [Online]. Available: http://arxiv.org/abs/2511. 21312</li> <li>L. Meier, D. Honegger, and M. Pollefeys, \u201cPx4: A node-based multithreaded open source robotics framework for deeply embedded platforms,\u201d in 2015 IEEE international conference on robotics and automation (ICRA). IEEE, 2015, pp. 6235\u20136240.</li> <li>A. Koubaa, A. Allouch, M. Alajlan, Y. Javed, A. Belghith, and M. Khalgui, \u201cMicro air vehicle link (mavlink) in a nutshell: A survey,\u201d IEEE Access, vol. 7, pp. 87 658\u201387 680, 2019.</li> <li>F. Dellaert and GTSAM Contributors, \u201cborglab/gtsam,\u201d May 2022. [Online]. Available: https://github.com/borglab/gtsam</li> <li>N. Khedekar, M. Kulkarni, and K. Alexis, \u201cMIMOSA: A Multi- Modal SLAM Framework for Resilient Autonomy against Sensor Degradation,\u201d in 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Kyoto, Japan: IEEE, Oct. 2022, pp. 7153\u20137159. [Online]. Available: https://ieeexplore.ieee.org/document/9981108/</li> <li>M. Nissov, N. Khedekar, and K. Alexis, \u201cDegradation Resilient LiDAR-Radar-Inertial Odometry,\u201d in 2024 IEEE International Conference on Robotics and Automation (ICRA), May 2024, pp. 8587\u20138594. [Online]. Available: https://ieeexplore.ieee.org/document/10611444</li> <li>N. Khedekar and K. Alexis, \u201cPG-LIO: Photometric-Geometric fusion for Robust LiDAR-Inertial Odometry,\u201d Jun. 2025, arXiv:2506.18583 [cs]. [Online]. Available: http://arxiv.org/abs/2506.18583</li> <li>M. Nissov, J. A. Edlund, P. Spieler, C. Padgett, K. Alexis, and S. Khattak, \u201cRobust high-speed state estimation for off-road navigation using radar velocity factors,\u201d IEEE Robotics and Automation Letters, vol. 9, no. 12, pp. 11 146\u201311 153, 2024.</li> <li>M. Kaess, H. Johannsson, R. Roberts, V. Ila, J. Leonard, and F. Dellaert, \u201ciSAM2: Incremental smoothing and mapping with fluid relinearization and incremental variable reordering,\u201d in 2011 IEEE International Conference on Robotics and Automation. IEEE, May 2011, pp. 3281\u20133288.</li> <li>C. Forster, L. Carlone, F. Dellaert, and D. Scaramuzza, \u201cOn-manifold preintegration for real-time visual\u2013inertial odometry,\u201d IEEE Transactions on Robotics, vol. 33, no. 1, pp. 1\u201321, 2017.</li> <li>K. Harlow, H. Jang, T. D. Barfoot, A. Kim, and C. Heckman, \u201cA new wave in robotics: Survey on recent mmwave radar applications in robotics,\u201d IEEE Transactions on Robotics, vol. 40, pp. 4544\u20134560, 2024.</li> <li>M. Bloesch, M. Burri, S. Omari, M. Hutter, and R. Siegwart, \u201cIterated extended kalman filter based visual-inertial odometry using direct photometric feedback,\u201d The International Journal of Robotics Research, vol. 36, no. 10, pp. 1053\u20131072, 2017.</li> <li>H. Oleynikova, Z. Taylor, M. Fehr, R. Siegwart, and J. Nieto, \u201cVoxblox: Incremental 3d euclidean signed distance fields for onboard mav planning,\u201d in 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2017, pp. 1366\u20131373.</li> <li>P. Fankhauser, M. Bloesch, and M. Hutter, \u201cProbabilistic terrain mapping for mobile robots with uncertain localization,\u201d IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 3019\u20133026, 2018</li> <li>M. Kulkarni and K. Alexis, \u201cReinforcement Learning for Collision-free Flight Exploiting Deep Collision Encoding,\u201d in 2024 IEEE International Conference on Robotics and Automation (ICRA), May 2024, pp. 15 781\u201315 788. [Online]. Available: https://ieeexplore.ieee.org/document/10610287</li> <li>M. Kulkarni, W. Rehberg, and K. Alexis, \u201cAerial Gym Simulator: A Framework for Highly Parallelized Simulation of Aerial Robots,\u201d IEEE Robotics and Automation Letters, vol. 10, no. 4, pp. 4093\u20134100, Apr. 2025. [Online]. Available: https://ieeexplore.ieee.org/document/10910148/</li> <li>M. Kulkarni and K. Alexis, \u201cTask-Driven Compression for Collision Encoding Based on Depth Images,\u201d in Advances in Visual Computing, G. Bebis, G. Ghiasi, Y. Fang, A. Sharf, Y. Dong, C. Weaver, Z. Leo, J. J. LaViola Jr., and L. Kohli, Eds. Cham: Springer Nature Switzerland, 2023, pp. 259\u2013273.</li> <li>M. Kulkarni, B. Moon, K. Alexis, and S. Scherer, \u201cAerial field robotics,\u201d in Encyclopedia of Robotics. Springer, 2022, pp. 1\u201315.</li> <li>M. Harms, M. Jacquet, and K. Alexis, \u201cSafe Quadrotor Navigation Using Composite Control Barrier Functions,\u201d in 2025 IEEE International Conference on Robotics and Automation (ICRA), May 2025, pp. 6343\u20136349. [Online]. Available: https://ieeexplore.ieee.org/document/11127368</li> <li>M. Kulkarni, M. Dharmadhikari, N. Khedekar, M. Nissov, M. Singh, P. Weiss, and K. Alexis, \u201cUnipilot: Enabling gps-denied autonomy across embodiments,\u201d arXiv preprint arXiv:2509.11793, 2025.</li> </ol>"}]}