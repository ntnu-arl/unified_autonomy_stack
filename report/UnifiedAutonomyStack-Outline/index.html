
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Comprehensive documentation for the Unified Autonomy Stack covering SLAM, Planning, Navigation, Control, and Robots.">
      
      
        <meta name="author" content="Autonomous Robots Lab, NTNU">
      
      
      
        <link rel="prev" href="../../planned_upgrades/">
      
      
      
        
      
      
      <link rel="icon" href="../../figures/arl_logo.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.0">
    
    
      
        <title>Technical Report - Unified Autonomy Stack</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.618322db.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="deep-orange" data-md-color-accent="blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#the-unified-autonomy-stack-toward-a-blueprint-for-generalizable-robot-autonomy" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Unified Autonomy Stack" class="md-header__button md-logo" aria-label="Unified Autonomy Stack" data-md-component="logo">
      
  <img src="../../figures/arl_logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Unified Autonomy Stack
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Technical Report
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="deep-orange" data-md-color-accent="blue"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 2c-1.82 0-3.53.5-5 1.35C8 5.08 10 8.3 10 12s-2 6.92-5 8.65C6.47 21.5 8.18 22 10 22a10 10 0 0 0 10-10A10 10 0 0 0 10 2"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="green" data-md-color-accent="teal"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/ntnu-arl/unified_autonomy_stack" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    ntnu-arl/unified_autonomy_stack
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../installation/" class="md-tabs__link">
          
  
  
  Getting Started

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../architecture/" class="md-tabs__link">
          
  
  
  System Overview

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../prior_results/" class="md-tabs__link">
          
  
  
  Evaluation

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../planned_upgrades/" class="md-tabs__link">
        
  
  
    
  
  Planned Upgrades

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="./" class="md-tabs__link">
        
  
  
    
  
  Technical Report

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Unified Autonomy Stack" class="md-nav__button md-logo" aria-label="Unified Autonomy Stack" data-md-component="logo">
      
  <img src="../../figures/arl_logo.png" alt="logo">

    </a>
    Unified Autonomy Stack
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/ntnu-arl/unified_autonomy_stack" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    ntnu-arl/unified_autonomy_stack
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Getting Started
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Getting Started
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../installation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Installation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deployment/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Deployment
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Examples
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    System Overview
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    System Overview
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../architecture/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Architecture
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Perception Module
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Perception Module
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../slam/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Multi-Modal SLAM
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../vlm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    VLM
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3" >
        
          
          <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Planning Module
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Planning Module
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../planning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Planning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_4" >
        
          
          <label class="md-nav__link" for="__nav_3_4" id="__nav_3_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Navigation Module
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Navigation Module
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../navigation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Navigation Architecture
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../nmpc/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Neural SDF-MPC
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../rl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Deep RL for Navigation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cbf/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Composite CBF Safety Filter
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_5" >
        
          
          <label class="md-nav__link" for="__nav_3_5" id="__nav_3_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Simulation and Hardware
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    Simulation and Hardware
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../simulation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Simulation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../platforms/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Platforms
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../hardware/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Hardware and Drivers
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Evaluation
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Evaluation
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../prior_results/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Prior Results
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../indicative_results/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Indicative Results
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../datasets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Datasets
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../planned_upgrades/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Planned Upgrades
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Technical Report
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Technical Report
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#abstract" class="md-nav__link">
    <span class="md-ellipsis">
      
        Abstract
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-introduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Introduction
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-unified-autonomy" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Unified Autonomy
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Unified Autonomy">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-autonomy-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.1 Autonomy Architecture
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-perception-module" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.2 Perception Module
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.2 Perception Module">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#221-multi-modal-slam" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.2.1 Multi-Modal SLAM
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#222-vision-language-reasoning" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.2.2 Vision-Language Reasoning
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-planning-module" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.3 Planning Module
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.3 Planning Module">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#231-core-planning-strategy" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.3.1 Core Planning Strategy
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#232-collision-free-planning-to-a-target" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.3.2 Collision-free Planning to a Target
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#233-exploration-planning" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.3.3 Exploration Planning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#234-inspection-planning" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.3.4 Inspection Planning
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#24-navigation-module" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.4 Navigation Module
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.4 Navigation Module">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#241-neural-sdf-nmpc" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.4.1 Neural SDF-NMPC
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#242-deep-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.4.2 Deep Reinforcement Learning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#243-composite-cbf-based-safety-filter" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.4.3 Composite CBF-based Safety Filter
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#244-exteroceptive-overwrite" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.4.4 Exteroceptive Overwrite
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#25-assumed-low-level-interfaces" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.5 Assumed Low-level Interfaces
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-hardware" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Hardware
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Hardware">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#components-mounted-on-the-unipilot-module" class="md-nav__link">
    <span class="md-ellipsis">
      
        Components Mounted on the UniPilot Module
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-validation-results" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Validation Results
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-conclusion-future-work" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. Conclusion &amp; Future Work
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Conclusion &amp; Future Work">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      
        References
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#abstract" class="md-nav__link">
    <span class="md-ellipsis">
      
        Abstract
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-introduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Introduction
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-unified-autonomy" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Unified Autonomy
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Unified Autonomy">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-autonomy-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.1 Autonomy Architecture
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-perception-module" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.2 Perception Module
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.2 Perception Module">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#221-multi-modal-slam" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.2.1 Multi-Modal SLAM
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#222-vision-language-reasoning" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.2.2 Vision-Language Reasoning
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-planning-module" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.3 Planning Module
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.3 Planning Module">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#231-core-planning-strategy" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.3.1 Core Planning Strategy
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#232-collision-free-planning-to-a-target" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.3.2 Collision-free Planning to a Target
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#233-exploration-planning" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.3.3 Exploration Planning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#234-inspection-planning" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.3.4 Inspection Planning
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#24-navigation-module" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.4 Navigation Module
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.4 Navigation Module">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#241-neural-sdf-nmpc" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.4.1 Neural SDF-NMPC
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#242-deep-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.4.2 Deep Reinforcement Learning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#243-composite-cbf-based-safety-filter" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.4.3 Composite CBF-based Safety Filter
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#244-exteroceptive-overwrite" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.4.4 Exteroceptive Overwrite
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#25-assumed-low-level-interfaces" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.5 Assumed Low-level Interfaces
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-hardware" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Hardware
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Hardware">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#components-mounted-on-the-unipilot-module" class="md-nav__link">
    <span class="md-ellipsis">
      
        Components Mounted on the UniPilot Module
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-validation-results" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Validation Results
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-conclusion-future-work" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. Conclusion &amp; Future Work
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Conclusion &amp; Future Work">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      
        References
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="the-unified-autonomy-stack-toward-a-blueprint-for-generalizable-robot-autonomy">The Unified Autonomy Stack: Toward a Blueprint for Generalizable Robot Autonomy</h1>
<p><strong>Autonomous Robots Lab</strong>, Norwegian University of Science and Technology, Norway</p>
<hr />
<h2 id="abstract">Abstract</h2>
<p>We introduce and open-source the <code>Unified Autonomy Stack</code>, a system-level solution that enables robust autonomy across a wide range of aerial and ground robot morphologies. The architecture centers on three broadly applicable modules -multi-modal perception, multi-stage planning, and multi-layered safety mechanisms- that together deliver end-to-end mission autonomy. The resulting behaviors include safe navigation into unknown regions, exploration of complex environments, efficient inspection planning, and object-level scene reasoning. The stack has been validated on multiple multirotor platforms and legged robots operating in GPS-denied and perceptually degraded environments (e.g., self-similar, textureless, smoke/dust-filled settings), demonstrating resilient performance in demanding conditions. It currently supports two major embodiment families, namely rotorcraft and certain ground systems such as legged and differential-drive platforms, while several of its modules have already been validated on additional robot types, including fixed-wing aircraft for GPS-denied flight and underwater exploration planning. To facilitate ease of adoption and extension, we additionally release a reference hardware design that integrates a full multi-modal sensing suite, time-synchronization electronics, and high-performance compute capable of running the entire stack while leaving headroom for further development. Strategically, we aim to expand the Unified Autonomy Stack to cover most robot configurations across air, land, and sea.</p>
<hr />
<h2 id="1-introduction">1. Introduction</h2>
<p>Autonomy is conventionally designed in a manner bespoke to distinct robot classes. Despite some generalizability in certain subsystems of autonomy, existing literature and available autonomy stacks (e.g., the works in [1, 2, 3, 4, 5, 6, 7, 8]) do not emphasize a unified solution across diverse robot configurations. However, recent advances point toward the potential for a universal autonomy engine. Despite the fact that such a goal is still immature and a wide range of approaches are worth investigating - spanning from ``conventional'' strategies to fully data-driven techniques - the benefits of unification and the collective need to advance robot capabilities highlight the need for general autonomy solutions.</p>
<p><img alt="Figure 1. We release the Unified Autonomy Stack aiming for a common blueprint across diverse robot configurations. Currently supporting widely adopted aerial and ground robot types, the stack is planned to cover different robot morphologies across air, land, and sea. Several of its submodules have been tested across the three operational domains." src="../figs/merged-multi-domain-photo.jpg" />
<em>Figure 1: We release the <code>Unified Autonomy Stack</code> aiming for a common blueprint across diverse robot configurations. Currently supporting widely adopted aerial and ground robot types, the stack is planned to cover different robot morphologies across air, land, and sea. Several of its submodules have been tested across the three operational domains.</em></p>
<p>Motivated by the above, we present the <code>Unified Autonomy Stack</code>, a comprehensive open-source autonomy stack applicable across diverse air and ground robot configurations. The <code>Unified Autonomy Stack</code> represents a step towards a common autonomy blueprint across diverse robot types - from multirotors and other rotorcrafts to legged robots - with the goal to push the frontier toward universal approaches in embodied AI. Its design emphasizes resilience in that it presents robustness, resourcefulness, and redundancy, enabling it to retain high performance across environments and conditions, including GPS-denied, visually-degraded, geometrically complex, and potentially adversarial settings that typically challenge safe navigation and mission autonomy.</p>
<p>At its core, the <code>Unified Autonomy Stack</code> enables navigation for diverse systems in GPS-denied conditions. Simultaneously, it currently offers mission-complete capabilities for a set of information-gathering tasks such as autonomous exploration and inspection. Importantly, the <code>Unified Autonomy Stack</code> is open for extension, both from the perspective of the robots it readily supports and the missions it enables. Our team is explicitly targeting its full-fledged expansion to a) fixed-wing aerial robots, and b) small-scale underwater robots, while facilitating enhanced object-guided behaviors. Developed over a decade of research with its modules (independently or collectively) verified across a great variety of environments and missions spanning from subterranean spaces such as caves and mines - especially in the DAPRA Subterranean Challenge (planner module and a previous version of our perception stack [9, 10, 11]) - to ship ballast tanks and offshore facilities [12], industrial facilities [13], urban facilities [14, 15], and dense forests [16], the presented stack brings high technology readiness while simultaneously facilitating open research investigations.</p>
<h2 id="2-unified-autonomy">2. Unified Autonomy</h2>
<p>This section presents the architecture and key modules of the unified autonomy stack.</p>
<h3 id="21-autonomy-architecture">2.1 Autonomy Architecture</h3>
<p><img alt="Figure 2. The architecture of the Unified Autonomy Stack." src="../figs/UnifiedResilientAutonomy.png" />
<em>Figure 2. The architecture of the Unified Autonomy Stack.</em></p>
<p>The <code>Unified Autonomy Stack</code> is organized around three core modules - <code>perception</code>, <code>planning</code>, and <code>navigation</code> - following the key principles of the "sense-think-act" loop, while targeting generalizability across aerial and ground robot configurations, and resilience in demanding environments. Its overall architecture is presented in Figure 2. As shown, the <code>Unified Autonomy Stack</code> consumes sensor data and outputs low-level commands to standard controllers available in most modern robotic systems, for example on Pixhawk/PX4-based drones [17] (or any other MAVLink-compatible autopilot [18]) and standard velocity controllers on ground platforms. Its key features are as follows:</p>
<ul>
<li><strong>Generalizability:</strong> The <code>Unified Autonomy Stack</code> applies with few adjustments to particularly diverse robot configurations, offering identical user-experience for navigation and informative path planning tasks despite system differences. Currently supporting out-of-the-box multirotors and other rotorcrafts, alongside several ground systems and especially legged robots, while many of its sub-modules have been verified on a broader set of systems including fixed-wing aircraft and underwater robots, it represents a sound point of departure for research in unified embodied AI.</li>
<li><strong>Multi-modality:</strong> The <code>Unified Autonomy Stack</code> fuses complementary sensor cues currently including LiDAR, FMCW radar, vision, and IMU in order to enable resilience in perceptually-degraded GPS-denied conditions [20].</li>
<li><strong>Multi-layer Safety:</strong> The <code>Unified Autonomy Stack</code> departs from conventional architectures in which safety is ensured by solutions with a single-point-of-failure. Most commonly, modern autonomy solutions rely on a cascade of calculations in which collision-free planning takes place only on an online (or offline) reconstructed map. In practice, this means that non-trivial localization or mapping errors (often encountered in perceptually-degraded settings or with thin obstacles) can lead to collisions. The <code>Unified Autonomy Stack</code> implements multiple layers of safety by combining map-based collision-free motion planning with a set of navigation policies and safety filters that directly consume online exteroception data and if necessary adjust the robot's path to correct for safety.</li>
<li><strong>Hybrid Methods:</strong> The <code>Unified Autonomy Stack</code> integrates both "conventional" techniques and deep learning methods. Indicative examples include its factor graph-based multi-modal Simultaneous Localizalition And Mapping (SLAM) and its navigation policies offering options for Exteroceptive Deep Reinforcement Learning (DRL)-based and Neural Signed Distance Field (SDF)-based Nonlinear Model Predictive Control (Neural SDF-NMPC).</li>
</ul>
<p>Subsequently, we outline the key modules of the <code>Unified Autonomy Stack</code> and point to prior works as applicable. Furthermore, we discuss the interfaces considered and how the <code>Unified Autonomy Stack</code> can be extended to new robot configurations.</p>
<h3 id="22-perception-module">2.2 Perception Module</h3>
<p>The <code>perception module</code> includes our solution for multi-modal SLAM, alongside integration with a VLM-based reasoning step.</p>
<h4 id="221-multi-modal-slam">2.2.1 Multi-Modal SLAM</h4>
<p>The multi-modal SLAM (dubbed MIMOSA) system (depicted in Figure 3) uses a factor graph estimator to fuse LiDAR, radar, camera, and IMU measurements using a windowed smoother [19], for computational efficiency. This architecture, as shown in the referenced figures, is based on the methods proposed in [20, 21, 22, 23] for enhanced LiDAR and radar integration. The estimator considers a state space comprised of position, velocity, attitude, accelerometer/gyroscope bias, and gravity direction states. These are estimated over a temporal window by a nonlinear optimizer [24], where the optimal estimate <span class="arithmatex">\(\mathcal{X}^{*}\)</span> is found by minimizing the weighted (by their covariances <span class="arithmatex">\(\Sigma_{*}\)</span>) sum of the residuals <span class="arithmatex">\(\boldsymbol{e}_{*}\)</span> derived from the different sensor measurements. This minimization problem can thus be written as follows:</p>
<div class="arithmatex">\[
\mathcal{X}^{*} = \underset{\mathcal{X}}{\arg\min} \Big[ 
        \lVert \boldsymbol{e}_{0} \rVert_{\Sigma_{0}}^{2}
        + \sum_{i\in\mathcal{F}_{\mathcal{I}}} \lVert \boldsymbol{e}_{\mathcal{I}_{i}} \rVert_{\Sigma_{\mathcal{I}}}^{2}
        + \sum_{i\in\mathcal{F}_{\mathcal{L}}} \lVert \boldsymbol{e}_{\mathcal{L}_{i}} \rVert_{\Sigma_{\mathcal{L}}}^{2}
        + \sum_{i\in\mathcal{F}_{\mathcal{R}}} \lVert \boldsymbol{e}_{\mathcal{R}_{i}} \rVert_{\Sigma_{\mathcal{R}}}^{2}
        + \sum_{i\in\mathcal{F}_{\mathcal{V}}} \lVert \boldsymbol{e}_{\mathcal{V}_{i}} \rVert_{\Sigma_{\mathcal{V}}}^{2}
    \Big],
\]</div>
<p>for the marginalization prior <span class="arithmatex">\(\boldsymbol{e}_{0}\)</span>,<span class="arithmatex">\(\Sigma_{0}\)</span>, and the IMU, LiDAR, radar, and vision factors <span class="arithmatex">\(\mathcal{F}_{*}\)</span> in the window time frame (denoted by <span class="arithmatex">\(\mathcal{I}\)</span>, <span class="arithmatex">\(\mathcal{L}\)</span>, <span class="arithmatex">\(\mathcal{R}\)</span>, and <span class="arithmatex">\(\mathcal{V}\)</span>, respectively). The details on how sensor measurements are used to construct each of these factors are provided in the following sections.</p>
<p><img alt="Figure 3. Overarching factor graph of our MIMOSA Multi-modal." src="../figs/mimosa-factorgraph.png" />
<em>Figure 3. Overarching factor graph of our MIMOSA Multi-modal.</em></p>
<p><strong>Inertial Measurement Unit</strong>
Inertial Measurement Unit (IMU) measurements are stored in a buffer on arrival, for easy use upon receiving measurements from one of the aiding sensors. At that point, the corresponding exteroceptive factors are created and connected to the graph by an IMU preintegration factor, following [25]. Note that, given the multiple exteroceptive sensors and the possibility of transmission latency, a node in the graph with the same or very similar timestamp may already exist at this point. If this is the case, the new factors are attached to the existing node.</p>
<p><strong>LiDAR</strong>
Upon receiving a LiDAR measurement, the point cloud is first deskewed, using the IMU measurements corresponding to the LiDAR's sweep. Afterwards, the point cloud is downsampled, for computational efficiency, first by removing three out of four points, and second by organizing the point cloud into a voxel grid and subsampling. Subsequently, the correspondences are found by relating points in the current cloud with planes so as to fit in the map; these correspondences are added to the graph in the form of point-to-plane residuals. For outlier rejection, these residuals are augmented with Huber M-estimators. Post-optimization, the pose is compared with previous key frames, and if a significant difference in position or attitude is detected, a new key frame is created, and the current point cloud is added to the monolithic map. The core method is presented in [22].</p>
<p><strong>Radar</strong>
Unlike the estimator proposed in [21], here the least-squares calculation of linear velocity from the radar point cloud is omitted. Instead, the individual points from the radar point cloud are integrated into the graph. This avoids the potential limitations associated with first estimating linear velocity independently. Namely, these are the minimum number and diversity of points required for fully resolving the 3 axes of linear velocity. As the radar sensor is known to be noisy [26], the residual is augmented with a Cauchy M-estimator for outlier rejection. This has the added benefit of improved resilience against dynamic objects.</p>
<p><strong>Vision</strong>
Vision factors are added in a loosely-coupled manner, taking advantage of the wealth of capable estimators that exist in the vision community. Specifically, a visual-inertial estimator based on [27] processes the camera and IMU measurements, creating odometry estimates as a result. The pose estimates from this fusion are stored in a buffer and added to the graph of our multi-modal SLAM as a between factor by calculating the relative transform between two pose estimates. This approach draws inspiration from [20].</p>
<h4 id="222-vision-language-reasoning">2.2.2 Vision-Language Reasoning</h4>
<p>The system currently integrates two complementary vision-language model (VLM) modalities: (a) open-vocabulary object detection with 3D spatial grounding, and (b) binary visual question-answering (Yes/No) with reasoning. In their combination, these capabilities collectively enable semantic scene understanding and contextual judgment based on online visual data.</p>
<p><strong>Open-Vocabulary Object Detection with 3D Projection</strong>
Object detection is performed using either a prompt-free detector (YOLOe) or a VLM-based detector (GPT-4V) initialized with a set of labels. These models operate on the front-camera image and produce 2D bounding boxes. In parallel, a downsampled voxel grid derived from the LiDAR point cloud and our MIMOSA SLAM odometry estimate is maintained. Accordingly, LiDAR points are projected into the camera frame using the current pose estimate (and the relevant extrinsics), and clustered to identify which points fall within each 2D detection. This produces aligned 2D detections and corresponding 3D bounding volumes.</p>
<p><strong>Yes/No VLM Question Answering</strong>
For high-level semantic assessment, a VLM (currently GPT-4V) processes the front-camera image together with a ``Yes/No'' question. Indicative queries relate to assessing safety- or navigation-related properties of the scene (e.g., is an object blocking a door?). The model returns the binary answer, alongside a color-coded confidence overlay, and a brief explanation of the reasoning.</p>
<h3 id="23-planning-module">2.3 Planning Module</h3>
<p>Path planning in the <code>Unified Autonomy Stack</code> is facilitated through an updated and extended version of the graph-based planner GBPlanner (called GBPlanner3) originally presented in [10, 11] and augmented for coverage behaviors in [12]. Currently applicable to systems for which graph-based planning is a viable choice, the system enables both target navigation as well as informative planning behaviors such as exploration and inspection in a unified approach (Figure 4).</p>
<p><img alt="Figure 4. Planning module architecture." src="../figs/Universal-Planning-Detail.png" />
<em>Figure 4. Planning module architecture.</em></p>
<h4 id="231-core-planning-strategy">2.3.1 Core Planning Strategy</h4>
<p>GBPlanner's core principles rely on random sampling-based graphs and a local/global architecture that facilitates scalable operation across environments of vast scale and complex geometries. The method samples a dense local graph around the robot's location at any given time, while it simultaneously maintains a sparse global graph that spans over all the regions the robot has visited up to any point in its mission. Graph vertices and edges are sampled only in a collision-free manner, exploiting the volumetric representation in Voxblox [28]. LiDAR or any other depth data is used to update Voxblox online. On top of this planning kernel, the system facilitates collision-free path planning to desired targets, as well as informative behaviors such as exploration of unknown environments and inspection planning. Specifically for ground robots, the method further incorporates traversability constraints, building upon the representation in [29]. A set of other modifications as compared to the prior works in [10, 11, 12] improve the overall behavior, including but not limited to batch vertex-edge sampling in the local graph, availability of multiple distributions (uniform and normal tailored to narrow spaces) for vertex sampling, and active collision re-check and path modification during global re-positioning and return-to-home plans.</p>
<h4 id="232-collision-free-planning-to-a-target">2.3.2 Collision-free Planning to a Target</h4>
<p>The <code>Unified Autonomy Stack</code> facilitates planning to a desired waypoint both within the already explored space, as well as in the unknown, as long as this is iteratively found to be possible. For the first goal, the method simply exploits its local and global graphs to plan a safe path to any of the explored locations. For the latter, the method plans to a waypoint close to the frontier of the explored volume and in the direction of the desired target, while it then iteratively repeats this process as the robot unveils more of the space based on its onboard depth sensing.</p>
<h4 id="233-exploration-planning">2.3.3 Exploration Planning</h4>
<p>When the behavior exploration of unknown volumes is set, the planner is searching within its local graph for paths that maximize the anticipated gain of new volume to be explored, considering a defined range sensor model, as detailed in [10, 11]. When the solutions within the local graph report that no such informative path exists, the method re-positions the robot to a previously explored area in order for the exploration mission to be effectively continued. If the considered robot endurance limits are about to be reached, the method timely commands a return-to-home path for the system (considering the time it takes to return to that location).</p>
<h4 id="234-inspection-planning">2.3.4 Inspection Planning</h4>
<p>When the behavior of optimized inspection of previously mapped structures is set, the planner is using its random graph to search for a route that observes as much as possible of the considered subset of the environment while keeping the collective path length small. The method assumes a certain sensor frustum for inspection that may be distinct from the sensor with which the robot explores the world volumetrically. Inspection resolution and angle guarantees are provided and the overall methodology is inspired by the concept of General Visual Inspection (GVI) in industrial facilities. The procedure is detailed in [12]. Critically, the exploration and inspection behaviors can be combined with the planner triggering each mode as necessary for a broader mission.</p>
<h3 id="24-navigation-module">2.4 Navigation Module</h3>
<p>The <code>Unified Autonomy Stack</code> takes a multi-layered approach to safety illustrated in Figure 5. Conventionally, modern safe navigation and collision avoidance are based on the planning of paths (and trajectories) which are then blindly followed by an onboard controller. However, as discussed in [16, 30], this represents a single point of failure which can lead robots to collisions due to odometry errors or erroneous/incomplete mapping. While the <code>Unified Autonomy Stack</code> maintains map-based avoidance as the core approach to safety, it adds layers of safety either through (a) the combined use of exteroceptive Neural SDF-NMPC [16] and last-resort safety filtering based on CBFs, or (b) via exteroceptive DRL-based policies trained for safe navigation and smooth collision avoidance. The two distinct approaches -presented below- are offered simultaneously for the user to select owing to their relative benefits in certain conditions and in order to enable research across areas. These methods replace conventional position controllers - or similar methods - that assume that the provided path planned on the reconstructed map is to be followed blindly, and instead enable local deviations if necessary. They provide commands to a low-level controller following standard interfaces applicable across most widely used Autopilots and Robot Operating System (ROS)-based ground vehicles or similar systems. </p>
<p><img alt="Figure 5. Navigation module architecture Note that currently Neural SDF-NMPC provides acceleration references potentially adjusted by the Composite CBF safety filter, while when Exteroceptive-DRL is utilized the output is velocity references and integration with the safety filter is not currently provided." src="../figs/navigation-stack-diagram.png" />
<em>Figure 5. Navigation module architecture Note that currently Neural SDF-NMPC provides acceleration references potentially adjusted by the Composite CBF safety filter, while when Exteroceptive-DRL is utilized the output is velocity references and integration with the safety filter is not currently provided.</em></p>
<h4 id="241-neural-sdf-nmpc">2.4.1 Neural SDF-NMPC</h4>
<p>Detailed in [16], the Neural SDF-NMPC enables collision-free navigation in unknown environments even without access to a map, errors in a robot's map or drifting odometry. The method employs onboard range sensing (e.g., from a LiDAR or depth from stereo/RGB-D) and a deep neural network to convert range images to a SDF representation through a cascade structure. First, a convolutional encoder compresses the input image and then a Multi-Layer Perceptron (MLP) approximates the SDF. The learned SDF is then providing explicit position constraints for obstacle avoidance, directly into a nonlinear MPC. Thus, the method outputs acceleration commands to be tracked by a low-level control and enables safer navigation to target locations by adding a further layer of safety. Critically, the Neural SDF-NMPC provides theoretical guarantees when it comes to recursive feasibility and stability (under fixed sensor observations), which are essential features for the emphasis <code>Unified Autonomy Stack</code> puts on assured safety. Note that the method specifically tracks reference velocities and thus the interface with the <code>planning module</code> employs a function to produce such commands from planned paths.</p>
<h4 id="242-deep-reinforcement-learning">2.4.2 Deep Reinforcement Learning</h4>
<p>Alternatively to the use of the Neural SDF-NMPC, the <code>Unified Autonomy Stack</code> further offers exteroceptive DRL-based navigation (dubbed Exteroceptive-DRL) based on the work in [30, 31]. The provided policies consider as input both the robot's odometry and the instantaneous real-time depth (or range) image from any relevant sensor (including stereo or RGB-D cameras, as well as LiDARs). We offer the option to both train the policy by (a) first employing the DCE originally presented in [32] and thus encode the depth image to a low-dimension latent space responsible to maintain collision information, or (b) directly employ end-to-end learning from depth and odometry data to robot commands. In both cases, the command vector is the robot's reference body velocities. Like in the case of the Neural SDF-NMPC the method enables safe collision-free navigation even without a map and is thus provided under the concept of multi-layered safety, offering further assurance even when the onboard SLAM experiences odometry drift or the map fails to appropriately map certain obstacles (e.g., thin wires [33]).</p>
<h4 id="243-composite-cbf-based-safety-filter">2.4.3 Composite CBF-based Safety Filter</h4>
<p>Beyond the abovementioned navigation approaches -which combine map-based safety of the <code>planning module</code> with reactive collision avoidance- the <code>Unified Autonomy Stack</code> further provides a last-resort safety filter. Based on Composite Control Barrier Functions (C-CBFs) on a short sliding window of volumetric map data as detailed in [34], the solution modifies the reference acceleration (or velocity) transmitted to the robot's low-level controller when an unexpected impending collision is detected. To that point, the robot motion model is taken into consideration, while the individual obstacle constraints acquired by the map representation are combined into a composite function. The latter creates a smooth under-approximation of the safe set, and its gradient creates a "virtual obstacle" effect, acting as a weighted average of nearby obstacles to steer the robot away from danger. Although by-design considered for extremely rare engagement by the overall autonomy stack (as avoidance through planning and the presented navigation methods is made to function successfully), it is an important module to fully -and mathematically formally- safeguard autonomous robots operating in hard-to-access and overall demanding environments. Furthermore, a key decision variable for its use is the fact that both Neural SDF-NMPC and Exteroceptive-DRL involve a deep neural network the assured and explainable performance of which is not strictly guaranteed.</p>
<h4 id="244-exteroceptive-overwrite">2.4.4 Exteroceptive Overwrite</h4>
<p>When map-based safety from the <code>planning module</code> is considered sufficient or when the specific exteroceptive navigation methods are not thought to be desirable for (or compatible with) a certain robot, the <code>Unified Autonomy Stack</code> allows to use a more conventional state feedback controller by the user. This can be any such solution, such as Linear MPC, the underlying MPC method in Neural SDF-NMPC disabling collision constraints, or a solution for any other particular robot or through any autopilot (e.g., PX4). A common option can be the use of PX4's position control mode or a standard waypoint controller provided by the robot's manufacturer or through existing open-source packages.</p>
<h3 id="25-assumed-low-level-interfaces">2.5 Assumed Low-level Interfaces</h3>
<p>The <code>Unified Autonomy Stack</code> interfaces diverse aerial and ground robots through the following interfaces:
* If the multi-layered safe navigation is not necessary, we provide waypoints or 3D accelerations to existing autopilots and low-level controllers to diverse rotorcrafts and ground robots. Waypoints are straightforward for all systems that have such control. However, it does not deliver the full stack functionality of multi-layered safe navigation.
* When the full stack with multi-layered safety is considered, we command reference accelerations or velocities. Those we can provide directly to compatible autopilots and low-level controllers such as any PX4/ArduPilot-based drone.</p>
<p>We plan to provide support for most widely-adopted low-level control/autopilot interfaces.</p>
<h2 id="3-hardware">3. Hardware</h2>
<p>To support early adoption, we combine the open-source release of the <code>Unified Autonomy Stack</code> with a reference hardware design incorporating all the considered sensing modalities, time-synchronization electronics and ample onboard compute. Called "UniPilot" the system is detailed in [35]. UniPilot is a compact modular autonomy payload combining LiDAR, radar, vision, Time-of-Flight (ToF)-depth and IMU sensing with an NVIDIA Orin NX-based compute and is deployable across heterogeneous robots. The specific sensor types are shown in the table below. Power, interfacing and synchronization electronics enable a compact and high-performance solution. All subsystems are housed in a lightweight SLA-printed enclosure engineered for mechanical robustness and cross-platform mounting. Tested onboard multitors, legged robots, and VTOL convertible aerial robots, UniPilot represents a practical, fully integrated autonomy payload enabling mapping, planning, safety, and control across diverse platforms in challenging GPS-denied visually-degraded, communications-denied environments. It can readily run the <code>Unified Autonomy Stack</code> and thus enable its immediate adoption. The full sensing configuration (and optional choices) of UniPilot are shown in the table below.</p>
<h3 id="components-mounted-on-the-unipilot-module">Components Mounted on the UniPilot Module</h3>
<table>
<thead>
<tr>
<th style="text-align: left;">Category</th>
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Weight [g]</th>
<th style="text-align: left;">Power [W]</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Compute</strong></td>
<td style="text-align: left;">1x Orin NX</td>
<td style="text-align: left;">28</td>
<td style="text-align: left;">25</td>
<td style="text-align: left;">RAM: 16 GB, 8-core Arm Cortex CPU, 1024-core NVIDIA GPU</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">1x Boson-22 carrier board + heat sink</td>
<td style="text-align: left;">152</td>
<td style="text-align: left;">13.8</td>
<td style="text-align: left;">4x MIPI CSI-2, 2x Gigabit Ethernet, 1x USB 2.0, 1x USB 3.1, 8x GPIO, 1x CAN, 2x I2C, 3x UART, 2x SPI interfaces</td>
</tr>
<tr>
<td style="text-align: left;"><strong>IMU</strong></td>
<td style="text-align: left;">1x VectorNav VN-100</td>
<td style="text-align: left;">15</td>
<td style="text-align: left;">0.22</td>
<td style="text-align: left;">800 Hz, Accel.: <span class="arithmatex">\(140 \mu g/\sqrt{Hz}\)</span>, Gyro.: <span class="arithmatex">\(0.0035^{\circ}/s/\sqrt{Hz}\)</span></td>
</tr>
<tr>
<td style="text-align: left;"><strong>Cameras</strong></td>
<td style="text-align: left;">2x VC MIPI IMX296M</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.759</td>
<td style="text-align: left;">Grayscale, Resolution: <span class="arithmatex">\(1440\times1080\)</span>, FoV: <span class="arithmatex">\(185^{\circ}\)</span></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">1x VC MIPI IMX296C</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.759</td>
<td style="text-align: left;">Color, Resolution: <span class="arithmatex">\(1440\times1080\)</span>, FoV: <span class="arithmatex">\(118^{\circ}\times94^{\circ}\)</span></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">1x PMD Flexx2 ToF</td>
<td style="text-align: left;">13</td>
<td style="text-align: left;">0.68</td>
<td style="text-align: left;">Range: 4 m, FoV: <span class="arithmatex">\(56 \times 44^{\circ}\)</span></td>
</tr>
<tr>
<td style="text-align: left;"><strong>Radar</strong></td>
<td style="text-align: left;">1x D3 Embedded RS-6843AOPU</td>
<td style="text-align: left;">25</td>
<td style="text-align: left;">7.5</td>
<td style="text-align: left;">Range: 49 m, FoV: <span class="arithmatex">\(180 \times 180^{\circ}\)</span></td>
</tr>
<tr>
<td style="text-align: left;"><strong>LiDAR</strong></td>
<td style="text-align: left;">1x RoboSense Airy or OUSTER OS0</td>
<td style="text-align: left;">230</td>
<td style="text-align: left;">8</td>
<td style="text-align: left;">Range: 30 m, FoV: <span class="arithmatex">\(360 \times 90^{\circ}\)</span></td>
</tr>
</tbody>
</table>
<p>The animation below presents the UniPilot core subsystems.</p>
<p><img alt="View of the main sensors and compute of UniPilot" src="../figs/unipilot-expansion-loop.gif" />
<em>Figure 6. 3D view of the UniPilot reference hardware design.</em></p>
<h2 id="4-validation-results">4. Validation Results</h2>
<p>A set of prior results (documented in the linked publications and this <a href="https://youtube.com/playlist?list=PLu70ME0whad-QOtLW7t3qoU35-Yba8NmC&amp;si=qbFSqxXjuFBPGSQU">YouTube List</a>) demonstrate the individual evaluation of the underlying modules, while a set of new results, simulation set-ups and open datasets are planned for release with the <code>Unified Autonomy Stack</code>. All results will be documented (continuously) in this repository.</p>
<h2 id="5-conclusion-future-work">5. Conclusion &amp; Future Work</h2>
<p>The <code>Unified Autonomy Stack</code> is openly released with the aim of serving as a foundation for a common autonomy blueprint across diverse robot configurations operating in the air, on land, and at sea. We seek to collaborate with the research community towards its most reliable operation and resilient performance, alongside its extension to different robot morphologies. We are confident that robot autonomy can take the explosive trajectory observed in autopilots and thus be highly democratized while maintaining exceptional performance and robustness.</p>
<hr />
<h3 id="references">References</h3>
<ol>
<li>M. Fernandez-Cortizas, M. Molina, P. Arias-Perez, R. Perez-Segui, D. Perez-Saura, and P. Campoy, Aerostack2: A software framework for developing multi-robot aerial systems, arXiv preprint, arXiv:2303.18237, 2023.</li>
<li>J. L. Sanchez-Lopez, R. A. S. Fernndez, H. Bavle, C. Sampedro, M. Molina, J. Pestana, and P. Campoy, Aerostack: An architecture and open-source software framework for aerial robotics, in 2016 International Conference on Unmanned Aircraft Systems (ICUAS). IEEE, 2016, pp. 332341.</li>
<li>T. Baca, M. Petrlik, M. Vrba, V. Spurny, R. Penicka, D. Hert, and M. Saska, The mrs uav system: Pushing the frontiers of reproducible research, real-world deployment, and education with autonomous unmanned aerial vehicles, Journal of Intelligent &amp; Robotic Systems, vol. 102, no. 1, p. 26, 2021.</li>
<li>K. Mohta, M. Watterson, Y. Mulgaonkar, S. Liu, C. Qu, A. Makineni, K. Saulnier, K. Sun, A. Zhu, J. Delmerico, D. Thakur, K. Karydis, N. Atanasov, G. Loianno, D. Scaramuzza, K. Daniilidis, C. J. Taylor, and V. Kumar, Fast, autonomous flight in gps-denied and cluttered environments, Journal of Field Robotics, vol. 35, no. 1, pp. 101120, 2018.</li>
<li>P. Foehn, E. Kaufmann, A. Romero, R. Penicka, S. Sun, L. Bauersfeld, T. Laengle, G. Cioffi, Y. Song, A. Loquercio et al., Agilicious: Opensource and open-hardware agile quadrotor for vision-based flight, Science robotics, vol. 7, no. 67, p. eabl6259, 2022.</li>
<li>C. Goodin, M. N. Moore, D. W. Carruth, C. R. Hudson, L. D. Cagle, S. Wapnick, and P. Jayakumar, The nature autonomy stack: an open-source stack for off-road navigation, in Unmanned Systems Technology XXVI, vol. 13055. SPIE, 2024, pp. 817.</li>
<li>Carnegie Mellon University, AirLab, Airstack, https://github.com/castacks/AirStack, 2025, accessed: 2025-02-04.</li>
<li>F. Real, A. Torres-Gonzlez, P. R. Soria, J. Capitn, and A. Ollero, Unmanned aerial vehicle abstraction layer: An abstraction layer to operate unmanned aerial vehicles, International Journal of Advanced Robotic Systems, vol. 17, no. 4, pp. 113, 2020. [Online]. Available: https://doi.org/10.1177/1729881420925011</li>
<li>M. Tranzatto, T. Miki, M. Dharmadhikari, L. Bernreiter, M. Kulkarni, F. Mascarich, O. Andersson, S. Khattak, M. Hutter, R. Siegwart, and K. Alexis, Cerberus in the darpa subterranean challenge, Science Robotics, vol. 7, no. 66, p. eabp9742, 2022.</li>
<li>T. Dang, M. Tranzatto, S. Khattak, F. Mascarich, K. Alexis, and M. Hutter, Graph-based subterranean exploration path planning using aerial and legged robots, Journal of Field Robotics, vol. 37, no. 8, pp. 13631388, 2020, eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.21993. [Online]. Available: https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21993</li>
<li>M. Kulkarni, M. Dharmadhikari, M. Tranzatto, S. Zimmermann, V. Reijgwart, P. De Petris, H. Nguyen, N. Khedekar, C. Papachristos, L. Ott, R. Siegwart, M. Hutter, and K. Alexis, Autonomous Teamed Exploration of Subterranean Environments using Legged and Aerial Robots, in 2022 International Conference on Robotics and Automation (ICRA), May 2022, pp. 33063313. [Online]. Available: https://ieeexplore.ieee.org/document/9812401</li>
<li>M. Dharmadhikari, P. De Petris, M. Kulkarni, N. Khedekar, H. Nguyen, A. E. Stene, E. Sjvold, K. Solheim, B. Gussiaas, and K. Alexis, Autonomous Exploration and General Visual Inspection of Ship Ballast Water Tanks Using Aerial Robots, in 2023 21st International Conference on Advanced Robotics (ICAR), Dec. 2023, pp. 409416, iSSN: 2572-6919. [Online]. Available: https://ieeexplore.ieee.org/document/10406928</li>
<li>M. Dharmadhikari, N. Khedekar, P. De Petris, M. Kulkarni, M. Nissov, and K. Alexis, Maritime vessel tank inspection using aerial robots: Experience from the field and dataset release, arXiv preprint arXiv:2404.19045, 2024.</li>
<li>M. Singh, M. Dharmadhikari, and K. Alexis, Ariel explores: Vision-based underwater exploration and inspection via generalist drone-level autonomy, arXiv preprint arXiv:2507.10003, 2025.</li>
<li>M. Tranzatto, F. Mascarich, L. Bernreiter, C. Godinho, M. Camurri, S. Khattak, T. Dang, V. Reijgwart, J. Loeje, D. Wisth et al., Cerberus: Autonomous legged and aerial robotic exploration in the tunnel and urban circuits of the darpa subterranean challenge, arXiv preprint arXiv:2201.07067, vol. 3, 2022.</li>
<li>M. Jacquet, M. Harms, and K. Alexis, Neural NMPC through Signed Distance Field Encoding for Collision Avoidance, Nov. 2025, arXiv:2511.21312 [cs]. [Online]. Available: http://arxiv.org/abs/2511. 21312</li>
<li>L. Meier, D. Honegger, and M. Pollefeys, Px4: A node-based multithreaded open source robotics framework for deeply embedded platforms, in 2015 IEEE international conference on robotics and automation (ICRA). IEEE, 2015, pp. 62356240.</li>
<li>A. Koubaa, A. Allouch, M. Alajlan, Y. Javed, A. Belghith, and M. Khalgui, Micro air vehicle link (mavlink) in a nutshell: A survey, IEEE Access, vol. 7, pp. 87 65887 680, 2019.</li>
<li>F. Dellaert and GTSAM Contributors, borglab/gtsam, May 2022. [Online]. Available: https://github.com/borglab/gtsam</li>
<li>N. Khedekar, M. Kulkarni, and K. Alexis, MIMOSA: A Multi- Modal SLAM Framework for Resilient Autonomy against Sensor Degradation, in 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Kyoto, Japan: IEEE, Oct. 2022, pp. 71537159. [Online]. Available: https://ieeexplore.ieee.org/document/9981108/</li>
<li>M. Nissov, N. Khedekar, and K. Alexis, Degradation Resilient LiDAR-Radar-Inertial Odometry, in 2024 IEEE International Conference on Robotics and Automation (ICRA), May 2024, pp. 85878594. [Online]. Available: https://ieeexplore.ieee.org/document/10611444</li>
<li>N. Khedekar and K. Alexis, PG-LIO: Photometric-Geometric fusion for Robust LiDAR-Inertial Odometry, Jun. 2025, arXiv:2506.18583 [cs]. [Online]. Available: http://arxiv.org/abs/2506.18583</li>
<li>M. Nissov, J. A. Edlund, P. Spieler, C. Padgett, K. Alexis, and S. Khattak, Robust high-speed state estimation for off-road navigation using radar velocity factors, IEEE Robotics and Automation Letters, vol. 9, no. 12, pp. 11 14611 153, 2024.</li>
<li>M. Kaess, H. Johannsson, R. Roberts, V. Ila, J. Leonard, and F. Dellaert, iSAM2: Incremental smoothing and mapping with fluid relinearization and incremental variable reordering, in 2011 IEEE International Conference on Robotics and Automation. IEEE, May 2011, pp. 32813288.</li>
<li>C. Forster, L. Carlone, F. Dellaert, and D. Scaramuzza, On-manifold preintegration for real-time visualinertial odometry, IEEE Transactions on Robotics, vol. 33, no. 1, pp. 121, 2017.</li>
<li>K. Harlow, H. Jang, T. D. Barfoot, A. Kim, and C. Heckman, A new wave in robotics: Survey on recent mmwave radar applications in robotics, IEEE Transactions on Robotics, vol. 40, pp. 45444560, 2024.</li>
<li>M. Bloesch, M. Burri, S. Omari, M. Hutter, and R. Siegwart, Iterated extended kalman filter based visual-inertial odometry using direct photometric feedback, The International Journal of Robotics Research, vol. 36, no. 10, pp. 10531072, 2017.</li>
<li>H. Oleynikova, Z. Taylor, M. Fehr, R. Siegwart, and J. Nieto, Voxblox: Incremental 3d euclidean signed distance fields for onboard mav planning, in 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2017, pp. 13661373.</li>
<li>P. Fankhauser, M. Bloesch, and M. Hutter, Probabilistic terrain mapping for mobile robots with uncertain localization, IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 30193026, 2018</li>
<li>M. Kulkarni and K. Alexis, Reinforcement Learning for Collision-free Flight Exploiting Deep Collision Encoding, in 2024 IEEE International Conference on Robotics and Automation (ICRA), May 2024, pp. 15 78115 788. [Online]. Available: https://ieeexplore.ieee.org/document/10610287</li>
<li>M. Kulkarni, W. Rehberg, and K. Alexis, Aerial Gym Simulator: A Framework for Highly Parallelized Simulation of Aerial Robots, IEEE Robotics and Automation Letters, vol. 10, no. 4, pp. 40934100, Apr. 2025. [Online]. Available: https://ieeexplore.ieee.org/document/10910148/</li>
<li>M. Kulkarni and K. Alexis, Task-Driven Compression for Collision Encoding Based on Depth Images, in Advances in Visual Computing, G. Bebis, G. Ghiasi, Y. Fang, A. Sharf, Y. Dong, C. Weaver, Z. Leo, J. J. LaViola Jr., and L. Kohli, Eds. Cham: Springer Nature Switzerland, 2023, pp. 259273.</li>
<li>M. Kulkarni, B. Moon, K. Alexis, and S. Scherer, Aerial field robotics, in Encyclopedia of Robotics. Springer, 2022, pp. 115.</li>
<li>M. Harms, M. Jacquet, and K. Alexis, Safe Quadrotor Navigation Using Composite Control Barrier Functions, in 2025 IEEE International Conference on Robotics and Automation (ICRA), May 2025, pp. 63436349. [Online]. Available: https://ieeexplore.ieee.org/document/11127368</li>
<li>M. Kulkarni, M. Dharmadhikari, N. Khedekar, M. Nissov, M. Singh, P. Weiss, and K. Alexis, Unipilot: Enabling gps-denied autonomy across embodiments, arXiv preprint arXiv:2509.11793, 2025.</li>
</ol>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../../planned_upgrades/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Planned Upgrades">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Planned Upgrades
              </div>
            </div>
          </a>
        
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Rights reserved with the Autonomous Robots Lab, NTNU
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.tabs", "navigation.footer", "navigation.top", "navigation.expand", "search.suggest", "search.highlight", "content.code.copy"], "search": "../../assets/javascripts/workers/search.7a47a382.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.e71a0d61.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>